{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Dense, Flatten, Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import MeCab\n",
    "import json\n",
    "import hashlib\n",
    "from googletrans import Translator\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import torch\n",
    "from transformers.modeling_bert import BertModel\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "mt = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "mt.parse('')\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "\n",
    "model_doc = Doc2Vec.load(\"jawiki.doc2vec.dbow300d.model\")\n",
    "model_word = word2vec.Word2Vec.load(\"wiki_plus.model\")\n",
    "model_bert = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(text):\n",
    "    word = {}\n",
    "    node = mt.parseToNode(text)\n",
    "    while node:\n",
    "        fields = node.feature.split(\",\")\n",
    "        if (fields[0] == '名詞' or fields[0] == '動詞' or fields[0] == '形容詞') and node.surface in model_word.wv:\n",
    "            w = node.surface\n",
    "            word[w] = word.get(w, 0) + 1\n",
    "        node = node.next\n",
    "    return word\n",
    "\n",
    "def weighted_mean_vec(text):\n",
    "    v = np.zeros(model_word.vector_size)\n",
    "    s = 1.0\n",
    "    for w,weight in get_tags(text).items():\n",
    "        v += weight * model_word.wv[w]  #Eventクラスeの単語wの個数＊単語wのベクトル\n",
    "        s += weight\n",
    "    return v / s\n",
    "\n",
    "def get_tags_for_doc2vec(text):\n",
    "    word = []\n",
    "    node = mt.parseToNode(text)\n",
    "    while node:\n",
    "        fields = node.feature.split(\",\")\n",
    "        if node.surface in model_doc.wv and node.surface !='':\n",
    "            w = node.surface\n",
    "            word.append(w)\n",
    "        node = node.next\n",
    "    return word\n",
    "\n",
    "#bertのベクトル化\n",
    "def get_vector_cls(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt') \n",
    "    result = model_bert(input_ids)\n",
    "    tensor_result = result[0][0][0]\n",
    "    numpy_result = tensor_result.to('cpu').detach().numpy().copy()\n",
    "    return numpy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event:\n",
    "    def __init__(self, id, type, score, desc, links):\n",
    "        self.id = id\n",
    "        self.type = type\n",
    "        self.score = score\n",
    "        self.desc = desc\n",
    "        self.links = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON ファイルから event set をロード\n",
    "def load_events(jsonfile):\n",
    "    with open(jsonfile) as f:\n",
    "        df = json.load(f)\n",
    "    events = {x['id']: Event(x['id'], x['type'], x['score'], x['desc'], x['links']) for x in df} #eventsにidをkeyとしそのオブジェクトをvalueとした辞書を生成\n",
    "    for k,x in events.items():\n",
    "        x.links = [events[e] for e in x.links] #Event.linkの中身をidの配列からEventの配列に変更\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = load_events('sesaku2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "labels = []\n",
    "columns=[]\n",
    "index=[]\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='部品':\n",
    "        index.append(v1.desc)\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='対策':\n",
    "        if not v1.desc in columns:\n",
    "            columns.append(v1.desc)\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='部品':\n",
    "        for k2, v2 in events.items():\n",
    "            if v2.type[-1] == '対策':\n",
    "                    if v2 in v1.links:\n",
    "                        df.at[v1.desc, v2.desc] = 1\n",
    "                    else:\n",
    "                        df.at[v1.desc, v2.desc] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "taisaku_vec_word = {}\n",
    "for i in df:\n",
    "    taisaku_vec_word[i]=weighted_mean_vec(i)\n",
    "\n",
    "taisaku_vec_doc = {}\n",
    "for i in df:\n",
    "    taisaku_vec_doc[i]=model_doc.infer_vector(get_tags_for_doc2vec(i))\n",
    "\n",
    "taisaku_vec_bert = {}\n",
    "for i in df:\n",
    "    taisaku_vec_bert[i]=get_vector_cls(i)\n",
    "\n",
    "class Label:\n",
    "    TAISAKU = 1\n",
    "    NASI = 0\n",
    "\n",
    "data_word = []\n",
    "labels_word = []\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    x1 = weighted_mean_vec(index)\n",
    "    for i in df:#対策\n",
    "        x2 =  taisaku_vec_word[i]#対策\n",
    "        data_word.extend([np.append(x1, x2)])\n",
    "        if row[i] ==1:\n",
    "            labels_word.append(Label.TAISAKU)\n",
    "        else:\n",
    "            labels_word.append(Label.NASI)\n",
    "\n",
    "data_doc = []\n",
    "labels_doc = []\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    x1 = model_doc.infer_vector(get_tags_for_doc2vec(index))\n",
    "    for i in df:#対策\n",
    "        x2 =  taisaku_vec_doc[i]#対策\n",
    "        data_doc.extend([np.append(x1, x2)])\n",
    "        if row[i] ==1:\n",
    "            labels_doc.append(Label.TAISAKU)\n",
    "        else:\n",
    "            labels_doc.append(Label.NASI)\n",
    "\n",
    "data_BERT_cls = []\n",
    "labels_BERT_cls = []\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    x1 = get_vector_cls(index) #部品\n",
    "    for i in df:#対策\n",
    "        x2=taisaku_vec_bert[i]\n",
    "        data_BERT_cls.extend([np.append(x1, x2)])\n",
    "        if row[i] ==1:\n",
    "            labels_BERT_cls.append(Label.TAISAKU)\n",
    "        else:\n",
    "            labels_BERT_cls.append(Label.NASI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " #生成器\n",
    "def build_generator(data_size, z_dim):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(128, input_dim = z_dim))\n",
    "        \n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "        \n",
    "        model.add(Dense(data_size, activation='tanh'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#識別器\n",
    "\n",
    "def build_discriminator(data_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128, activation=LeakyReLU(alpha=0.01), input_shape=(data_size,)))\n",
    "    \n",
    "    #model.add(LeakyReLU(alpha=0.01))\n",
    "        \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_compile(data, z_dim):\n",
    "    data_size = len(data[0])\n",
    "    #識別器の構築とコンパイル\n",
    "    discriminator = build_discriminator(data_size)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    #生成器の構築\n",
    "    generator = build_generator(data_size, z_dim)\n",
    "\n",
    "    #生成器の構築中は識別器のパラメータを固定\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    #生成器の訓練のため、識別器は固定し、GANモデルの構築とコンパイルを行う\n",
    "    gan = build_gan(generator, discriminator)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "    \n",
    "    return discriminator, generator, gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(data,labels):\n",
    "    index_1 = [i for i, x in enumerate(labels) if x == 1]\n",
    "    index_0 = [i for i, x in enumerate(labels) if x == 0]\n",
    "    index_0 = random.sample(index_0, len(index_1))\n",
    "    data_1 = [data[i] for i in index_1]\n",
    "    data_0 = [data[i] for i in index_0]\n",
    "    labels = [Label.TAISAKU]*len(data_1) + [Label.NASI]*len(data_0) \n",
    "    data = data_1 + data_0\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_y_train(data,labels):\n",
    "    index_1 = [i for i, x in enumerate(labels) if x == 1]\n",
    "    index_0 = [i for i, x in enumerate(labels) if x == 0]\n",
    "    index_0 = random.sample(index_0, len(index_1))\n",
    "    data_1 = [data[i] for i in index_1]\n",
    "    data_0 = [data[i] for i in index_0]\n",
    "\n",
    "    labels_1 = [Label.TAISAKU]*len(data_1)\n",
    "    labels_0 = [Label.NASI]*len(data_0) \n",
    "    data_0 = np.array(data_0)\n",
    "    labels_0 = np.array(labels_0)\n",
    "    data_1 = np.array(data_1)\n",
    "    labels_1 = np.array(labels_1)\n",
    "    return data_0, labels_0, data_1, labels_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "iteration_checkpoints = []\n",
    "\n",
    "\n",
    "def train(data, labels, iterations, batch_size, sample_interval):\n",
    "    \n",
    "    data_0, labels_0, data_1, lables_1 = x_y_train(data,labels)\n",
    "    #1の数7303\n",
    "    #0の数7303\n",
    "    \n",
    "    #ラベル1\n",
    "    real = np.ones((batch_size,1))\n",
    "    #ラベル0\n",
    "    fake = np.zeros((batch_size,1))\n",
    "    \n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        #-------------------\n",
    "        #識別器の訓練\n",
    "        #-------------------\n",
    "        \n",
    "        #ランダムに関係があるベクトルをとる\n",
    "        idx = np.random.randint(0,len(data_1),batch_size)\n",
    "        vecs = data_1[idx]\n",
    "        \n",
    "        \n",
    "        z = np.random.normal(0,1,(batch_size, 100))\n",
    "        \n",
    "        gen_vec = generator.predict(z)\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(vecs, real)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_vec, fake)\n",
    "        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        #-------------------\n",
    "        #生成器の訓練\n",
    "        #-------------------\n",
    "        \n",
    "        #平均0標準偏差１の正規分布に従う乱数を生成\n",
    "        z = np.random.normal(0, 1,(batch_size, 100))\n",
    "            \n",
    "        gen_vec = generator.predict(z)\n",
    "        \n",
    "        g_loss = gan.train_on_batch(z,real)\n",
    "        \n",
    "        if(iteration +1) % sample_interval ==0:\n",
    "            losses.append((d_loss, g_loss))\n",
    "            accuracies.append(100.0 * accuracy)\n",
    "            iteration_checkpoints.append(iteration +1)\n",
    "        \n",
    "        print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %(iteration +1,d_loss, 100.0*accuracy,g_loss))\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0, labels_0, data_1, lables_1 = x_y_train(data_word,labels_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randint(0,len(data_1),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.962333, acc.: 41.41%] [G loss: 0.944094]\n",
      "2 [D loss: 0.417514, acc.: 68.75%] [G loss: 0.766589]\n",
      "3 [D loss: 0.444082, acc.: 59.38%] [G loss: 0.633709]\n",
      "4 [D loss: 0.521503, acc.: 52.73%] [G loss: 0.535652]\n",
      "5 [D loss: 0.580824, acc.: 51.17%] [G loss: 0.527657]\n",
      "6 [D loss: 0.634639, acc.: 50.39%] [G loss: 0.425568]\n",
      "7 [D loss: 0.652550, acc.: 50.39%] [G loss: 0.440438]\n",
      "8 [D loss: 0.683285, acc.: 50.39%] [G loss: 0.448032]\n",
      "9 [D loss: 0.652323, acc.: 50.78%] [G loss: 0.487600]\n",
      "10 [D loss: 0.598742, acc.: 50.78%] [G loss: 0.584463]\n",
      "11 [D loss: 0.515210, acc.: 53.52%] [G loss: 0.658654]\n",
      "12 [D loss: 0.454722, acc.: 61.33%] [G loss: 0.908089]\n",
      "13 [D loss: 0.355974, acc.: 78.12%] [G loss: 1.071683]\n",
      "14 [D loss: 0.263202, acc.: 92.58%] [G loss: 1.363299]\n",
      "15 [D loss: 0.221081, acc.: 95.31%] [G loss: 1.654232]\n",
      "16 [D loss: 0.166246, acc.: 98.44%] [G loss: 1.921894]\n",
      "17 [D loss: 0.137351, acc.: 99.61%] [G loss: 2.164061]\n",
      "18 [D loss: 0.109910, acc.: 99.61%] [G loss: 2.283379]\n",
      "19 [D loss: 0.113599, acc.: 99.22%] [G loss: 2.326894]\n",
      "20 [D loss: 0.118269, acc.: 99.22%] [G loss: 2.292561]\n",
      "21 [D loss: 0.126855, acc.: 100.00%] [G loss: 2.216823]\n",
      "22 [D loss: 0.180731, acc.: 97.27%] [G loss: 2.082688]\n",
      "23 [D loss: 0.227273, acc.: 94.14%] [G loss: 1.879986]\n",
      "24 [D loss: 0.273223, acc.: 91.41%] [G loss: 1.630967]\n",
      "25 [D loss: 0.345165, acc.: 85.16%] [G loss: 1.506415]\n",
      "26 [D loss: 0.349684, acc.: 84.38%] [G loss: 1.630731]\n",
      "27 [D loss: 0.387143, acc.: 85.16%] [G loss: 1.595675]\n",
      "28 [D loss: 0.332070, acc.: 86.72%] [G loss: 1.735825]\n",
      "29 [D loss: 0.284270, acc.: 92.58%] [G loss: 1.811068]\n",
      "30 [D loss: 0.228864, acc.: 94.92%] [G loss: 2.051476]\n",
      "31 [D loss: 0.197000, acc.: 96.88%] [G loss: 2.386307]\n",
      "32 [D loss: 0.165787, acc.: 98.44%] [G loss: 2.444260]\n",
      "33 [D loss: 0.128305, acc.: 98.05%] [G loss: 2.791069]\n",
      "34 [D loss: 0.096157, acc.: 100.00%] [G loss: 2.951388]\n",
      "35 [D loss: 0.092658, acc.: 99.61%] [G loss: 2.961294]\n",
      "36 [D loss: 0.075793, acc.: 100.00%] [G loss: 3.038558]\n",
      "37 [D loss: 0.059813, acc.: 100.00%] [G loss: 3.186453]\n",
      "38 [D loss: 0.054709, acc.: 100.00%] [G loss: 3.120685]\n",
      "39 [D loss: 0.051583, acc.: 100.00%] [G loss: 3.249078]\n",
      "40 [D loss: 0.055705, acc.: 100.00%] [G loss: 3.086243]\n",
      "41 [D loss: 0.064890, acc.: 100.00%] [G loss: 3.282156]\n",
      "42 [D loss: 0.060065, acc.: 100.00%] [G loss: 3.209197]\n",
      "43 [D loss: 0.055369, acc.: 99.61%] [G loss: 3.325214]\n",
      "44 [D loss: 0.050170, acc.: 100.00%] [G loss: 3.305627]\n",
      "45 [D loss: 0.048750, acc.: 100.00%] [G loss: 3.304988]\n",
      "46 [D loss: 0.049045, acc.: 100.00%] [G loss: 3.440193]\n",
      "47 [D loss: 0.047234, acc.: 100.00%] [G loss: 3.658883]\n",
      "48 [D loss: 0.042790, acc.: 100.00%] [G loss: 3.623797]\n",
      "49 [D loss: 0.050712, acc.: 100.00%] [G loss: 3.578282]\n",
      "50 [D loss: 0.041408, acc.: 100.00%] [G loss: 3.492279]\n",
      "51 [D loss: 0.043571, acc.: 100.00%] [G loss: 3.653613]\n",
      "52 [D loss: 0.042114, acc.: 100.00%] [G loss: 3.762319]\n",
      "53 [D loss: 0.040819, acc.: 100.00%] [G loss: 3.583038]\n",
      "54 [D loss: 0.043571, acc.: 100.00%] [G loss: 3.681803]\n",
      "55 [D loss: 0.038017, acc.: 100.00%] [G loss: 3.724417]\n",
      "56 [D loss: 0.037550, acc.: 100.00%] [G loss: 3.712912]\n",
      "57 [D loss: 0.037255, acc.: 100.00%] [G loss: 3.805376]\n",
      "58 [D loss: 0.031311, acc.: 100.00%] [G loss: 3.865342]\n",
      "59 [D loss: 0.039616, acc.: 100.00%] [G loss: 3.854204]\n",
      "60 [D loss: 0.035031, acc.: 100.00%] [G loss: 3.911391]\n",
      "61 [D loss: 0.032999, acc.: 100.00%] [G loss: 3.916516]\n",
      "62 [D loss: 0.035678, acc.: 100.00%] [G loss: 3.824674]\n",
      "63 [D loss: 0.036168, acc.: 100.00%] [G loss: 3.960224]\n",
      "64 [D loss: 0.033782, acc.: 100.00%] [G loss: 3.823154]\n",
      "65 [D loss: 0.036967, acc.: 100.00%] [G loss: 3.896867]\n",
      "66 [D loss: 0.035630, acc.: 100.00%] [G loss: 4.021812]\n",
      "67 [D loss: 0.034530, acc.: 100.00%] [G loss: 4.034285]\n",
      "68 [D loss: 0.032165, acc.: 100.00%] [G loss: 4.002479]\n",
      "69 [D loss: 0.031550, acc.: 100.00%] [G loss: 3.992874]\n",
      "70 [D loss: 0.034807, acc.: 100.00%] [G loss: 4.071203]\n",
      "71 [D loss: 0.032442, acc.: 100.00%] [G loss: 4.078887]\n",
      "72 [D loss: 0.032432, acc.: 100.00%] [G loss: 4.162179]\n",
      "73 [D loss: 0.031144, acc.: 100.00%] [G loss: 4.138685]\n",
      "74 [D loss: 0.028599, acc.: 100.00%] [G loss: 4.209461]\n",
      "75 [D loss: 0.027030, acc.: 100.00%] [G loss: 4.331750]\n",
      "76 [D loss: 0.023081, acc.: 100.00%] [G loss: 4.554205]\n",
      "77 [D loss: 0.022367, acc.: 100.00%] [G loss: 4.434079]\n",
      "78 [D loss: 0.023783, acc.: 100.00%] [G loss: 4.486955]\n",
      "79 [D loss: 0.023814, acc.: 100.00%] [G loss: 4.645451]\n",
      "80 [D loss: 0.018527, acc.: 100.00%] [G loss: 4.481907]\n",
      "81 [D loss: 0.022658, acc.: 100.00%] [G loss: 4.559613]\n",
      "82 [D loss: 0.026799, acc.: 100.00%] [G loss: 4.570189]\n",
      "83 [D loss: 0.016187, acc.: 100.00%] [G loss: 4.717319]\n",
      "84 [D loss: 0.018358, acc.: 100.00%] [G loss: 4.794083]\n",
      "85 [D loss: 0.014093, acc.: 100.00%] [G loss: 5.024694]\n",
      "86 [D loss: 0.017382, acc.: 100.00%] [G loss: 4.888378]\n",
      "87 [D loss: 0.014753, acc.: 100.00%] [G loss: 4.937923]\n",
      "88 [D loss: 0.014204, acc.: 100.00%] [G loss: 4.895289]\n",
      "89 [D loss: 0.013175, acc.: 100.00%] [G loss: 4.856159]\n",
      "90 [D loss: 0.011339, acc.: 100.00%] [G loss: 4.993251]\n",
      "91 [D loss: 0.011139, acc.: 100.00%] [G loss: 5.010511]\n",
      "92 [D loss: 0.009347, acc.: 100.00%] [G loss: 5.015991]\n",
      "93 [D loss: 0.008515, acc.: 100.00%] [G loss: 5.319666]\n",
      "94 [D loss: 0.006769, acc.: 100.00%] [G loss: 5.307826]\n",
      "95 [D loss: 0.006697, acc.: 100.00%] [G loss: 5.235776]\n",
      "96 [D loss: 0.008226, acc.: 100.00%] [G loss: 5.405909]\n",
      "97 [D loss: 0.006953, acc.: 100.00%] [G loss: 5.476202]\n",
      "98 [D loss: 0.004977, acc.: 100.00%] [G loss: 5.466580]\n",
      "99 [D loss: 0.006713, acc.: 100.00%] [G loss: 5.410035]\n",
      "100 [D loss: 0.006578, acc.: 100.00%] [G loss: 5.458101]\n",
      "101 [D loss: 0.007729, acc.: 100.00%] [G loss: 5.454049]\n",
      "102 [D loss: 0.005118, acc.: 100.00%] [G loss: 5.382669]\n",
      "103 [D loss: 0.006589, acc.: 100.00%] [G loss: 5.394032]\n",
      "104 [D loss: 0.006200, acc.: 100.00%] [G loss: 5.315794]\n",
      "105 [D loss: 0.005210, acc.: 100.00%] [G loss: 5.302099]\n",
      "106 [D loss: 0.006717, acc.: 100.00%] [G loss: 5.488824]\n",
      "107 [D loss: 0.005613, acc.: 100.00%] [G loss: 5.604991]\n",
      "108 [D loss: 0.005082, acc.: 100.00%] [G loss: 5.398312]\n",
      "109 [D loss: 0.005851, acc.: 100.00%] [G loss: 5.486496]\n",
      "110 [D loss: 0.006619, acc.: 100.00%] [G loss: 5.395672]\n",
      "111 [D loss: 0.005915, acc.: 100.00%] [G loss: 5.693069]\n",
      "112 [D loss: 0.005816, acc.: 100.00%] [G loss: 5.574759]\n",
      "113 [D loss: 0.006131, acc.: 100.00%] [G loss: 5.612435]\n",
      "114 [D loss: 0.004824, acc.: 100.00%] [G loss: 5.569382]\n",
      "115 [D loss: 0.004834, acc.: 100.00%] [G loss: 5.618349]\n",
      "116 [D loss: 0.005156, acc.: 100.00%] [G loss: 5.624844]\n",
      "117 [D loss: 0.006222, acc.: 100.00%] [G loss: 5.777252]\n",
      "118 [D loss: 0.006937, acc.: 100.00%] [G loss: 5.736397]\n",
      "119 [D loss: 0.006211, acc.: 100.00%] [G loss: 5.769958]\n",
      "120 [D loss: 0.008634, acc.: 100.00%] [G loss: 5.750093]\n",
      "121 [D loss: 0.006766, acc.: 100.00%] [G loss: 5.630752]\n",
      "122 [D loss: 0.007458, acc.: 100.00%] [G loss: 5.685474]\n",
      "123 [D loss: 0.006251, acc.: 100.00%] [G loss: 5.680423]\n",
      "124 [D loss: 0.009097, acc.: 100.00%] [G loss: 5.530863]\n",
      "125 [D loss: 0.006221, acc.: 100.00%] [G loss: 5.775324]\n",
      "126 [D loss: 0.005302, acc.: 100.00%] [G loss: 5.883535]\n",
      "127 [D loss: 0.005837, acc.: 100.00%] [G loss: 5.877734]\n",
      "128 [D loss: 0.006199, acc.: 100.00%] [G loss: 5.782044]\n",
      "129 [D loss: 0.004929, acc.: 100.00%] [G loss: 6.022735]\n",
      "130 [D loss: 0.006978, acc.: 100.00%] [G loss: 5.889774]\n",
      "131 [D loss: 0.005907, acc.: 100.00%] [G loss: 5.895869]\n",
      "132 [D loss: 0.006111, acc.: 100.00%] [G loss: 5.690144]\n",
      "133 [D loss: 0.006191, acc.: 100.00%] [G loss: 5.958435]\n",
      "134 [D loss: 0.009132, acc.: 100.00%] [G loss: 5.694156]\n",
      "135 [D loss: 0.007183, acc.: 100.00%] [G loss: 5.879833]\n",
      "136 [D loss: 0.007947, acc.: 100.00%] [G loss: 5.515937]\n",
      "137 [D loss: 0.006588, acc.: 100.00%] [G loss: 5.472955]\n",
      "138 [D loss: 0.008013, acc.: 100.00%] [G loss: 5.506740]\n",
      "139 [D loss: 0.007250, acc.: 100.00%] [G loss: 5.659050]\n",
      "140 [D loss: 0.008789, acc.: 100.00%] [G loss: 5.720241]\n",
      "141 [D loss: 0.007116, acc.: 100.00%] [G loss: 5.668066]\n",
      "142 [D loss: 0.008974, acc.: 100.00%] [G loss: 5.635942]\n",
      "143 [D loss: 0.010547, acc.: 100.00%] [G loss: 5.444666]\n",
      "144 [D loss: 0.006820, acc.: 100.00%] [G loss: 5.409066]\n",
      "145 [D loss: 0.009189, acc.: 100.00%] [G loss: 5.594874]\n",
      "146 [D loss: 0.013437, acc.: 100.00%] [G loss: 5.378134]\n",
      "147 [D loss: 0.010132, acc.: 100.00%] [G loss: 5.556337]\n",
      "148 [D loss: 0.009796, acc.: 100.00%] [G loss: 5.590498]\n",
      "149 [D loss: 0.009081, acc.: 100.00%] [G loss: 5.747907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.009086, acc.: 100.00%] [G loss: 5.661693]\n",
      "151 [D loss: 0.010081, acc.: 100.00%] [G loss: 5.505837]\n",
      "152 [D loss: 0.011247, acc.: 100.00%] [G loss: 5.356610]\n",
      "153 [D loss: 0.011699, acc.: 100.00%] [G loss: 5.224894]\n",
      "154 [D loss: 0.013641, acc.: 100.00%] [G loss: 5.028386]\n",
      "155 [D loss: 0.013060, acc.: 100.00%] [G loss: 5.056720]\n",
      "156 [D loss: 0.015436, acc.: 100.00%] [G loss: 5.191667]\n",
      "157 [D loss: 0.015532, acc.: 100.00%] [G loss: 5.198680]\n",
      "158 [D loss: 0.015750, acc.: 100.00%] [G loss: 5.061482]\n",
      "159 [D loss: 0.019354, acc.: 99.61%] [G loss: 5.284676]\n",
      "160 [D loss: 0.011670, acc.: 100.00%] [G loss: 5.379464]\n",
      "161 [D loss: 0.015175, acc.: 100.00%] [G loss: 5.618203]\n",
      "162 [D loss: 0.008695, acc.: 100.00%] [G loss: 5.430086]\n",
      "163 [D loss: 0.012093, acc.: 100.00%] [G loss: 5.277867]\n",
      "164 [D loss: 0.011534, acc.: 100.00%] [G loss: 5.133811]\n",
      "165 [D loss: 0.015996, acc.: 99.61%] [G loss: 5.208025]\n",
      "166 [D loss: 0.014212, acc.: 100.00%] [G loss: 5.349712]\n",
      "167 [D loss: 0.011878, acc.: 100.00%] [G loss: 5.199132]\n",
      "168 [D loss: 0.010224, acc.: 100.00%] [G loss: 5.406397]\n",
      "169 [D loss: 0.009532, acc.: 100.00%] [G loss: 5.161120]\n",
      "170 [D loss: 0.010782, acc.: 100.00%] [G loss: 5.446717]\n",
      "171 [D loss: 0.010159, acc.: 100.00%] [G loss: 5.283428]\n",
      "172 [D loss: 0.011030, acc.: 100.00%] [G loss: 5.499408]\n",
      "173 [D loss: 0.008291, acc.: 100.00%] [G loss: 5.363588]\n",
      "174 [D loss: 0.010426, acc.: 100.00%] [G loss: 5.635343]\n",
      "175 [D loss: 0.009649, acc.: 100.00%] [G loss: 5.359045]\n",
      "176 [D loss: 0.009277, acc.: 100.00%] [G loss: 5.303578]\n",
      "177 [D loss: 0.009496, acc.: 100.00%] [G loss: 5.477286]\n",
      "178 [D loss: 0.008976, acc.: 100.00%] [G loss: 5.315149]\n",
      "179 [D loss: 0.009308, acc.: 100.00%] [G loss: 5.369938]\n",
      "180 [D loss: 0.008498, acc.: 100.00%] [G loss: 5.489351]\n",
      "181 [D loss: 0.009079, acc.: 100.00%] [G loss: 5.225397]\n",
      "182 [D loss: 0.008390, acc.: 100.00%] [G loss: 5.091717]\n",
      "183 [D loss: 0.008770, acc.: 100.00%] [G loss: 5.217686]\n",
      "184 [D loss: 0.008051, acc.: 100.00%] [G loss: 5.187800]\n",
      "185 [D loss: 0.008960, acc.: 100.00%] [G loss: 5.107041]\n",
      "186 [D loss: 0.007179, acc.: 100.00%] [G loss: 5.131554]\n",
      "187 [D loss: 0.009208, acc.: 100.00%] [G loss: 5.122424]\n",
      "188 [D loss: 0.008905, acc.: 100.00%] [G loss: 5.244406]\n",
      "189 [D loss: 0.009340, acc.: 100.00%] [G loss: 4.946024]\n",
      "190 [D loss: 0.011246, acc.: 100.00%] [G loss: 5.093628]\n",
      "191 [D loss: 0.010951, acc.: 100.00%] [G loss: 5.092506]\n",
      "192 [D loss: 0.011657, acc.: 100.00%] [G loss: 4.965148]\n",
      "193 [D loss: 0.009873, acc.: 100.00%] [G loss: 4.751055]\n",
      "194 [D loss: 0.013568, acc.: 100.00%] [G loss: 5.085026]\n",
      "195 [D loss: 0.011695, acc.: 100.00%] [G loss: 4.907431]\n",
      "196 [D loss: 0.017741, acc.: 100.00%] [G loss: 4.706825]\n",
      "197 [D loss: 0.014021, acc.: 100.00%] [G loss: 4.598713]\n",
      "198 [D loss: 0.016327, acc.: 100.00%] [G loss: 4.738394]\n",
      "199 [D loss: 0.019858, acc.: 100.00%] [G loss: 4.584352]\n",
      "200 [D loss: 0.018428, acc.: 100.00%] [G loss: 4.591219]\n",
      "201 [D loss: 0.018729, acc.: 100.00%] [G loss: 4.757833]\n",
      "202 [D loss: 0.020261, acc.: 100.00%] [G loss: 4.811918]\n",
      "203 [D loss: 0.023505, acc.: 100.00%] [G loss: 4.683149]\n",
      "204 [D loss: 0.019674, acc.: 100.00%] [G loss: 4.642527]\n",
      "205 [D loss: 0.020799, acc.: 100.00%] [G loss: 4.684557]\n",
      "206 [D loss: 0.017518, acc.: 100.00%] [G loss: 4.424515]\n",
      "207 [D loss: 0.022600, acc.: 100.00%] [G loss: 4.580282]\n",
      "208 [D loss: 0.021513, acc.: 100.00%] [G loss: 4.507362]\n",
      "209 [D loss: 0.022937, acc.: 100.00%] [G loss: 4.218751]\n",
      "210 [D loss: 0.026816, acc.: 100.00%] [G loss: 4.362275]\n",
      "211 [D loss: 0.025331, acc.: 100.00%] [G loss: 4.181096]\n",
      "212 [D loss: 0.029229, acc.: 100.00%] [G loss: 4.323045]\n",
      "213 [D loss: 0.033931, acc.: 100.00%] [G loss: 4.150215]\n",
      "214 [D loss: 0.029228, acc.: 100.00%] [G loss: 4.317440]\n",
      "215 [D loss: 0.033319, acc.: 100.00%] [G loss: 3.910569]\n",
      "216 [D loss: 0.034900, acc.: 100.00%] [G loss: 3.887926]\n",
      "217 [D loss: 0.036361, acc.: 100.00%] [G loss: 4.025320]\n",
      "218 [D loss: 0.032488, acc.: 100.00%] [G loss: 3.892301]\n",
      "219 [D loss: 0.036005, acc.: 100.00%] [G loss: 3.473197]\n",
      "220 [D loss: 0.035138, acc.: 100.00%] [G loss: 3.799120]\n",
      "221 [D loss: 0.035446, acc.: 100.00%] [G loss: 4.019567]\n",
      "222 [D loss: 0.038407, acc.: 100.00%] [G loss: 3.722028]\n",
      "223 [D loss: 0.043889, acc.: 100.00%] [G loss: 3.587505]\n",
      "224 [D loss: 0.047736, acc.: 100.00%] [G loss: 3.514419]\n",
      "225 [D loss: 0.040083, acc.: 100.00%] [G loss: 3.540756]\n",
      "226 [D loss: 0.043418, acc.: 100.00%] [G loss: 3.426369]\n",
      "227 [D loss: 0.045811, acc.: 100.00%] [G loss: 3.572580]\n",
      "228 [D loss: 0.042931, acc.: 100.00%] [G loss: 3.605469]\n",
      "229 [D loss: 0.037707, acc.: 100.00%] [G loss: 3.474914]\n",
      "230 [D loss: 0.037904, acc.: 100.00%] [G loss: 3.525137]\n",
      "231 [D loss: 0.043803, acc.: 100.00%] [G loss: 3.335518]\n",
      "232 [D loss: 0.046897, acc.: 100.00%] [G loss: 3.327505]\n",
      "233 [D loss: 0.045775, acc.: 100.00%] [G loss: 3.324378]\n",
      "234 [D loss: 0.051001, acc.: 100.00%] [G loss: 3.131754]\n",
      "235 [D loss: 0.046731, acc.: 100.00%] [G loss: 3.366249]\n",
      "236 [D loss: 0.049545, acc.: 100.00%] [G loss: 3.245767]\n",
      "237 [D loss: 0.046191, acc.: 100.00%] [G loss: 3.321984]\n",
      "238 [D loss: 0.049236, acc.: 100.00%] [G loss: 3.431111]\n",
      "239 [D loss: 0.050108, acc.: 100.00%] [G loss: 3.175126]\n",
      "240 [D loss: 0.048217, acc.: 100.00%] [G loss: 3.077779]\n",
      "241 [D loss: 0.053255, acc.: 100.00%] [G loss: 3.069188]\n",
      "242 [D loss: 0.042029, acc.: 100.00%] [G loss: 3.053955]\n",
      "243 [D loss: 0.046196, acc.: 100.00%] [G loss: 3.466769]\n",
      "244 [D loss: 0.046328, acc.: 100.00%] [G loss: 3.204359]\n",
      "245 [D loss: 0.052140, acc.: 99.22%] [G loss: 3.149324]\n",
      "246 [D loss: 0.045151, acc.: 100.00%] [G loss: 3.225004]\n",
      "247 [D loss: 0.050851, acc.: 100.00%] [G loss: 2.988973]\n",
      "248 [D loss: 0.056891, acc.: 100.00%] [G loss: 2.891847]\n",
      "249 [D loss: 0.056461, acc.: 100.00%] [G loss: 3.009469]\n",
      "250 [D loss: 0.049285, acc.: 100.00%] [G loss: 2.798991]\n",
      "251 [D loss: 0.054309, acc.: 100.00%] [G loss: 2.852212]\n",
      "252 [D loss: 0.046418, acc.: 100.00%] [G loss: 2.802618]\n",
      "253 [D loss: 0.049359, acc.: 100.00%] [G loss: 2.769831]\n",
      "254 [D loss: 0.061010, acc.: 100.00%] [G loss: 2.804749]\n",
      "255 [D loss: 0.050742, acc.: 100.00%] [G loss: 2.871654]\n",
      "256 [D loss: 0.056977, acc.: 100.00%] [G loss: 2.835839]\n",
      "257 [D loss: 0.058601, acc.: 100.00%] [G loss: 2.775789]\n",
      "258 [D loss: 0.058870, acc.: 100.00%] [G loss: 2.752160]\n",
      "259 [D loss: 0.053669, acc.: 100.00%] [G loss: 2.800017]\n",
      "260 [D loss: 0.057604, acc.: 100.00%] [G loss: 2.789033]\n",
      "261 [D loss: 0.057272, acc.: 99.61%] [G loss: 2.686440]\n",
      "262 [D loss: 0.060770, acc.: 100.00%] [G loss: 2.597431]\n",
      "263 [D loss: 0.061595, acc.: 100.00%] [G loss: 2.587873]\n",
      "264 [D loss: 0.061298, acc.: 100.00%] [G loss: 2.689773]\n",
      "265 [D loss: 0.066117, acc.: 100.00%] [G loss: 2.546484]\n",
      "266 [D loss: 0.055655, acc.: 100.00%] [G loss: 2.420232]\n",
      "267 [D loss: 0.059798, acc.: 100.00%] [G loss: 2.568748]\n",
      "268 [D loss: 0.056542, acc.: 100.00%] [G loss: 2.579331]\n",
      "269 [D loss: 0.060209, acc.: 100.00%] [G loss: 2.480019]\n",
      "270 [D loss: 0.062899, acc.: 100.00%] [G loss: 2.463624]\n",
      "271 [D loss: 0.060049, acc.: 100.00%] [G loss: 2.447268]\n",
      "272 [D loss: 0.059032, acc.: 100.00%] [G loss: 2.461031]\n",
      "273 [D loss: 0.065411, acc.: 100.00%] [G loss: 2.419923]\n",
      "274 [D loss: 0.068951, acc.: 100.00%] [G loss: 2.396170]\n",
      "275 [D loss: 0.060058, acc.: 100.00%] [G loss: 2.418653]\n",
      "276 [D loss: 0.065902, acc.: 100.00%] [G loss: 2.284404]\n",
      "277 [D loss: 0.069527, acc.: 100.00%] [G loss: 2.317127]\n",
      "278 [D loss: 0.067087, acc.: 100.00%] [G loss: 2.297449]\n",
      "279 [D loss: 0.063704, acc.: 100.00%] [G loss: 2.271229]\n",
      "280 [D loss: 0.068846, acc.: 100.00%] [G loss: 2.287529]\n",
      "281 [D loss: 0.071299, acc.: 100.00%] [G loss: 2.311848]\n",
      "282 [D loss: 0.070562, acc.: 100.00%] [G loss: 2.276161]\n",
      "283 [D loss: 0.072870, acc.: 100.00%] [G loss: 2.251908]\n",
      "284 [D loss: 0.074369, acc.: 100.00%] [G loss: 2.238101]\n",
      "285 [D loss: 0.070788, acc.: 100.00%] [G loss: 2.217608]\n",
      "286 [D loss: 0.069259, acc.: 100.00%] [G loss: 2.236903]\n",
      "287 [D loss: 0.067526, acc.: 100.00%] [G loss: 2.210362]\n",
      "288 [D loss: 0.065799, acc.: 100.00%] [G loss: 2.242809]\n",
      "289 [D loss: 0.069290, acc.: 100.00%] [G loss: 2.268206]\n",
      "290 [D loss: 0.066465, acc.: 100.00%] [G loss: 2.257628]\n",
      "291 [D loss: 0.068418, acc.: 100.00%] [G loss: 2.306955]\n",
      "292 [D loss: 0.063130, acc.: 100.00%] [G loss: 2.288475]\n",
      "293 [D loss: 0.062372, acc.: 100.00%] [G loss: 2.275941]\n",
      "294 [D loss: 0.061592, acc.: 100.00%] [G loss: 2.300517]\n",
      "295 [D loss: 0.062311, acc.: 100.00%] [G loss: 2.346936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 0.062916, acc.: 100.00%] [G loss: 2.332694]\n",
      "297 [D loss: 0.068883, acc.: 99.61%] [G loss: 2.319421]\n",
      "298 [D loss: 0.064677, acc.: 100.00%] [G loss: 2.308158]\n",
      "299 [D loss: 0.064334, acc.: 99.61%] [G loss: 2.293146]\n",
      "300 [D loss: 0.064444, acc.: 100.00%] [G loss: 2.282376]\n",
      "301 [D loss: 0.061227, acc.: 100.00%] [G loss: 2.300202]\n",
      "302 [D loss: 0.061059, acc.: 100.00%] [G loss: 2.311715]\n",
      "303 [D loss: 0.060406, acc.: 100.00%] [G loss: 2.338560]\n",
      "304 [D loss: 0.058862, acc.: 100.00%] [G loss: 2.417393]\n",
      "305 [D loss: 0.056909, acc.: 100.00%] [G loss: 2.372139]\n",
      "306 [D loss: 0.057509, acc.: 100.00%] [G loss: 2.428709]\n",
      "307 [D loss: 0.061683, acc.: 100.00%] [G loss: 2.433014]\n",
      "308 [D loss: 0.057318, acc.: 100.00%] [G loss: 2.427903]\n",
      "309 [D loss: 0.056421, acc.: 100.00%] [G loss: 2.398629]\n",
      "310 [D loss: 0.059191, acc.: 100.00%] [G loss: 2.415679]\n",
      "311 [D loss: 0.054647, acc.: 100.00%] [G loss: 2.418190]\n",
      "312 [D loss: 0.054907, acc.: 100.00%] [G loss: 2.467316]\n",
      "313 [D loss: 0.053830, acc.: 100.00%] [G loss: 2.456238]\n",
      "314 [D loss: 0.052078, acc.: 100.00%] [G loss: 2.453123]\n",
      "315 [D loss: 0.050678, acc.: 100.00%] [G loss: 2.519868]\n",
      "316 [D loss: 0.050842, acc.: 100.00%] [G loss: 2.499382]\n",
      "317 [D loss: 0.052120, acc.: 100.00%] [G loss: 2.559669]\n",
      "318 [D loss: 0.051673, acc.: 100.00%] [G loss: 2.542970]\n",
      "319 [D loss: 0.052415, acc.: 100.00%] [G loss: 2.539344]\n",
      "320 [D loss: 0.053686, acc.: 100.00%] [G loss: 2.503835]\n",
      "321 [D loss: 0.052358, acc.: 100.00%] [G loss: 2.530416]\n",
      "322 [D loss: 0.052831, acc.: 100.00%] [G loss: 2.490667]\n",
      "323 [D loss: 0.053860, acc.: 100.00%] [G loss: 2.562219]\n",
      "324 [D loss: 0.052016, acc.: 100.00%] [G loss: 2.583307]\n",
      "325 [D loss: 0.050452, acc.: 100.00%] [G loss: 2.558494]\n",
      "326 [D loss: 0.049425, acc.: 100.00%] [G loss: 2.586091]\n",
      "327 [D loss: 0.046738, acc.: 100.00%] [G loss: 2.614006]\n",
      "328 [D loss: 0.046254, acc.: 100.00%] [G loss: 2.595387]\n",
      "329 [D loss: 0.048589, acc.: 100.00%] [G loss: 2.623866]\n",
      "330 [D loss: 0.045279, acc.: 100.00%] [G loss: 2.638900]\n",
      "331 [D loss: 0.041875, acc.: 100.00%] [G loss: 2.718992]\n",
      "332 [D loss: 0.040651, acc.: 100.00%] [G loss: 2.730460]\n",
      "333 [D loss: 0.037482, acc.: 100.00%] [G loss: 2.821257]\n",
      "334 [D loss: 0.034593, acc.: 100.00%] [G loss: 2.880621]\n",
      "335 [D loss: 0.034807, acc.: 100.00%] [G loss: 2.954737]\n",
      "336 [D loss: 0.031413, acc.: 100.00%] [G loss: 2.977549]\n",
      "337 [D loss: 0.030720, acc.: 100.00%] [G loss: 3.030702]\n",
      "338 [D loss: 0.032226, acc.: 100.00%] [G loss: 3.025113]\n",
      "339 [D loss: 0.031238, acc.: 100.00%] [G loss: 3.026237]\n",
      "340 [D loss: 0.029063, acc.: 100.00%] [G loss: 3.025721]\n",
      "341 [D loss: 0.028560, acc.: 100.00%] [G loss: 3.026342]\n",
      "342 [D loss: 0.028314, acc.: 100.00%] [G loss: 3.073662]\n",
      "343 [D loss: 0.028568, acc.: 100.00%] [G loss: 3.099382]\n",
      "344 [D loss: 0.031215, acc.: 100.00%] [G loss: 3.069298]\n",
      "345 [D loss: 0.027361, acc.: 100.00%] [G loss: 3.072301]\n",
      "346 [D loss: 0.029578, acc.: 100.00%] [G loss: 3.075929]\n",
      "347 [D loss: 0.030858, acc.: 100.00%] [G loss: 3.057737]\n",
      "348 [D loss: 0.030757, acc.: 100.00%] [G loss: 3.038570]\n",
      "349 [D loss: 0.030505, acc.: 100.00%] [G loss: 3.023473]\n",
      "350 [D loss: 0.031336, acc.: 100.00%] [G loss: 2.987622]\n",
      "351 [D loss: 0.032156, acc.: 100.00%] [G loss: 2.935057]\n",
      "352 [D loss: 0.033808, acc.: 100.00%] [G loss: 2.941630]\n",
      "353 [D loss: 0.035480, acc.: 100.00%] [G loss: 2.931218]\n",
      "354 [D loss: 0.034517, acc.: 100.00%] [G loss: 2.941075]\n",
      "355 [D loss: 0.036719, acc.: 100.00%] [G loss: 2.917101]\n",
      "356 [D loss: 0.039178, acc.: 100.00%] [G loss: 2.841998]\n",
      "357 [D loss: 0.038555, acc.: 100.00%] [G loss: 2.838099]\n",
      "358 [D loss: 0.041196, acc.: 100.00%] [G loss: 2.905876]\n",
      "359 [D loss: 0.040080, acc.: 100.00%] [G loss: 2.870911]\n",
      "360 [D loss: 0.039739, acc.: 100.00%] [G loss: 2.850868]\n",
      "361 [D loss: 0.041745, acc.: 100.00%] [G loss: 2.867117]\n",
      "362 [D loss: 0.042605, acc.: 100.00%] [G loss: 2.832293]\n",
      "363 [D loss: 0.042987, acc.: 100.00%] [G loss: 2.849154]\n",
      "364 [D loss: 0.043002, acc.: 100.00%] [G loss: 2.805926]\n",
      "365 [D loss: 0.043935, acc.: 100.00%] [G loss: 2.844105]\n",
      "366 [D loss: 0.045916, acc.: 100.00%] [G loss: 2.809085]\n",
      "367 [D loss: 0.043493, acc.: 100.00%] [G loss: 2.846914]\n",
      "368 [D loss: 0.040760, acc.: 100.00%] [G loss: 2.860768]\n",
      "369 [D loss: 0.046071, acc.: 100.00%] [G loss: 2.779201]\n",
      "370 [D loss: 0.046832, acc.: 100.00%] [G loss: 2.805533]\n",
      "371 [D loss: 0.045567, acc.: 100.00%] [G loss: 2.760287]\n",
      "372 [D loss: 0.044331, acc.: 100.00%] [G loss: 2.843583]\n",
      "373 [D loss: 0.042601, acc.: 100.00%] [G loss: 2.832938]\n",
      "374 [D loss: 0.041828, acc.: 100.00%] [G loss: 2.864626]\n",
      "375 [D loss: 0.040256, acc.: 100.00%] [G loss: 2.999006]\n",
      "376 [D loss: 0.041577, acc.: 100.00%] [G loss: 3.064589]\n",
      "377 [D loss: 0.037513, acc.: 100.00%] [G loss: 3.020539]\n",
      "378 [D loss: 0.038441, acc.: 100.00%] [G loss: 3.045857]\n",
      "379 [D loss: 0.034643, acc.: 100.00%] [G loss: 3.034075]\n",
      "380 [D loss: 0.037005, acc.: 100.00%] [G loss: 3.087614]\n",
      "381 [D loss: 0.032258, acc.: 100.00%] [G loss: 3.137810]\n",
      "382 [D loss: 0.032321, acc.: 100.00%] [G loss: 3.144725]\n",
      "383 [D loss: 0.032406, acc.: 100.00%] [G loss: 3.250427]\n",
      "384 [D loss: 0.034373, acc.: 100.00%] [G loss: 3.253062]\n",
      "385 [D loss: 0.029571, acc.: 100.00%] [G loss: 3.259475]\n",
      "386 [D loss: 0.032210, acc.: 100.00%] [G loss: 3.211007]\n",
      "387 [D loss: 0.029737, acc.: 100.00%] [G loss: 3.375452]\n",
      "388 [D loss: 0.026637, acc.: 100.00%] [G loss: 3.287509]\n",
      "389 [D loss: 0.026817, acc.: 100.00%] [G loss: 3.321221]\n",
      "390 [D loss: 0.031843, acc.: 100.00%] [G loss: 3.311395]\n",
      "391 [D loss: 0.025523, acc.: 100.00%] [G loss: 3.395646]\n",
      "392 [D loss: 0.024813, acc.: 100.00%] [G loss: 3.431424]\n",
      "393 [D loss: 0.023164, acc.: 100.00%] [G loss: 3.490560]\n",
      "394 [D loss: 0.023990, acc.: 100.00%] [G loss: 3.557139]\n",
      "395 [D loss: 0.021605, acc.: 100.00%] [G loss: 3.705694]\n",
      "396 [D loss: 0.025593, acc.: 100.00%] [G loss: 3.702327]\n",
      "397 [D loss: 0.022917, acc.: 100.00%] [G loss: 3.646092]\n",
      "398 [D loss: 0.020291, acc.: 100.00%] [G loss: 3.608271]\n",
      "399 [D loss: 0.022408, acc.: 100.00%] [G loss: 3.577804]\n",
      "400 [D loss: 0.022578, acc.: 100.00%] [G loss: 3.565569]\n",
      "401 [D loss: 0.022851, acc.: 100.00%] [G loss: 3.632705]\n",
      "402 [D loss: 0.020824, acc.: 100.00%] [G loss: 3.594424]\n",
      "403 [D loss: 0.021268, acc.: 100.00%] [G loss: 3.678005]\n",
      "404 [D loss: 0.022884, acc.: 100.00%] [G loss: 3.722547]\n",
      "405 [D loss: 0.021527, acc.: 100.00%] [G loss: 3.703104]\n",
      "406 [D loss: 0.025336, acc.: 100.00%] [G loss: 3.552174]\n",
      "407 [D loss: 0.028782, acc.: 100.00%] [G loss: 3.431699]\n",
      "408 [D loss: 0.027921, acc.: 100.00%] [G loss: 3.440426]\n",
      "409 [D loss: 0.030257, acc.: 100.00%] [G loss: 3.317750]\n",
      "410 [D loss: 0.044009, acc.: 99.61%] [G loss: 3.187963]\n",
      "411 [D loss: 0.033283, acc.: 100.00%] [G loss: 3.127800]\n",
      "412 [D loss: 0.036234, acc.: 100.00%] [G loss: 3.256065]\n",
      "413 [D loss: 0.034133, acc.: 100.00%] [G loss: 3.425347]\n",
      "414 [D loss: 0.033377, acc.: 100.00%] [G loss: 3.454144]\n",
      "415 [D loss: 0.031890, acc.: 100.00%] [G loss: 3.489369]\n",
      "416 [D loss: 0.033631, acc.: 100.00%] [G loss: 3.390040]\n",
      "417 [D loss: 0.031430, acc.: 100.00%] [G loss: 3.331334]\n",
      "418 [D loss: 0.030221, acc.: 100.00%] [G loss: 3.396621]\n",
      "419 [D loss: 0.029101, acc.: 100.00%] [G loss: 3.355878]\n",
      "420 [D loss: 0.024862, acc.: 100.00%] [G loss: 3.572117]\n",
      "421 [D loss: 0.028191, acc.: 100.00%] [G loss: 3.648635]\n",
      "422 [D loss: 0.028186, acc.: 100.00%] [G loss: 3.705485]\n",
      "423 [D loss: 0.028701, acc.: 100.00%] [G loss: 3.620674]\n",
      "424 [D loss: 0.024670, acc.: 100.00%] [G loss: 3.614120]\n",
      "425 [D loss: 0.027042, acc.: 100.00%] [G loss: 3.634320]\n",
      "426 [D loss: 0.019334, acc.: 100.00%] [G loss: 3.863760]\n",
      "427 [D loss: 0.023252, acc.: 100.00%] [G loss: 4.005686]\n",
      "428 [D loss: 0.023511, acc.: 100.00%] [G loss: 4.100603]\n",
      "429 [D loss: 0.021934, acc.: 100.00%] [G loss: 3.948550]\n",
      "430 [D loss: 0.023762, acc.: 99.61%] [G loss: 3.894454]\n",
      "431 [D loss: 0.023322, acc.: 100.00%] [G loss: 3.940415]\n",
      "432 [D loss: 0.024097, acc.: 100.00%] [G loss: 3.811522]\n",
      "433 [D loss: 0.030170, acc.: 99.61%] [G loss: 3.807958]\n",
      "434 [D loss: 0.023902, acc.: 100.00%] [G loss: 3.757026]\n",
      "435 [D loss: 0.027219, acc.: 100.00%] [G loss: 3.736480]\n",
      "436 [D loss: 0.027647, acc.: 100.00%] [G loss: 3.573499]\n",
      "437 [D loss: 0.039987, acc.: 100.00%] [G loss: 3.632615]\n",
      "438 [D loss: 0.039386, acc.: 100.00%] [G loss: 3.714279]\n",
      "439 [D loss: 0.042842, acc.: 99.61%] [G loss: 3.801756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440 [D loss: 0.033870, acc.: 100.00%] [G loss: 3.872492]\n",
      "441 [D loss: 0.031464, acc.: 99.22%] [G loss: 4.001492]\n",
      "442 [D loss: 0.026807, acc.: 100.00%] [G loss: 4.125230]\n",
      "443 [D loss: 0.032095, acc.: 100.00%] [G loss: 4.321135]\n",
      "444 [D loss: 0.021100, acc.: 100.00%] [G loss: 4.400486]\n",
      "445 [D loss: 0.028898, acc.: 100.00%] [G loss: 4.326350]\n",
      "446 [D loss: 0.024080, acc.: 100.00%] [G loss: 4.231606]\n",
      "447 [D loss: 0.020967, acc.: 100.00%] [G loss: 4.263068]\n",
      "448 [D loss: 0.018470, acc.: 100.00%] [G loss: 4.442733]\n",
      "449 [D loss: 0.024968, acc.: 100.00%] [G loss: 4.340350]\n",
      "450 [D loss: 0.017115, acc.: 100.00%] [G loss: 4.492451]\n",
      "451 [D loss: 0.025344, acc.: 99.61%] [G loss: 4.297660]\n",
      "452 [D loss: 0.019850, acc.: 100.00%] [G loss: 4.397995]\n",
      "453 [D loss: 0.028031, acc.: 100.00%] [G loss: 4.255454]\n",
      "454 [D loss: 0.026982, acc.: 99.61%] [G loss: 4.276411]\n",
      "455 [D loss: 0.030842, acc.: 100.00%] [G loss: 4.182442]\n",
      "456 [D loss: 0.037723, acc.: 99.61%] [G loss: 4.233121]\n",
      "457 [D loss: 0.043081, acc.: 99.61%] [G loss: 4.117708]\n",
      "458 [D loss: 0.035163, acc.: 100.00%] [G loss: 4.687806]\n",
      "459 [D loss: 0.044643, acc.: 99.61%] [G loss: 4.349148]\n",
      "460 [D loss: 0.036145, acc.: 100.00%] [G loss: 4.483098]\n",
      "461 [D loss: 0.031226, acc.: 100.00%] [G loss: 4.395144]\n",
      "462 [D loss: 0.040378, acc.: 99.61%] [G loss: 4.197134]\n",
      "463 [D loss: 0.037031, acc.: 100.00%] [G loss: 4.354065]\n",
      "464 [D loss: 0.042768, acc.: 99.61%] [G loss: 4.864851]\n",
      "465 [D loss: 0.024417, acc.: 100.00%] [G loss: 4.996914]\n",
      "466 [D loss: 0.030592, acc.: 99.22%] [G loss: 4.953611]\n",
      "467 [D loss: 0.029297, acc.: 99.61%] [G loss: 4.904497]\n",
      "468 [D loss: 0.034116, acc.: 99.22%] [G loss: 4.441635]\n",
      "469 [D loss: 0.026165, acc.: 100.00%] [G loss: 4.765233]\n",
      "470 [D loss: 0.029332, acc.: 100.00%] [G loss: 4.714371]\n",
      "471 [D loss: 0.024547, acc.: 100.00%] [G loss: 4.637973]\n",
      "472 [D loss: 0.023276, acc.: 100.00%] [G loss: 4.607525]\n",
      "473 [D loss: 0.020145, acc.: 100.00%] [G loss: 4.716144]\n",
      "474 [D loss: 0.021550, acc.: 100.00%] [G loss: 4.855533]\n",
      "475 [D loss: 0.023070, acc.: 100.00%] [G loss: 4.767403]\n",
      "476 [D loss: 0.029954, acc.: 100.00%] [G loss: 4.390296]\n",
      "477 [D loss: 0.034479, acc.: 100.00%] [G loss: 4.444983]\n",
      "478 [D loss: 0.025104, acc.: 100.00%] [G loss: 4.409628]\n",
      "479 [D loss: 0.029920, acc.: 99.61%] [G loss: 4.482374]\n",
      "480 [D loss: 0.034799, acc.: 99.61%] [G loss: 4.580305]\n",
      "481 [D loss: 0.023461, acc.: 100.00%] [G loss: 4.683161]\n",
      "482 [D loss: 0.025273, acc.: 100.00%] [G loss: 4.569250]\n",
      "483 [D loss: 0.023021, acc.: 100.00%] [G loss: 4.828226]\n",
      "484 [D loss: 0.029393, acc.: 98.83%] [G loss: 4.627525]\n",
      "485 [D loss: 0.022429, acc.: 100.00%] [G loss: 4.713275]\n",
      "486 [D loss: 0.026342, acc.: 100.00%] [G loss: 4.891949]\n",
      "487 [D loss: 0.026638, acc.: 100.00%] [G loss: 4.755768]\n",
      "488 [D loss: 0.036727, acc.: 100.00%] [G loss: 4.516150]\n",
      "489 [D loss: 0.024198, acc.: 100.00%] [G loss: 4.660732]\n",
      "490 [D loss: 0.036790, acc.: 100.00%] [G loss: 4.487147]\n",
      "491 [D loss: 0.033470, acc.: 100.00%] [G loss: 4.667187]\n",
      "492 [D loss: 0.043835, acc.: 100.00%] [G loss: 4.787239]\n",
      "493 [D loss: 0.032032, acc.: 99.61%] [G loss: 4.720166]\n",
      "494 [D loss: 0.035839, acc.: 100.00%] [G loss: 4.388712]\n",
      "495 [D loss: 0.025283, acc.: 100.00%] [G loss: 5.028821]\n",
      "496 [D loss: 0.026300, acc.: 99.61%] [G loss: 4.912860]\n",
      "497 [D loss: 0.019199, acc.: 100.00%] [G loss: 5.403950]\n",
      "498 [D loss: 0.030996, acc.: 99.22%] [G loss: 5.399359]\n",
      "499 [D loss: 0.036841, acc.: 99.61%] [G loss: 4.945864]\n",
      "500 [D loss: 0.033247, acc.: 99.61%] [G loss: 5.006187]\n",
      "501 [D loss: 0.045829, acc.: 98.44%] [G loss: 5.082099]\n",
      "502 [D loss: 0.027800, acc.: 100.00%] [G loss: 5.135371]\n",
      "503 [D loss: 0.020587, acc.: 99.61%] [G loss: 5.559880]\n",
      "504 [D loss: 0.026148, acc.: 100.00%] [G loss: 5.511081]\n",
      "505 [D loss: 0.035856, acc.: 98.83%] [G loss: 5.339787]\n",
      "506 [D loss: 0.024634, acc.: 100.00%] [G loss: 5.500208]\n",
      "507 [D loss: 0.026848, acc.: 99.22%] [G loss: 5.530562]\n",
      "508 [D loss: 0.018124, acc.: 100.00%] [G loss: 5.640803]\n",
      "509 [D loss: 0.018128, acc.: 100.00%] [G loss: 5.567501]\n",
      "510 [D loss: 0.018031, acc.: 100.00%] [G loss: 5.341517]\n",
      "511 [D loss: 0.029025, acc.: 100.00%] [G loss: 5.934566]\n",
      "512 [D loss: 0.016104, acc.: 100.00%] [G loss: 5.668769]\n",
      "513 [D loss: 0.025318, acc.: 100.00%] [G loss: 5.822590]\n",
      "514 [D loss: 0.020243, acc.: 100.00%] [G loss: 6.020893]\n",
      "515 [D loss: 0.021775, acc.: 100.00%] [G loss: 5.769861]\n",
      "516 [D loss: 0.016209, acc.: 100.00%] [G loss: 5.608277]\n",
      "517 [D loss: 0.014867, acc.: 100.00%] [G loss: 5.483496]\n",
      "518 [D loss: 0.019216, acc.: 100.00%] [G loss: 5.752229]\n",
      "519 [D loss: 0.017102, acc.: 99.61%] [G loss: 5.771827]\n",
      "520 [D loss: 0.014535, acc.: 100.00%] [G loss: 5.812249]\n",
      "521 [D loss: 0.012492, acc.: 100.00%] [G loss: 5.427040]\n",
      "522 [D loss: 0.014413, acc.: 100.00%] [G loss: 5.627105]\n",
      "523 [D loss: 0.012469, acc.: 100.00%] [G loss: 5.577084]\n",
      "524 [D loss: 0.011972, acc.: 100.00%] [G loss: 5.703409]\n",
      "525 [D loss: 0.011895, acc.: 100.00%] [G loss: 5.620320]\n",
      "526 [D loss: 0.013578, acc.: 100.00%] [G loss: 5.408517]\n",
      "527 [D loss: 0.011332, acc.: 100.00%] [G loss: 5.471280]\n",
      "528 [D loss: 0.017225, acc.: 100.00%] [G loss: 4.966800]\n",
      "529 [D loss: 0.013196, acc.: 100.00%] [G loss: 4.932703]\n",
      "530 [D loss: 0.012407, acc.: 100.00%] [G loss: 4.805997]\n",
      "531 [D loss: 0.012425, acc.: 100.00%] [G loss: 4.960177]\n",
      "532 [D loss: 0.012183, acc.: 100.00%] [G loss: 4.877967]\n",
      "533 [D loss: 0.013996, acc.: 100.00%] [G loss: 5.102962]\n",
      "534 [D loss: 0.011059, acc.: 100.00%] [G loss: 5.030628]\n",
      "535 [D loss: 0.014766, acc.: 99.61%] [G loss: 5.138909]\n",
      "536 [D loss: 0.014859, acc.: 100.00%] [G loss: 5.120196]\n",
      "537 [D loss: 0.015181, acc.: 100.00%] [G loss: 4.850947]\n",
      "538 [D loss: 0.014066, acc.: 100.00%] [G loss: 5.040853]\n",
      "539 [D loss: 0.013245, acc.: 100.00%] [G loss: 5.315774]\n",
      "540 [D loss: 0.010936, acc.: 100.00%] [G loss: 5.100677]\n",
      "541 [D loss: 0.014777, acc.: 100.00%] [G loss: 4.973202]\n",
      "542 [D loss: 0.016802, acc.: 100.00%] [G loss: 4.689040]\n",
      "543 [D loss: 0.015780, acc.: 100.00%] [G loss: 4.731998]\n",
      "544 [D loss: 0.018561, acc.: 100.00%] [G loss: 4.782825]\n",
      "545 [D loss: 0.011857, acc.: 100.00%] [G loss: 4.663339]\n",
      "546 [D loss: 0.017204, acc.: 100.00%] [G loss: 4.971901]\n",
      "547 [D loss: 0.014872, acc.: 100.00%] [G loss: 5.037157]\n",
      "548 [D loss: 0.017105, acc.: 100.00%] [G loss: 4.783291]\n",
      "549 [D loss: 0.017174, acc.: 100.00%] [G loss: 4.809589]\n",
      "550 [D loss: 0.023978, acc.: 100.00%] [G loss: 4.449339]\n",
      "551 [D loss: 0.033628, acc.: 100.00%] [G loss: 4.531412]\n",
      "552 [D loss: 0.027287, acc.: 100.00%] [G loss: 4.691011]\n",
      "553 [D loss: 0.017695, acc.: 100.00%] [G loss: 4.884108]\n",
      "554 [D loss: 0.022458, acc.: 100.00%] [G loss: 5.309507]\n",
      "555 [D loss: 0.025280, acc.: 100.00%] [G loss: 5.067354]\n",
      "556 [D loss: 0.025146, acc.: 100.00%] [G loss: 4.801205]\n",
      "557 [D loss: 0.022920, acc.: 100.00%] [G loss: 4.757152]\n",
      "558 [D loss: 0.028102, acc.: 100.00%] [G loss: 4.560102]\n",
      "559 [D loss: 0.034667, acc.: 99.61%] [G loss: 4.770930]\n",
      "560 [D loss: 0.027382, acc.: 100.00%] [G loss: 5.346212]\n",
      "561 [D loss: 0.050051, acc.: 98.83%] [G loss: 5.044633]\n",
      "562 [D loss: 0.035703, acc.: 100.00%] [G loss: 4.966756]\n",
      "563 [D loss: 0.025096, acc.: 100.00%] [G loss: 5.223954]\n",
      "564 [D loss: 0.023643, acc.: 100.00%] [G loss: 5.701663]\n",
      "565 [D loss: 0.023493, acc.: 100.00%] [G loss: 5.706142]\n",
      "566 [D loss: 0.022183, acc.: 100.00%] [G loss: 5.195948]\n",
      "567 [D loss: 0.019828, acc.: 100.00%] [G loss: 5.719803]\n",
      "568 [D loss: 0.013092, acc.: 100.00%] [G loss: 5.552398]\n",
      "569 [D loss: 0.012286, acc.: 100.00%] [G loss: 5.780622]\n",
      "570 [D loss: 0.012783, acc.: 100.00%] [G loss: 6.177827]\n",
      "571 [D loss: 0.021649, acc.: 100.00%] [G loss: 5.642333]\n",
      "572 [D loss: 0.017665, acc.: 99.61%] [G loss: 5.432917]\n",
      "573 [D loss: 0.012438, acc.: 100.00%] [G loss: 5.389157]\n",
      "574 [D loss: 0.014429, acc.: 100.00%] [G loss: 5.279791]\n",
      "575 [D loss: 0.012447, acc.: 100.00%] [G loss: 5.406496]\n",
      "576 [D loss: 0.017189, acc.: 100.00%] [G loss: 5.610043]\n",
      "577 [D loss: 0.011141, acc.: 100.00%] [G loss: 5.217708]\n",
      "578 [D loss: 0.020702, acc.: 100.00%] [G loss: 5.148045]\n",
      "579 [D loss: 0.016287, acc.: 100.00%] [G loss: 4.701016]\n",
      "580 [D loss: 0.015336, acc.: 100.00%] [G loss: 4.555380]\n",
      "581 [D loss: 0.016477, acc.: 100.00%] [G loss: 4.669298]\n",
      "582 [D loss: 0.014874, acc.: 100.00%] [G loss: 5.000136]\n",
      "583 [D loss: 0.013305, acc.: 100.00%] [G loss: 5.041077]\n",
      "584 [D loss: 0.014618, acc.: 100.00%] [G loss: 5.154531]\n",
      "585 [D loss: 0.011147, acc.: 100.00%] [G loss: 4.866087]\n",
      "586 [D loss: 0.010495, acc.: 100.00%] [G loss: 4.819518]\n",
      "587 [D loss: 0.013741, acc.: 100.00%] [G loss: 4.676870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588 [D loss: 0.010629, acc.: 100.00%] [G loss: 4.733716]\n",
      "589 [D loss: 0.010664, acc.: 100.00%] [G loss: 4.776690]\n",
      "590 [D loss: 0.011754, acc.: 100.00%] [G loss: 4.687382]\n",
      "591 [D loss: 0.013627, acc.: 100.00%] [G loss: 4.896551]\n",
      "592 [D loss: 0.014387, acc.: 100.00%] [G loss: 4.739288]\n",
      "593 [D loss: 0.017210, acc.: 100.00%] [G loss: 4.538580]\n",
      "594 [D loss: 0.020345, acc.: 100.00%] [G loss: 4.208739]\n",
      "595 [D loss: 0.019502, acc.: 100.00%] [G loss: 4.217289]\n",
      "596 [D loss: 0.015863, acc.: 100.00%] [G loss: 4.284447]\n",
      "597 [D loss: 0.015714, acc.: 100.00%] [G loss: 4.571493]\n",
      "598 [D loss: 0.023032, acc.: 100.00%] [G loss: 4.585365]\n",
      "599 [D loss: 0.022891, acc.: 100.00%] [G loss: 4.349692]\n",
      "600 [D loss: 0.028181, acc.: 100.00%] [G loss: 4.208442]\n",
      "601 [D loss: 0.024043, acc.: 100.00%] [G loss: 4.056297]\n",
      "602 [D loss: 0.026036, acc.: 100.00%] [G loss: 4.342232]\n",
      "603 [D loss: 0.031042, acc.: 100.00%] [G loss: 4.350864]\n",
      "604 [D loss: 0.032189, acc.: 100.00%] [G loss: 4.369738]\n",
      "605 [D loss: 0.030635, acc.: 100.00%] [G loss: 4.185015]\n",
      "606 [D loss: 0.039919, acc.: 99.22%] [G loss: 4.166637]\n",
      "607 [D loss: 0.039542, acc.: 100.00%] [G loss: 4.073967]\n",
      "608 [D loss: 0.040546, acc.: 99.61%] [G loss: 4.023836]\n",
      "609 [D loss: 0.038072, acc.: 100.00%] [G loss: 4.157301]\n",
      "610 [D loss: 0.029434, acc.: 100.00%] [G loss: 4.211114]\n",
      "611 [D loss: 0.031235, acc.: 100.00%] [G loss: 4.438042]\n",
      "612 [D loss: 0.031882, acc.: 100.00%] [G loss: 4.386503]\n",
      "613 [D loss: 0.026848, acc.: 100.00%] [G loss: 4.163687]\n",
      "614 [D loss: 0.018844, acc.: 100.00%] [G loss: 4.622061]\n",
      "615 [D loss: 0.026990, acc.: 99.61%] [G loss: 4.503107]\n",
      "616 [D loss: 0.023634, acc.: 100.00%] [G loss: 4.355255]\n",
      "617 [D loss: 0.017292, acc.: 100.00%] [G loss: 4.397475]\n",
      "618 [D loss: 0.015720, acc.: 100.00%] [G loss: 4.501647]\n",
      "619 [D loss: 0.013873, acc.: 100.00%] [G loss: 4.883346]\n",
      "620 [D loss: 0.013521, acc.: 100.00%] [G loss: 4.900084]\n",
      "621 [D loss: 0.013099, acc.: 100.00%] [G loss: 4.772184]\n",
      "622 [D loss: 0.012314, acc.: 100.00%] [G loss: 4.864944]\n",
      "623 [D loss: 0.015311, acc.: 100.00%] [G loss: 4.555445]\n",
      "624 [D loss: 0.020075, acc.: 99.61%] [G loss: 4.570541]\n",
      "625 [D loss: 0.024659, acc.: 100.00%] [G loss: 4.345981]\n",
      "626 [D loss: 0.023920, acc.: 100.00%] [G loss: 4.360842]\n",
      "627 [D loss: 0.022121, acc.: 100.00%] [G loss: 4.625948]\n",
      "628 [D loss: 0.028379, acc.: 99.22%] [G loss: 4.673596]\n",
      "629 [D loss: 0.027958, acc.: 100.00%] [G loss: 4.415679]\n",
      "630 [D loss: 0.027929, acc.: 100.00%] [G loss: 4.570366]\n",
      "631 [D loss: 0.048889, acc.: 99.22%] [G loss: 4.406767]\n",
      "632 [D loss: 0.021658, acc.: 100.00%] [G loss: 4.663055]\n",
      "633 [D loss: 0.018992, acc.: 100.00%] [G loss: 4.758136]\n",
      "634 [D loss: 0.024771, acc.: 99.61%] [G loss: 4.916667]\n",
      "635 [D loss: 0.028158, acc.: 99.61%] [G loss: 4.484536]\n",
      "636 [D loss: 0.026845, acc.: 100.00%] [G loss: 4.259136]\n",
      "637 [D loss: 0.019435, acc.: 100.00%] [G loss: 4.671103]\n",
      "638 [D loss: 0.028045, acc.: 100.00%] [G loss: 4.460662]\n",
      "639 [D loss: 0.024256, acc.: 100.00%] [G loss: 4.349230]\n",
      "640 [D loss: 0.032875, acc.: 99.61%] [G loss: 4.127656]\n",
      "641 [D loss: 0.034979, acc.: 99.61%] [G loss: 4.156684]\n",
      "642 [D loss: 0.034596, acc.: 100.00%] [G loss: 4.419697]\n",
      "643 [D loss: 0.030299, acc.: 100.00%] [G loss: 4.519073]\n",
      "644 [D loss: 0.047395, acc.: 99.61%] [G loss: 4.309145]\n",
      "645 [D loss: 0.050072, acc.: 99.61%] [G loss: 3.938856]\n",
      "646 [D loss: 0.033635, acc.: 100.00%] [G loss: 4.689859]\n",
      "647 [D loss: 0.024292, acc.: 100.00%] [G loss: 4.928919]\n",
      "648 [D loss: 0.037081, acc.: 98.44%] [G loss: 4.842555]\n",
      "649 [D loss: 0.027960, acc.: 100.00%] [G loss: 4.497435]\n",
      "650 [D loss: 0.026238, acc.: 100.00%] [G loss: 4.764104]\n",
      "651 [D loss: 0.026004, acc.: 100.00%] [G loss: 4.882047]\n",
      "652 [D loss: 0.021704, acc.: 100.00%] [G loss: 4.641029]\n",
      "653 [D loss: 0.028376, acc.: 100.00%] [G loss: 4.486266]\n",
      "654 [D loss: 0.027077, acc.: 100.00%] [G loss: 4.224613]\n",
      "655 [D loss: 0.029764, acc.: 100.00%] [G loss: 4.327761]\n",
      "656 [D loss: 0.025587, acc.: 100.00%] [G loss: 4.188270]\n",
      "657 [D loss: 0.026260, acc.: 100.00%] [G loss: 4.342883]\n",
      "658 [D loss: 0.037904, acc.: 100.00%] [G loss: 4.285813]\n",
      "659 [D loss: 0.036070, acc.: 99.61%] [G loss: 4.256275]\n",
      "660 [D loss: 0.035435, acc.: 99.61%] [G loss: 4.140230]\n",
      "661 [D loss: 0.037969, acc.: 100.00%] [G loss: 3.947811]\n",
      "662 [D loss: 0.043130, acc.: 99.61%] [G loss: 4.146786]\n",
      "663 [D loss: 0.031477, acc.: 100.00%] [G loss: 4.297400]\n",
      "664 [D loss: 0.040759, acc.: 99.61%] [G loss: 4.417609]\n",
      "665 [D loss: 0.035551, acc.: 99.61%] [G loss: 4.264032]\n",
      "666 [D loss: 0.024749, acc.: 100.00%] [G loss: 4.526209]\n",
      "667 [D loss: 0.038125, acc.: 99.22%] [G loss: 4.398225]\n",
      "668 [D loss: 0.022012, acc.: 100.00%] [G loss: 4.660508]\n",
      "669 [D loss: 0.017850, acc.: 100.00%] [G loss: 4.691270]\n",
      "670 [D loss: 0.015677, acc.: 100.00%] [G loss: 4.953712]\n",
      "671 [D loss: 0.015665, acc.: 100.00%] [G loss: 5.235883]\n",
      "672 [D loss: 0.018887, acc.: 100.00%] [G loss: 5.079451]\n",
      "673 [D loss: 0.018439, acc.: 100.00%] [G loss: 5.173423]\n",
      "674 [D loss: 0.027899, acc.: 100.00%] [G loss: 4.809059]\n",
      "675 [D loss: 0.028814, acc.: 99.61%] [G loss: 4.557014]\n",
      "676 [D loss: 0.024222, acc.: 100.00%] [G loss: 4.793751]\n",
      "677 [D loss: 0.022247, acc.: 100.00%] [G loss: 4.731570]\n",
      "678 [D loss: 0.022128, acc.: 100.00%] [G loss: 4.910174]\n",
      "679 [D loss: 0.034676, acc.: 100.00%] [G loss: 4.667200]\n",
      "680 [D loss: 0.026939, acc.: 100.00%] [G loss: 4.778553]\n",
      "681 [D loss: 0.043760, acc.: 99.61%] [G loss: 4.451845]\n",
      "682 [D loss: 0.032882, acc.: 100.00%] [G loss: 4.306507]\n",
      "683 [D loss: 0.032041, acc.: 100.00%] [G loss: 4.967091]\n",
      "684 [D loss: 0.033671, acc.: 100.00%] [G loss: 4.795120]\n",
      "685 [D loss: 0.030105, acc.: 100.00%] [G loss: 4.900750]\n",
      "686 [D loss: 0.027086, acc.: 100.00%] [G loss: 4.560635]\n",
      "687 [D loss: 0.027582, acc.: 100.00%] [G loss: 4.694307]\n",
      "688 [D loss: 0.028375, acc.: 100.00%] [G loss: 4.994621]\n",
      "689 [D loss: 0.034004, acc.: 100.00%] [G loss: 4.804704]\n",
      "690 [D loss: 0.029312, acc.: 100.00%] [G loss: 4.709103]\n",
      "691 [D loss: 0.033872, acc.: 100.00%] [G loss: 4.334902]\n",
      "692 [D loss: 0.031894, acc.: 100.00%] [G loss: 4.562803]\n",
      "693 [D loss: 0.049506, acc.: 99.61%] [G loss: 4.284851]\n",
      "694 [D loss: 0.030697, acc.: 100.00%] [G loss: 4.149761]\n",
      "695 [D loss: 0.035838, acc.: 100.00%] [G loss: 4.550204]\n",
      "696 [D loss: 0.030026, acc.: 100.00%] [G loss: 4.843040]\n",
      "697 [D loss: 0.034391, acc.: 99.61%] [G loss: 4.808825]\n",
      "698 [D loss: 0.033702, acc.: 100.00%] [G loss: 4.479059]\n",
      "699 [D loss: 0.039868, acc.: 99.61%] [G loss: 4.395469]\n",
      "700 [D loss: 0.033369, acc.: 100.00%] [G loss: 4.282752]\n",
      "701 [D loss: 0.021653, acc.: 100.00%] [G loss: 5.040091]\n",
      "702 [D loss: 0.031931, acc.: 99.22%] [G loss: 4.745014]\n",
      "703 [D loss: 0.033068, acc.: 99.22%] [G loss: 4.422795]\n",
      "704 [D loss: 0.017769, acc.: 100.00%] [G loss: 4.214577]\n",
      "705 [D loss: 0.017447, acc.: 100.00%] [G loss: 4.470561]\n",
      "706 [D loss: 0.012663, acc.: 100.00%] [G loss: 4.696577]\n",
      "707 [D loss: 0.020115, acc.: 100.00%] [G loss: 4.713329]\n",
      "708 [D loss: 0.015363, acc.: 100.00%] [G loss: 4.873280]\n",
      "709 [D loss: 0.019016, acc.: 100.00%] [G loss: 4.431387]\n",
      "710 [D loss: 0.020318, acc.: 100.00%] [G loss: 4.528478]\n",
      "711 [D loss: 0.033180, acc.: 99.61%] [G loss: 4.453173]\n",
      "712 [D loss: 0.022894, acc.: 100.00%] [G loss: 4.434973]\n",
      "713 [D loss: 0.020026, acc.: 100.00%] [G loss: 4.710602]\n",
      "714 [D loss: 0.034601, acc.: 99.61%] [G loss: 4.307909]\n",
      "715 [D loss: 0.023701, acc.: 100.00%] [G loss: 4.270507]\n",
      "716 [D loss: 0.020857, acc.: 100.00%] [G loss: 4.485957]\n",
      "717 [D loss: 0.018166, acc.: 100.00%] [G loss: 4.847825]\n",
      "718 [D loss: 0.027032, acc.: 99.61%] [G loss: 4.808318]\n",
      "719 [D loss: 0.024176, acc.: 100.00%] [G loss: 4.510691]\n",
      "720 [D loss: 0.020741, acc.: 100.00%] [G loss: 4.444917]\n",
      "721 [D loss: 0.030134, acc.: 100.00%] [G loss: 4.453373]\n",
      "722 [D loss: 0.024588, acc.: 99.61%] [G loss: 4.679635]\n",
      "723 [D loss: 0.020453, acc.: 100.00%] [G loss: 4.928131]\n",
      "724 [D loss: 0.029324, acc.: 99.61%] [G loss: 4.685587]\n",
      "725 [D loss: 0.025723, acc.: 100.00%] [G loss: 4.846517]\n",
      "726 [D loss: 0.025289, acc.: 99.61%] [G loss: 4.826811]\n",
      "727 [D loss: 0.032633, acc.: 99.61%] [G loss: 4.783895]\n",
      "728 [D loss: 0.021432, acc.: 100.00%] [G loss: 4.710394]\n",
      "729 [D loss: 0.021794, acc.: 100.00%] [G loss: 4.957075]\n",
      "730 [D loss: 0.022232, acc.: 100.00%] [G loss: 4.872345]\n",
      "731 [D loss: 0.020191, acc.: 100.00%] [G loss: 4.995729]\n",
      "732 [D loss: 0.015316, acc.: 100.00%] [G loss: 5.241805]\n",
      "733 [D loss: 0.026463, acc.: 100.00%] [G loss: 4.922651]\n",
      "734 [D loss: 0.015078, acc.: 100.00%] [G loss: 4.843525]\n",
      "735 [D loss: 0.020405, acc.: 100.00%] [G loss: 4.902934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736 [D loss: 0.020905, acc.: 100.00%] [G loss: 4.853936]\n",
      "737 [D loss: 0.021291, acc.: 100.00%] [G loss: 4.833130]\n",
      "738 [D loss: 0.015168, acc.: 100.00%] [G loss: 4.490783]\n",
      "739 [D loss: 0.018454, acc.: 100.00%] [G loss: 4.699024]\n",
      "740 [D loss: 0.017219, acc.: 100.00%] [G loss: 5.090503]\n",
      "741 [D loss: 0.019528, acc.: 100.00%] [G loss: 5.104705]\n",
      "742 [D loss: 0.022696, acc.: 100.00%] [G loss: 4.992892]\n",
      "743 [D loss: 0.017184, acc.: 100.00%] [G loss: 4.839030]\n",
      "744 [D loss: 0.014698, acc.: 100.00%] [G loss: 5.068901]\n",
      "745 [D loss: 0.011846, acc.: 100.00%] [G loss: 5.332542]\n",
      "746 [D loss: 0.025231, acc.: 100.00%] [G loss: 4.703808]\n",
      "747 [D loss: 0.017069, acc.: 100.00%] [G loss: 4.492939]\n",
      "748 [D loss: 0.023261, acc.: 100.00%] [G loss: 4.666101]\n",
      "749 [D loss: 0.017040, acc.: 100.00%] [G loss: 4.935949]\n",
      "750 [D loss: 0.019073, acc.: 100.00%] [G loss: 4.994081]\n",
      "751 [D loss: 0.017474, acc.: 100.00%] [G loss: 4.964401]\n",
      "752 [D loss: 0.020953, acc.: 100.00%] [G loss: 4.633874]\n",
      "753 [D loss: 0.029620, acc.: 100.00%] [G loss: 4.468688]\n",
      "754 [D loss: 0.029659, acc.: 100.00%] [G loss: 4.451647]\n",
      "755 [D loss: 0.019015, acc.: 100.00%] [G loss: 4.775726]\n",
      "756 [D loss: 0.013084, acc.: 100.00%] [G loss: 5.194069]\n",
      "757 [D loss: 0.023161, acc.: 100.00%] [G loss: 4.999891]\n",
      "758 [D loss: 0.019334, acc.: 100.00%] [G loss: 4.814452]\n",
      "759 [D loss: 0.018769, acc.: 100.00%] [G loss: 4.673766]\n",
      "760 [D loss: 0.017513, acc.: 100.00%] [G loss: 5.121186]\n",
      "761 [D loss: 0.013684, acc.: 100.00%] [G loss: 5.501966]\n",
      "762 [D loss: 0.012484, acc.: 100.00%] [G loss: 5.271047]\n",
      "763 [D loss: 0.026604, acc.: 100.00%] [G loss: 4.868894]\n",
      "764 [D loss: 0.024491, acc.: 100.00%] [G loss: 4.367741]\n",
      "765 [D loss: 0.026728, acc.: 100.00%] [G loss: 4.469810]\n",
      "766 [D loss: 0.018972, acc.: 100.00%] [G loss: 5.219105]\n",
      "767 [D loss: 0.020651, acc.: 100.00%] [G loss: 5.276054]\n",
      "768 [D loss: 0.024214, acc.: 100.00%] [G loss: 5.192424]\n",
      "769 [D loss: 0.019552, acc.: 100.00%] [G loss: 4.741669]\n",
      "770 [D loss: 0.022362, acc.: 100.00%] [G loss: 5.011821]\n",
      "771 [D loss: 0.028349, acc.: 100.00%] [G loss: 4.650713]\n",
      "772 [D loss: 0.019115, acc.: 100.00%] [G loss: 4.659059]\n",
      "773 [D loss: 0.026164, acc.: 100.00%] [G loss: 4.850426]\n",
      "774 [D loss: 0.020386, acc.: 100.00%] [G loss: 4.779844]\n",
      "775 [D loss: 0.023250, acc.: 99.61%] [G loss: 4.526597]\n",
      "776 [D loss: 0.023455, acc.: 100.00%] [G loss: 4.659647]\n",
      "777 [D loss: 0.023294, acc.: 100.00%] [G loss: 4.445517]\n",
      "778 [D loss: 0.022937, acc.: 100.00%] [G loss: 4.675864]\n",
      "779 [D loss: 0.016516, acc.: 100.00%] [G loss: 4.935939]\n",
      "780 [D loss: 0.017415, acc.: 100.00%] [G loss: 4.865756]\n",
      "781 [D loss: 0.017695, acc.: 100.00%] [G loss: 5.175008]\n",
      "782 [D loss: 0.022028, acc.: 100.00%] [G loss: 4.401320]\n",
      "783 [D loss: 0.021259, acc.: 100.00%] [G loss: 4.579173]\n",
      "784 [D loss: 0.018004, acc.: 100.00%] [G loss: 4.815890]\n",
      "785 [D loss: 0.019736, acc.: 100.00%] [G loss: 4.808369]\n",
      "786 [D loss: 0.015723, acc.: 100.00%] [G loss: 5.001595]\n",
      "787 [D loss: 0.017381, acc.: 100.00%] [G loss: 4.782069]\n",
      "788 [D loss: 0.017788, acc.: 100.00%] [G loss: 4.608075]\n",
      "789 [D loss: 0.019908, acc.: 100.00%] [G loss: 4.451761]\n",
      "790 [D loss: 0.020231, acc.: 100.00%] [G loss: 4.732941]\n",
      "791 [D loss: 0.016486, acc.: 100.00%] [G loss: 4.842202]\n",
      "792 [D loss: 0.025449, acc.: 99.22%] [G loss: 4.452530]\n",
      "793 [D loss: 0.019525, acc.: 100.00%] [G loss: 4.437587]\n",
      "794 [D loss: 0.018958, acc.: 100.00%] [G loss: 4.396440]\n",
      "795 [D loss: 0.016571, acc.: 100.00%] [G loss: 4.682012]\n",
      "796 [D loss: 0.019648, acc.: 100.00%] [G loss: 4.702065]\n",
      "797 [D loss: 0.018698, acc.: 100.00%] [G loss: 4.766388]\n",
      "798 [D loss: 0.026787, acc.: 99.61%] [G loss: 4.494215]\n",
      "799 [D loss: 0.021573, acc.: 100.00%] [G loss: 4.562907]\n",
      "800 [D loss: 0.013197, acc.: 100.00%] [G loss: 4.880316]\n",
      "801 [D loss: 0.010713, acc.: 100.00%] [G loss: 5.205332]\n",
      "802 [D loss: 0.011381, acc.: 100.00%] [G loss: 5.379393]\n",
      "803 [D loss: 0.014492, acc.: 100.00%] [G loss: 4.999240]\n",
      "804 [D loss: 0.012057, acc.: 100.00%] [G loss: 5.071864]\n",
      "805 [D loss: 0.011460, acc.: 100.00%] [G loss: 4.491975]\n",
      "806 [D loss: 0.011953, acc.: 100.00%] [G loss: 4.568595]\n",
      "807 [D loss: 0.012490, acc.: 100.00%] [G loss: 4.922336]\n",
      "808 [D loss: 0.013653, acc.: 100.00%] [G loss: 4.945506]\n",
      "809 [D loss: 0.013610, acc.: 100.00%] [G loss: 5.046189]\n",
      "810 [D loss: 0.012486, acc.: 100.00%] [G loss: 4.980336]\n",
      "811 [D loss: 0.014032, acc.: 100.00%] [G loss: 4.785625]\n",
      "812 [D loss: 0.018293, acc.: 100.00%] [G loss: 4.661643]\n",
      "813 [D loss: 0.018549, acc.: 100.00%] [G loss: 4.444882]\n",
      "814 [D loss: 0.018939, acc.: 100.00%] [G loss: 4.549417]\n",
      "815 [D loss: 0.018237, acc.: 100.00%] [G loss: 4.640251]\n",
      "816 [D loss: 0.022862, acc.: 100.00%] [G loss: 4.508182]\n",
      "817 [D loss: 0.019807, acc.: 100.00%] [G loss: 4.432302]\n",
      "818 [D loss: 0.016908, acc.: 100.00%] [G loss: 4.479556]\n",
      "819 [D loss: 0.017143, acc.: 100.00%] [G loss: 4.694427]\n",
      "820 [D loss: 0.013014, acc.: 100.00%] [G loss: 4.973033]\n",
      "821 [D loss: 0.020844, acc.: 100.00%] [G loss: 4.786246]\n",
      "822 [D loss: 0.014827, acc.: 100.00%] [G loss: 4.577998]\n",
      "823 [D loss: 0.016115, acc.: 100.00%] [G loss: 4.808409]\n",
      "824 [D loss: 0.015264, acc.: 100.00%] [G loss: 4.912014]\n",
      "825 [D loss: 0.011910, acc.: 100.00%] [G loss: 5.283605]\n",
      "826 [D loss: 0.012085, acc.: 100.00%] [G loss: 5.333472]\n",
      "827 [D loss: 0.015272, acc.: 100.00%] [G loss: 5.233413]\n",
      "828 [D loss: 0.014759, acc.: 100.00%] [G loss: 4.974705]\n",
      "829 [D loss: 0.015378, acc.: 100.00%] [G loss: 5.148088]\n",
      "830 [D loss: 0.017730, acc.: 100.00%] [G loss: 4.832136]\n",
      "831 [D loss: 0.014883, acc.: 100.00%] [G loss: 5.084030]\n",
      "832 [D loss: 0.022745, acc.: 100.00%] [G loss: 5.138431]\n",
      "833 [D loss: 0.014651, acc.: 100.00%] [G loss: 5.312814]\n",
      "834 [D loss: 0.030446, acc.: 100.00%] [G loss: 4.603423]\n",
      "835 [D loss: 0.022511, acc.: 100.00%] [G loss: 4.567218]\n",
      "836 [D loss: 0.016639, acc.: 100.00%] [G loss: 5.061554]\n",
      "837 [D loss: 0.017349, acc.: 100.00%] [G loss: 5.358866]\n",
      "838 [D loss: 0.017904, acc.: 100.00%] [G loss: 5.085557]\n",
      "839 [D loss: 0.018178, acc.: 100.00%] [G loss: 4.934012]\n",
      "840 [D loss: 0.013073, acc.: 100.00%] [G loss: 4.706014]\n",
      "841 [D loss: 0.011913, acc.: 100.00%] [G loss: 4.951303]\n",
      "842 [D loss: 0.013557, acc.: 100.00%] [G loss: 5.072793]\n",
      "843 [D loss: 0.010521, acc.: 100.00%] [G loss: 5.151344]\n",
      "844 [D loss: 0.009217, acc.: 100.00%] [G loss: 5.255543]\n",
      "845 [D loss: 0.016656, acc.: 100.00%] [G loss: 5.005790]\n",
      "846 [D loss: 0.017216, acc.: 100.00%] [G loss: 4.928549]\n",
      "847 [D loss: 0.014059, acc.: 100.00%] [G loss: 5.042117]\n",
      "848 [D loss: 0.016311, acc.: 100.00%] [G loss: 4.756823]\n",
      "849 [D loss: 0.015405, acc.: 100.00%] [G loss: 4.929032]\n",
      "850 [D loss: 0.014104, acc.: 100.00%] [G loss: 4.990128]\n",
      "851 [D loss: 0.013936, acc.: 100.00%] [G loss: 4.766234]\n",
      "852 [D loss: 0.012232, acc.: 100.00%] [G loss: 4.971073]\n",
      "853 [D loss: 0.015915, acc.: 100.00%] [G loss: 4.711020]\n",
      "854 [D loss: 0.011448, acc.: 100.00%] [G loss: 4.534705]\n",
      "855 [D loss: 0.015814, acc.: 100.00%] [G loss: 4.545793]\n",
      "856 [D loss: 0.017115, acc.: 100.00%] [G loss: 4.600181]\n",
      "857 [D loss: 0.013014, acc.: 100.00%] [G loss: 4.573691]\n",
      "858 [D loss: 0.010350, acc.: 100.00%] [G loss: 5.038678]\n",
      "859 [D loss: 0.018928, acc.: 100.00%] [G loss: 4.606444]\n",
      "860 [D loss: 0.018220, acc.: 100.00%] [G loss: 4.338195]\n",
      "861 [D loss: 0.019709, acc.: 100.00%] [G loss: 4.398971]\n",
      "862 [D loss: 0.011311, acc.: 100.00%] [G loss: 4.972230]\n",
      "863 [D loss: 0.009527, acc.: 100.00%] [G loss: 5.464572]\n",
      "864 [D loss: 0.007518, acc.: 100.00%] [G loss: 5.581059]\n",
      "865 [D loss: 0.013963, acc.: 100.00%] [G loss: 5.443007]\n",
      "866 [D loss: 0.011200, acc.: 100.00%] [G loss: 4.965377]\n",
      "867 [D loss: 0.010635, acc.: 100.00%] [G loss: 4.826208]\n",
      "868 [D loss: 0.016368, acc.: 99.61%] [G loss: 4.551881]\n",
      "869 [D loss: 0.015973, acc.: 100.00%] [G loss: 4.506393]\n",
      "870 [D loss: 0.013029, acc.: 100.00%] [G loss: 5.134568]\n",
      "871 [D loss: 0.012219, acc.: 100.00%] [G loss: 5.167311]\n",
      "872 [D loss: 0.021968, acc.: 99.61%] [G loss: 4.935610]\n",
      "873 [D loss: 0.017576, acc.: 99.61%] [G loss: 4.631322]\n",
      "874 [D loss: 0.020536, acc.: 100.00%] [G loss: 4.363905]\n",
      "875 [D loss: 0.020891, acc.: 100.00%] [G loss: 4.690807]\n",
      "876 [D loss: 0.023318, acc.: 100.00%] [G loss: 5.085192]\n",
      "877 [D loss: 0.019526, acc.: 99.61%] [G loss: 4.890661]\n",
      "878 [D loss: 0.021068, acc.: 100.00%] [G loss: 4.779905]\n",
      "879 [D loss: 0.013098, acc.: 100.00%] [G loss: 4.939013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880 [D loss: 0.017349, acc.: 99.61%] [G loss: 4.989862]\n",
      "881 [D loss: 0.013116, acc.: 100.00%] [G loss: 5.089453]\n",
      "882 [D loss: 0.010344, acc.: 100.00%] [G loss: 5.199720]\n",
      "883 [D loss: 0.014746, acc.: 100.00%] [G loss: 5.252855]\n",
      "884 [D loss: 0.011887, acc.: 100.00%] [G loss: 5.216047]\n",
      "885 [D loss: 0.010316, acc.: 100.00%] [G loss: 5.142741]\n",
      "886 [D loss: 0.011910, acc.: 100.00%] [G loss: 5.350079]\n",
      "887 [D loss: 0.017410, acc.: 99.61%] [G loss: 4.883411]\n",
      "888 [D loss: 0.014303, acc.: 100.00%] [G loss: 4.892160]\n",
      "889 [D loss: 0.018330, acc.: 100.00%] [G loss: 4.805072]\n",
      "890 [D loss: 0.022695, acc.: 100.00%] [G loss: 4.642638]\n",
      "891 [D loss: 0.028508, acc.: 100.00%] [G loss: 4.791015]\n",
      "892 [D loss: 0.020400, acc.: 100.00%] [G loss: 5.093263]\n",
      "893 [D loss: 0.027202, acc.: 100.00%] [G loss: 5.087263]\n",
      "894 [D loss: 0.022749, acc.: 99.61%] [G loss: 4.708878]\n",
      "895 [D loss: 0.021174, acc.: 99.61%] [G loss: 4.773655]\n",
      "896 [D loss: 0.012782, acc.: 100.00%] [G loss: 4.854096]\n",
      "897 [D loss: 0.012147, acc.: 100.00%] [G loss: 5.375577]\n",
      "898 [D loss: 0.021013, acc.: 99.61%] [G loss: 5.268253]\n",
      "899 [D loss: 0.015098, acc.: 100.00%] [G loss: 5.205683]\n",
      "900 [D loss: 0.018475, acc.: 100.00%] [G loss: 4.886312]\n",
      "901 [D loss: 0.012669, acc.: 100.00%] [G loss: 5.055291]\n",
      "902 [D loss: 0.009991, acc.: 100.00%] [G loss: 5.471075]\n",
      "903 [D loss: 0.010863, acc.: 100.00%] [G loss: 5.369944]\n",
      "904 [D loss: 0.009002, acc.: 100.00%] [G loss: 5.740323]\n",
      "905 [D loss: 0.010523, acc.: 100.00%] [G loss: 5.522688]\n",
      "906 [D loss: 0.013993, acc.: 100.00%] [G loss: 5.227202]\n",
      "907 [D loss: 0.012436, acc.: 100.00%] [G loss: 5.078542]\n",
      "908 [D loss: 0.014984, acc.: 100.00%] [G loss: 4.875946]\n",
      "909 [D loss: 0.018492, acc.: 100.00%] [G loss: 5.183098]\n",
      "910 [D loss: 0.011227, acc.: 100.00%] [G loss: 5.601919]\n",
      "911 [D loss: 0.018432, acc.: 100.00%] [G loss: 5.498096]\n",
      "912 [D loss: 0.022077, acc.: 99.61%] [G loss: 4.925468]\n",
      "913 [D loss: 0.024894, acc.: 100.00%] [G loss: 4.878189]\n",
      "914 [D loss: 0.014853, acc.: 100.00%] [G loss: 5.168862]\n",
      "915 [D loss: 0.015645, acc.: 100.00%] [G loss: 5.810002]\n",
      "916 [D loss: 0.030675, acc.: 99.61%] [G loss: 5.220603]\n",
      "917 [D loss: 0.017411, acc.: 100.00%] [G loss: 4.794136]\n",
      "918 [D loss: 0.015267, acc.: 100.00%] [G loss: 5.389324]\n",
      "919 [D loss: 0.013885, acc.: 100.00%] [G loss: 5.664116]\n",
      "920 [D loss: 0.016733, acc.: 100.00%] [G loss: 5.264274]\n",
      "921 [D loss: 0.010106, acc.: 100.00%] [G loss: 5.408295]\n",
      "922 [D loss: 0.009077, acc.: 100.00%] [G loss: 5.641850]\n",
      "923 [D loss: 0.012147, acc.: 100.00%] [G loss: 5.428042]\n",
      "924 [D loss: 0.009065, acc.: 100.00%] [G loss: 5.197321]\n",
      "925 [D loss: 0.011228, acc.: 100.00%] [G loss: 5.287513]\n",
      "926 [D loss: 0.011734, acc.: 100.00%] [G loss: 5.447635]\n",
      "927 [D loss: 0.013677, acc.: 100.00%] [G loss: 5.352026]\n",
      "928 [D loss: 0.014406, acc.: 100.00%] [G loss: 5.212997]\n",
      "929 [D loss: 0.013076, acc.: 100.00%] [G loss: 5.181258]\n",
      "930 [D loss: 0.009385, acc.: 100.00%] [G loss: 5.585807]\n",
      "931 [D loss: 0.007905, acc.: 100.00%] [G loss: 5.834998]\n",
      "932 [D loss: 0.010896, acc.: 100.00%] [G loss: 5.752093]\n",
      "933 [D loss: 0.009576, acc.: 100.00%] [G loss: 5.764613]\n",
      "934 [D loss: 0.011216, acc.: 100.00%] [G loss: 5.593643]\n",
      "935 [D loss: 0.007801, acc.: 100.00%] [G loss: 5.203100]\n",
      "936 [D loss: 0.009083, acc.: 100.00%] [G loss: 5.324664]\n",
      "937 [D loss: 0.007271, acc.: 100.00%] [G loss: 5.666215]\n",
      "938 [D loss: 0.011777, acc.: 100.00%] [G loss: 5.658606]\n",
      "939 [D loss: 0.010892, acc.: 100.00%] [G loss: 5.483792]\n",
      "940 [D loss: 0.007476, acc.: 100.00%] [G loss: 5.593128]\n",
      "941 [D loss: 0.009278, acc.: 100.00%] [G loss: 5.780647]\n",
      "942 [D loss: 0.011069, acc.: 100.00%] [G loss: 5.537735]\n",
      "943 [D loss: 0.011773, acc.: 100.00%] [G loss: 5.396420]\n",
      "944 [D loss: 0.009679, acc.: 100.00%] [G loss: 5.444299]\n",
      "945 [D loss: 0.010630, acc.: 100.00%] [G loss: 5.750669]\n",
      "946 [D loss: 0.006647, acc.: 100.00%] [G loss: 6.141016]\n",
      "947 [D loss: 0.010015, acc.: 100.00%] [G loss: 6.011621]\n",
      "948 [D loss: 0.009027, acc.: 100.00%] [G loss: 5.442297]\n",
      "949 [D loss: 0.012740, acc.: 100.00%] [G loss: 5.428480]\n",
      "950 [D loss: 0.013344, acc.: 100.00%] [G loss: 5.343140]\n",
      "951 [D loss: 0.009477, acc.: 100.00%] [G loss: 5.449494]\n",
      "952 [D loss: 0.008922, acc.: 100.00%] [G loss: 5.741684]\n",
      "953 [D loss: 0.012218, acc.: 100.00%] [G loss: 6.091450]\n",
      "954 [D loss: 0.013511, acc.: 100.00%] [G loss: 5.512615]\n",
      "955 [D loss: 0.012521, acc.: 100.00%] [G loss: 5.326667]\n",
      "956 [D loss: 0.010788, acc.: 100.00%] [G loss: 5.470617]\n",
      "957 [D loss: 0.011791, acc.: 100.00%] [G loss: 5.575458]\n",
      "958 [D loss: 0.012025, acc.: 100.00%] [G loss: 5.286472]\n",
      "959 [D loss: 0.009542, acc.: 100.00%] [G loss: 5.931384]\n",
      "960 [D loss: 0.011117, acc.: 100.00%] [G loss: 6.072576]\n",
      "961 [D loss: 0.011712, acc.: 100.00%] [G loss: 5.807366]\n",
      "962 [D loss: 0.011452, acc.: 100.00%] [G loss: 5.940688]\n",
      "963 [D loss: 0.012321, acc.: 100.00%] [G loss: 5.907054]\n",
      "964 [D loss: 0.012080, acc.: 100.00%] [G loss: 5.731038]\n",
      "965 [D loss: 0.013231, acc.: 100.00%] [G loss: 5.744914]\n",
      "966 [D loss: 0.011448, acc.: 100.00%] [G loss: 5.732341]\n",
      "967 [D loss: 0.013319, acc.: 100.00%] [G loss: 5.570682]\n",
      "968 [D loss: 0.017754, acc.: 100.00%] [G loss: 5.418665]\n",
      "969 [D loss: 0.012402, acc.: 100.00%] [G loss: 5.725627]\n",
      "970 [D loss: 0.008098, acc.: 100.00%] [G loss: 6.213106]\n",
      "971 [D loss: 0.008539, acc.: 100.00%] [G loss: 6.283139]\n",
      "972 [D loss: 0.008257, acc.: 100.00%] [G loss: 6.481711]\n",
      "973 [D loss: 0.008517, acc.: 100.00%] [G loss: 5.799732]\n",
      "974 [D loss: 0.011319, acc.: 100.00%] [G loss: 5.821068]\n",
      "975 [D loss: 0.008213, acc.: 100.00%] [G loss: 5.498178]\n",
      "976 [D loss: 0.009142, acc.: 100.00%] [G loss: 6.059760]\n",
      "977 [D loss: 0.009825, acc.: 100.00%] [G loss: 5.800627]\n",
      "978 [D loss: 0.009770, acc.: 100.00%] [G loss: 6.172637]\n",
      "979 [D loss: 0.013211, acc.: 100.00%] [G loss: 6.090784]\n",
      "980 [D loss: 0.014321, acc.: 100.00%] [G loss: 5.671779]\n",
      "981 [D loss: 0.011057, acc.: 100.00%] [G loss: 5.888964]\n",
      "982 [D loss: 0.008580, acc.: 100.00%] [G loss: 5.873125]\n",
      "983 [D loss: 0.007528, acc.: 100.00%] [G loss: 6.186598]\n",
      "984 [D loss: 0.006617, acc.: 100.00%] [G loss: 6.431639]\n",
      "985 [D loss: 0.006397, acc.: 100.00%] [G loss: 6.451895]\n",
      "986 [D loss: 0.008407, acc.: 100.00%] [G loss: 6.282178]\n",
      "987 [D loss: 0.008650, acc.: 100.00%] [G loss: 6.011621]\n",
      "988 [D loss: 0.007196, acc.: 100.00%] [G loss: 6.137647]\n",
      "989 [D loss: 0.005887, acc.: 100.00%] [G loss: 6.246846]\n",
      "990 [D loss: 0.007556, acc.: 100.00%] [G loss: 6.009359]\n",
      "991 [D loss: 0.006119, acc.: 100.00%] [G loss: 6.327406]\n",
      "992 [D loss: 0.007553, acc.: 100.00%] [G loss: 6.176865]\n",
      "993 [D loss: 0.007587, acc.: 100.00%] [G loss: 6.093522]\n",
      "994 [D loss: 0.007187, acc.: 100.00%] [G loss: 6.166789]\n",
      "995 [D loss: 0.005803, acc.: 100.00%] [G loss: 6.051572]\n",
      "996 [D loss: 0.008475, acc.: 100.00%] [G loss: 5.934761]\n",
      "997 [D loss: 0.005761, acc.: 100.00%] [G loss: 5.998914]\n",
      "998 [D loss: 0.006666, acc.: 100.00%] [G loss: 6.267271]\n",
      "999 [D loss: 0.008358, acc.: 100.00%] [G loss: 6.111533]\n",
      "1000 [D loss: 0.009372, acc.: 100.00%] [G loss: 6.265212]\n"
     ]
    }
   ],
   "source": [
    "z_dim = 100\n",
    "\n",
    "discriminator, generator, gan = build_compile(data_word, z_dim)\n",
    "\n",
    "iterations =1000\n",
    "batch_size = 128\n",
    "sample_interval = 1000\n",
    "\n",
    "generator = train(data_word,labels_word,iterations,batch_size,sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ベクトルを生成\n",
    "z = np.random.normal(0,1,(1, 100))\n",
    "vec = generator.predict(z)\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7303, 400)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ ,  _, data_1, _ = x_y_train(data_word,labels_word)\n",
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8048938099471006"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = 0\n",
    "for i in range(7303):\n",
    "    if result < cos_sim(vec[0],data_1[i]):\n",
    "        result = cos_sim(vec[0],data_1[i])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
