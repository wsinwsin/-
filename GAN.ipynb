{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Dense, Flatten, Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import MeCab\n",
    "import json\n",
    "import hashlib\n",
    "from googletrans import Translator\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import torch\n",
    "from transformers.modeling_bert import BertModel\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "mt = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "mt.parse('')\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "\n",
    "model_doc = Doc2Vec.load(\"jawiki.doc2vec.dbow300d.model\")\n",
    "model_word = word2vec.Word2Vec.load(\"wiki_plus.model\")\n",
    "model_bert = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(text):\n",
    "    word = {}\n",
    "    node = mt.parseToNode(text)\n",
    "    while node:\n",
    "        fields = node.feature.split(\",\")\n",
    "        if (fields[0] == '名詞' or fields[0] == '動詞' or fields[0] == '形容詞') and node.surface in model_word.wv:\n",
    "            w = node.surface\n",
    "            word[w] = word.get(w, 0) + 1\n",
    "        node = node.next\n",
    "    return word\n",
    "\n",
    "def weighted_mean_vec(text):\n",
    "    v = np.zeros(model_word.vector_size)\n",
    "    s = 1.0\n",
    "    for w,weight in get_tags(text).items():\n",
    "        v += weight * model_word.wv[w]  #Eventクラスeの単語wの個数＊単語wのベクトル\n",
    "        s += weight\n",
    "    return v / s\n",
    "\n",
    "def get_tags_for_doc2vec(text):\n",
    "    word = []\n",
    "    node = mt.parseToNode(text)\n",
    "    while node:\n",
    "        fields = node.feature.split(\",\")\n",
    "        if node.surface in model_doc.wv and node.surface !='':\n",
    "            w = node.surface\n",
    "            word.append(w)\n",
    "        node = node.next\n",
    "    return word\n",
    "\n",
    "#bertのベクトル化\n",
    "def get_vector_cls(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt') \n",
    "    result = model_bert(input_ids)\n",
    "    tensor_result = result[0][0][0]\n",
    "    numpy_result = tensor_result.to('cpu').detach().numpy().copy()\n",
    "    return numpy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event:\n",
    "    def __init__(self, id, type, score, desc, links):\n",
    "        self.id = id\n",
    "        self.type = type\n",
    "        self.score = score\n",
    "        self.desc = desc\n",
    "        self.links = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON ファイルから event set をロード\n",
    "def load_events(jsonfile):\n",
    "    with open(jsonfile) as f:\n",
    "        df = json.load(f)\n",
    "    events = {x['id']: Event(x['id'], x['type'], x['score'], x['desc'], x['links']) for x in df} #eventsにidをkeyとしそのオブジェクトをvalueとした辞書を生成\n",
    "    for k,x in events.items():\n",
    "        x.links = [events[e] for e in x.links] #Event.linkの中身をidの配列からEventの配列に変更\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = load_events('sesaku2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "labels = []\n",
    "columns=[]\n",
    "index=[]\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='部品':\n",
    "        index.append(v1.desc)\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='対策':\n",
    "        if not v1.desc in columns:\n",
    "            columns.append(v1.desc)\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='部品':\n",
    "        for k2, v2 in events.items():\n",
    "            if v2.type[-1] == '対策':\n",
    "                    if v2 in v1.links:\n",
    "                        df.at[v1.desc, v2.desc] = 1\n",
    "                    else:\n",
    "                        df.at[v1.desc, v2.desc] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "taisaku_vec_word = {}\n",
    "for i in df:\n",
    "    taisaku_vec_word[i]=weighted_mean_vec(i)\n",
    "\n",
    "taisaku_vec_doc = {}\n",
    "for i in df:\n",
    "    taisaku_vec_doc[i]=model_doc.infer_vector(get_tags_for_doc2vec(i))\n",
    "\n",
    "taisaku_vec_bert = {}\n",
    "for i in df:\n",
    "    taisaku_vec_bert[i]=get_vector_cls(i)\n",
    "\n",
    "class Label:\n",
    "    TAISAKU = 1\n",
    "    NASI = 0\n",
    "\n",
    "data_word = []\n",
    "labels_word = []\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    x1 = weighted_mean_vec(index)\n",
    "    for i in df:#対策\n",
    "        x2 =  taisaku_vec_word[i]#対策\n",
    "        data_word.extend([np.append(x1, x2)])\n",
    "        if row[i] ==1:\n",
    "            labels_word.append(Label.TAISAKU)\n",
    "        else:\n",
    "            labels_word.append(Label.NASI)\n",
    "\n",
    "data_doc = []\n",
    "labels_doc = []\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    x1 = model_doc.infer_vector(get_tags_for_doc2vec(index))\n",
    "    for i in df:#対策\n",
    "        x2 =  taisaku_vec_doc[i]#対策\n",
    "        data_doc.extend([np.append(x1, x2)])\n",
    "        if row[i] ==1:\n",
    "            labels_doc.append(Label.TAISAKU)\n",
    "        else:\n",
    "            labels_doc.append(Label.NASI)\n",
    "\n",
    "data_BERT_cls = []\n",
    "labels_BERT_cls = []\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    x1 = get_vector_cls(index) #部品\n",
    "    for i in df:#対策\n",
    "        x2=taisaku_vec_bert[i]\n",
    "        data_BERT_cls.extend([np.append(x1, x2)])\n",
    "        if row[i] ==1:\n",
    "            labels_BERT_cls.append(Label.TAISAKU)\n",
    "        else:\n",
    "            labels_BERT_cls.append(Label.NASI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.848580956459045 -6.142003456751506\n",
      "0.5652471 -0.57631934\n",
      "10.4636545 -1.5997354\n"
     ]
    }
   ],
   "source": [
    "#word,doc,bert のベクトルの範囲を確認する\n",
    "def max_min(data):\n",
    "    data_max = 0\n",
    "    data_min = 0\n",
    "    for i in data:\n",
    "        if data_max < i.max():\n",
    "            data_max = i.max()\n",
    "        if data_min > i.min():\n",
    "            data_min = i.min()\n",
    "    print(data_max,data_min)\n",
    "    return data_max, data_min\n",
    "\n",
    "word_max, word_min = max_min(data_word)\n",
    "doc_max, doc_min = max_min(data_doc)\n",
    "bert_max, bert_min = max_min(data_BERT_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " #生成器\n",
    "def build_generator(data_size, z_dim):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(128, input_dim = z_dim))\n",
    "        \n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "        \n",
    "        model.add(Dense(data_size, activation='tanh'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#識別器\n",
    "\n",
    "def build_discriminator(data_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128, activation=LeakyReLU(alpha=0.01), input_shape=(data_size,)))\n",
    "    \n",
    "    #model.add(LeakyReLU(alpha=0.01))\n",
    "        \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_compile(data, z_dim):\n",
    "    data_size = len(data[0])\n",
    "    #識別器の構築とコンパイル\n",
    "    discriminator = build_discriminator(data_size)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    #生成器の構築\n",
    "    generator = build_generator(data_size, z_dim)\n",
    "\n",
    "    #生成器の構築中は識別器のパラメータを固定\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    #生成器の訓練のため、識別器は固定し、GANモデルの構築とコンパイルを行う\n",
    "    gan = build_gan(generator, discriminator)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "    \n",
    "    return discriminator, generator, gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(data,labels):\n",
    "    index_1 = [i for i, x in enumerate(labels) if x == 1]\n",
    "    index_0 = [i for i, x in enumerate(labels) if x == 0]\n",
    "    index_0 = random.sample(index_0, len(index_1))\n",
    "    data_1 = [data[i] for i in index_1]\n",
    "    data_0 = [data[i] for i in index_0]\n",
    "    labels = [Label.TAISAKU]*len(data_1) + [Label.NASI]*len(data_0) \n",
    "    data = data_1 + data_0\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_y_train(data,labels):\n",
    "    index_1 = [i for i, x in enumerate(labels) if x == 1]\n",
    "    index_0 = [i for i, x in enumerate(labels) if x == 0]\n",
    "    index_0 = random.sample(index_0, len(index_1))\n",
    "    data_1 = [data[i] for i in index_1]\n",
    "    data_0 = [data[i] for i in index_0]\n",
    "\n",
    "    labels_1 = [Label.TAISAKU]*len(data_1)\n",
    "    labels_0 = [Label.NASI]*len(data_0) \n",
    "    data_0 = np.array(data_0)\n",
    "    labels_0 = np.array(labels_0)\n",
    "    data_1 = np.array(data_1)\n",
    "    labels_1 = np.array(labels_1)\n",
    "    return data_0, labels_0, data_1, labels_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "iteration_checkpoints = []\n",
    "\n",
    "\n",
    "def train(data, labels, iterations, batch_size, sample_interval):\n",
    "    \n",
    "    data_0, labels_0, data_1, lables_1 = x_y_train(data,labels)\n",
    "    #1の数7303\n",
    "    #0の数7303\n",
    "    \n",
    "    #ラベル1\n",
    "    real = np.ones((batch_size,1))\n",
    "    #ラベル0\n",
    "    fake = np.zeros((batch_size,1))\n",
    "    \n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        #-------------------\n",
    "        #識別器の訓練\n",
    "        #-------------------\n",
    "        \n",
    "        #ランダムに関係があるベクトルをとる\n",
    "        idx = np.random.randint(0,len(data_1),batch_size)\n",
    "        vecs = data_1[idx]\n",
    "        \n",
    "        \n",
    "        #word doc bert で生成する偽のベクトルの範囲を変化させる\n",
    "        if(len(data_1[0])==400):\n",
    "            z = np.random.normal(word_min, word_max,(batch_size, 100))\n",
    "        elif(len(data_1[0])==600):\n",
    "            z = np.random.normal(doc_min, doc_max,(batch_size, 100))\n",
    "        elif(len(data_1[0])==1536):\n",
    "            z = np.random.normal(bert_min, bert_max,(batch_size, 100))\n",
    "        \n",
    "        gen_vec = generator.predict(z)\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(vecs, real)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_vec, fake)\n",
    "        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        #-------------------\n",
    "        #生成器の訓練\n",
    "        #-------------------\n",
    "        if(len(data_1[0])==400):\n",
    "            z = np.random.normal(word_min, word_max,(batch_size, 100))\n",
    "        elif(len(data_1[0])==600):\n",
    "            z = np.random.normal(doc_min, doc_max,(batch_size, 100))\n",
    "        elif(len(data_1[0])==1536):\n",
    "            z = np.random.normal(bert_min, bert_max,(batch_size, 100))\n",
    "            \n",
    "        gen_vec = generator.predict(z)\n",
    "        \n",
    "        g_loss = gan.train_on_batch(z,real)\n",
    "        \n",
    "        if(iteration +1) % sample_interval ==0:\n",
    "            losses.append((d_loss, g_loss))\n",
    "            accuracies.append(100.0 * accuracy)\n",
    "            iteration_checkpoints.append(iteration +1)\n",
    "        \n",
    "        print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %(iteration +1,d_loss, 100.0*accuracy,g_loss))\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0, labels_0, data_1, lables_1 = x_y_train(data_word,labels_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randint(0,len(data_1),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 3004 calls to <function Model.make_train_function.<locals>.train_function at 0x1a3f924c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1 [D loss: 0.367484, acc.: 85.16%] [G loss: 1.631282]\n",
      "2 [D loss: 0.315841, acc.: 83.98%] [G loss: 1.672086]\n",
      "3 [D loss: 0.338613, acc.: 80.08%] [G loss: 1.641703]\n",
      "4 [D loss: 0.321846, acc.: 80.86%] [G loss: 1.565677]\n",
      "5 [D loss: 0.312600, acc.: 81.64%] [G loss: 1.592006]\n",
      "6 [D loss: 0.285491, acc.: 84.77%] [G loss: 1.609353]\n",
      "7 [D loss: 0.310954, acc.: 79.30%] [G loss: 1.786027]\n",
      "8 [D loss: 0.274887, acc.: 83.98%] [G loss: 1.979512]\n",
      "9 [D loss: 0.227357, acc.: 91.02%] [G loss: 1.879003]\n",
      "10 [D loss: 0.164901, acc.: 96.09%] [G loss: 2.244214]\n",
      "11 [D loss: 0.136092, acc.: 98.83%] [G loss: 2.536335]\n",
      "12 [D loss: 0.099817, acc.: 99.22%] [G loss: 2.831724]\n",
      "13 [D loss: 0.089699, acc.: 98.83%] [G loss: 3.255048]\n",
      "14 [D loss: 0.059855, acc.: 100.00%] [G loss: 3.548979]\n",
      "15 [D loss: 0.048536, acc.: 100.00%] [G loss: 3.621669]\n",
      "16 [D loss: 0.042818, acc.: 100.00%] [G loss: 3.789325]\n",
      "17 [D loss: 0.038055, acc.: 99.61%] [G loss: 3.735094]\n",
      "18 [D loss: 0.046841, acc.: 100.00%] [G loss: 3.601462]\n",
      "19 [D loss: 0.064488, acc.: 100.00%] [G loss: 3.596172]\n",
      "20 [D loss: 0.078336, acc.: 99.61%] [G loss: 3.391358]\n",
      "21 [D loss: 0.080919, acc.: 100.00%] [G loss: 3.479251]\n",
      "22 [D loss: 0.088720, acc.: 99.22%] [G loss: 3.478961]\n",
      "23 [D loss: 0.089120, acc.: 98.83%] [G loss: 3.570141]\n",
      "24 [D loss: 0.069973, acc.: 99.61%] [G loss: 3.796198]\n",
      "25 [D loss: 0.062662, acc.: 99.61%] [G loss: 3.852577]\n",
      "26 [D loss: 0.048440, acc.: 100.00%] [G loss: 4.158440]\n",
      "27 [D loss: 0.041699, acc.: 100.00%] [G loss: 4.038875]\n",
      "28 [D loss: 0.038204, acc.: 100.00%] [G loss: 4.222175]\n",
      "29 [D loss: 0.032143, acc.: 100.00%] [G loss: 4.091706]\n",
      "30 [D loss: 0.033747, acc.: 100.00%] [G loss: 4.244688]\n",
      "31 [D loss: 0.026104, acc.: 100.00%] [G loss: 4.268136]\n",
      "32 [D loss: 0.029664, acc.: 100.00%] [G loss: 4.289515]\n",
      "33 [D loss: 0.030625, acc.: 99.61%] [G loss: 4.431134]\n",
      "34 [D loss: 0.023248, acc.: 100.00%] [G loss: 4.581834]\n",
      "35 [D loss: 0.014750, acc.: 100.00%] [G loss: 4.805847]\n",
      "36 [D loss: 0.010247, acc.: 100.00%] [G loss: 5.011025]\n",
      "37 [D loss: 0.008049, acc.: 100.00%] [G loss: 5.121933]\n",
      "38 [D loss: 0.006765, acc.: 100.00%] [G loss: 5.163419]\n",
      "39 [D loss: 0.006395, acc.: 100.00%] [G loss: 5.296064]\n",
      "40 [D loss: 0.005388, acc.: 100.00%] [G loss: 5.380829]\n",
      "41 [D loss: 0.005070, acc.: 100.00%] [G loss: 5.337588]\n",
      "42 [D loss: 0.004896, acc.: 100.00%] [G loss: 5.263758]\n",
      "43 [D loss: 0.004989, acc.: 100.00%] [G loss: 5.179535]\n",
      "44 [D loss: 0.005937, acc.: 100.00%] [G loss: 5.069370]\n",
      "45 [D loss: 0.006314, acc.: 100.00%] [G loss: 4.954890]\n",
      "46 [D loss: 0.006538, acc.: 100.00%] [G loss: 4.912470]\n",
      "47 [D loss: 0.006503, acc.: 100.00%] [G loss: 5.001828]\n",
      "48 [D loss: 0.004955, acc.: 100.00%] [G loss: 5.156105]\n",
      "49 [D loss: 0.004086, acc.: 100.00%] [G loss: 5.385072]\n",
      "50 [D loss: 0.003003, acc.: 100.00%] [G loss: 5.587401]\n",
      "51 [D loss: 0.002859, acc.: 100.00%] [G loss: 5.709595]\n",
      "52 [D loss: 0.003221, acc.: 100.00%] [G loss: 5.746888]\n",
      "53 [D loss: 0.002965, acc.: 100.00%] [G loss: 5.679607]\n",
      "54 [D loss: 0.003526, acc.: 100.00%] [G loss: 5.458842]\n",
      "55 [D loss: 0.004096, acc.: 100.00%] [G loss: 5.310966]\n",
      "56 [D loss: 0.004283, acc.: 100.00%] [G loss: 5.262551]\n",
      "57 [D loss: 0.004208, acc.: 100.00%] [G loss: 5.248734]\n",
      "58 [D loss: 0.003637, acc.: 100.00%] [G loss: 5.397001]\n",
      "59 [D loss: 0.003166, acc.: 100.00%] [G loss: 5.423819]\n",
      "60 [D loss: 0.003022, acc.: 100.00%] [G loss: 5.489788]\n",
      "61 [D loss: 0.002500, acc.: 100.00%] [G loss: 5.647291]\n",
      "62 [D loss: 0.002308, acc.: 100.00%] [G loss: 5.803522]\n",
      "63 [D loss: 0.001956, acc.: 100.00%] [G loss: 5.993605]\n",
      "64 [D loss: 0.001666, acc.: 100.00%] [G loss: 6.164957]\n",
      "65 [D loss: 0.001468, acc.: 100.00%] [G loss: 6.243278]\n",
      "66 [D loss: 0.001504, acc.: 100.00%] [G loss: 6.207294]\n",
      "67 [D loss: 0.001639, acc.: 100.00%] [G loss: 6.130135]\n",
      "68 [D loss: 0.002199, acc.: 100.00%] [G loss: 5.975549]\n",
      "69 [D loss: 0.002449, acc.: 100.00%] [G loss: 5.685574]\n",
      "70 [D loss: 0.002337, acc.: 100.00%] [G loss: 5.627085]\n",
      "71 [D loss: 0.002915, acc.: 100.00%] [G loss: 5.669577]\n",
      "72 [D loss: 0.002399, acc.: 100.00%] [G loss: 5.659985]\n",
      "73 [D loss: 0.002523, acc.: 100.00%] [G loss: 5.784340]\n",
      "74 [D loss: 0.002288, acc.: 100.00%] [G loss: 5.853064]\n",
      "75 [D loss: 0.001989, acc.: 100.00%] [G loss: 5.863188]\n",
      "76 [D loss: 0.001993, acc.: 100.00%] [G loss: 5.962715]\n",
      "77 [D loss: 0.001805, acc.: 100.00%] [G loss: 5.963533]\n",
      "78 [D loss: 0.001700, acc.: 100.00%] [G loss: 6.120296]\n",
      "79 [D loss: 0.001410, acc.: 100.00%] [G loss: 6.228924]\n",
      "80 [D loss: 0.001260, acc.: 100.00%] [G loss: 6.316715]\n",
      "81 [D loss: 0.001199, acc.: 100.00%] [G loss: 6.428529]\n",
      "82 [D loss: 0.001077, acc.: 100.00%] [G loss: 6.470454]\n",
      "83 [D loss: 0.001333, acc.: 100.00%] [G loss: 6.529917]\n",
      "84 [D loss: 0.000982, acc.: 100.00%] [G loss: 6.488144]\n",
      "85 [D loss: 0.001058, acc.: 100.00%] [G loss: 6.496044]\n",
      "86 [D loss: 0.001098, acc.: 100.00%] [G loss: 6.435466]\n",
      "87 [D loss: 0.001395, acc.: 100.00%] [G loss: 6.454308]\n",
      "88 [D loss: 0.001056, acc.: 100.00%] [G loss: 6.403552]\n",
      "89 [D loss: 0.001175, acc.: 100.00%] [G loss: 6.437196]\n",
      "90 [D loss: 0.001165, acc.: 100.00%] [G loss: 6.466772]\n",
      "91 [D loss: 0.001157, acc.: 100.00%] [G loss: 6.455052]\n",
      "92 [D loss: 0.001094, acc.: 100.00%] [G loss: 6.440916]\n",
      "93 [D loss: 0.001200, acc.: 100.00%] [G loss: 6.281974]\n",
      "94 [D loss: 0.001457, acc.: 100.00%] [G loss: 6.082858]\n",
      "95 [D loss: 0.001771, acc.: 100.00%] [G loss: 5.928450]\n",
      "96 [D loss: 0.001721, acc.: 100.00%] [G loss: 5.941155]\n",
      "97 [D loss: 0.001650, acc.: 100.00%] [G loss: 6.105051]\n",
      "98 [D loss: 0.001225, acc.: 100.00%] [G loss: 6.303522]\n",
      "99 [D loss: 0.001171, acc.: 100.00%] [G loss: 6.449259]\n",
      "100 [D loss: 0.000976, acc.: 100.00%] [G loss: 6.518292]\n",
      "101 [D loss: 0.001002, acc.: 100.00%] [G loss: 6.550063]\n",
      "102 [D loss: 0.001036, acc.: 100.00%] [G loss: 6.506769]\n",
      "103 [D loss: 0.001148, acc.: 100.00%] [G loss: 6.375730]\n",
      "104 [D loss: 0.001102, acc.: 100.00%] [G loss: 6.335382]\n",
      "105 [D loss: 0.001127, acc.: 100.00%] [G loss: 6.405840]\n",
      "106 [D loss: 0.001000, acc.: 100.00%] [G loss: 6.482291]\n",
      "107 [D loss: 0.000995, acc.: 100.00%] [G loss: 6.558633]\n",
      "108 [D loss: 0.000990, acc.: 100.00%] [G loss: 6.543790]\n",
      "109 [D loss: 0.000992, acc.: 100.00%] [G loss: 6.505065]\n",
      "110 [D loss: 0.001000, acc.: 100.00%] [G loss: 6.468334]\n",
      "111 [D loss: 0.001089, acc.: 100.00%] [G loss: 6.421399]\n",
      "112 [D loss: 0.001446, acc.: 100.00%] [G loss: 6.326908]\n",
      "113 [D loss: 0.001613, acc.: 100.00%] [G loss: 6.157480]\n",
      "114 [D loss: 0.001846, acc.: 100.00%] [G loss: 6.021341]\n",
      "115 [D loss: 0.001785, acc.: 100.00%] [G loss: 6.078245]\n",
      "116 [D loss: 0.001556, acc.: 100.00%] [G loss: 6.167285]\n",
      "117 [D loss: 0.001129, acc.: 100.00%] [G loss: 6.435421]\n",
      "118 [D loss: 0.000791, acc.: 100.00%] [G loss: 6.775145]\n",
      "119 [D loss: 0.000731, acc.: 100.00%] [G loss: 7.007545]\n",
      "120 [D loss: 0.000723, acc.: 100.00%] [G loss: 6.915666]\n",
      "121 [D loss: 0.000898, acc.: 100.00%] [G loss: 6.828493]\n",
      "122 [D loss: 0.001326, acc.: 100.00%] [G loss: 6.393635]\n",
      "123 [D loss: 0.001739, acc.: 100.00%] [G loss: 6.172544]\n",
      "124 [D loss: 0.001906, acc.: 100.00%] [G loss: 6.112065]\n",
      "125 [D loss: 0.002222, acc.: 100.00%] [G loss: 5.900332]\n",
      "126 [D loss: 0.004494, acc.: 100.00%] [G loss: 5.578525]\n",
      "127 [D loss: 0.004430, acc.: 100.00%] [G loss: 5.568019]\n",
      "128 [D loss: 0.002857, acc.: 100.00%] [G loss: 5.877664]\n",
      "129 [D loss: 0.001484, acc.: 100.00%] [G loss: 6.525883]\n",
      "130 [D loss: 0.000700, acc.: 100.00%] [G loss: 7.144215]\n",
      "131 [D loss: 0.000608, acc.: 100.00%] [G loss: 7.669925]\n",
      "132 [D loss: 0.000638, acc.: 100.00%] [G loss: 8.012495]\n",
      "133 [D loss: 0.000294, acc.: 100.00%] [G loss: 8.344939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 [D loss: 0.000224, acc.: 100.00%] [G loss: 8.561267]\n",
      "135 [D loss: 0.000204, acc.: 100.00%] [G loss: 8.634319]\n",
      "136 [D loss: 0.000231, acc.: 100.00%] [G loss: 8.525746]\n",
      "137 [D loss: 0.000185, acc.: 100.00%] [G loss: 8.399386]\n",
      "138 [D loss: 0.000307, acc.: 100.00%] [G loss: 8.269097]\n",
      "139 [D loss: 0.000268, acc.: 100.00%] [G loss: 8.030260]\n",
      "140 [D loss: 0.000381, acc.: 100.00%] [G loss: 7.655058]\n",
      "141 [D loss: 0.000538, acc.: 100.00%] [G loss: 7.301559]\n",
      "142 [D loss: 0.000884, acc.: 100.00%] [G loss: 6.768270]\n",
      "143 [D loss: 0.001180, acc.: 100.00%] [G loss: 6.370023]\n",
      "144 [D loss: 0.001400, acc.: 100.00%] [G loss: 6.317873]\n",
      "145 [D loss: 0.001292, acc.: 100.00%] [G loss: 6.387547]\n",
      "146 [D loss: 0.001483, acc.: 100.00%] [G loss: 6.360975]\n",
      "147 [D loss: 0.001470, acc.: 100.00%] [G loss: 6.246059]\n",
      "148 [D loss: 0.001734, acc.: 100.00%] [G loss: 6.321857]\n",
      "149 [D loss: 0.001558, acc.: 100.00%] [G loss: 6.413896]\n",
      "150 [D loss: 0.001572, acc.: 100.00%] [G loss: 6.318861]\n",
      "151 [D loss: 0.001313, acc.: 100.00%] [G loss: 6.496330]\n",
      "152 [D loss: 0.001031, acc.: 100.00%] [G loss: 6.717336]\n",
      "153 [D loss: 0.000899, acc.: 100.00%] [G loss: 6.938983]\n",
      "154 [D loss: 0.000911, acc.: 100.00%] [G loss: 7.002123]\n",
      "155 [D loss: 0.000810, acc.: 100.00%] [G loss: 7.051442]\n",
      "156 [D loss: 0.001075, acc.: 100.00%] [G loss: 6.954548]\n",
      "157 [D loss: 0.000812, acc.: 100.00%] [G loss: 6.967371]\n",
      "158 [D loss: 0.000741, acc.: 100.00%] [G loss: 7.036328]\n",
      "159 [D loss: 0.000623, acc.: 100.00%] [G loss: 7.081930]\n",
      "160 [D loss: 0.000813, acc.: 100.00%] [G loss: 6.990191]\n",
      "161 [D loss: 0.001044, acc.: 100.00%] [G loss: 6.659185]\n",
      "162 [D loss: 0.001285, acc.: 100.00%] [G loss: 6.367016]\n",
      "163 [D loss: 0.001350, acc.: 100.00%] [G loss: 6.397459]\n",
      "164 [D loss: 0.001343, acc.: 100.00%] [G loss: 6.416622]\n",
      "165 [D loss: 0.001106, acc.: 100.00%] [G loss: 6.538778]\n",
      "166 [D loss: 0.001007, acc.: 100.00%] [G loss: 6.752556]\n",
      "167 [D loss: 0.000716, acc.: 100.00%] [G loss: 6.916238]\n",
      "168 [D loss: 0.001070, acc.: 100.00%] [G loss: 6.851647]\n",
      "169 [D loss: 0.001011, acc.: 100.00%] [G loss: 6.827600]\n",
      "170 [D loss: 0.001007, acc.: 100.00%] [G loss: 6.704207]\n",
      "171 [D loss: 0.001074, acc.: 100.00%] [G loss: 6.687994]\n",
      "172 [D loss: 0.001491, acc.: 100.00%] [G loss: 6.565287]\n",
      "173 [D loss: 0.002385, acc.: 100.00%] [G loss: 6.199644]\n",
      "174 [D loss: 0.003039, acc.: 100.00%] [G loss: 6.131361]\n",
      "175 [D loss: 0.002986, acc.: 100.00%] [G loss: 6.239203]\n",
      "176 [D loss: 0.002516, acc.: 100.00%] [G loss: 6.567438]\n",
      "177 [D loss: 0.001745, acc.: 100.00%] [G loss: 6.892361]\n",
      "178 [D loss: 0.001530, acc.: 100.00%] [G loss: 6.924986]\n",
      "179 [D loss: 0.003721, acc.: 100.00%] [G loss: 6.540246]\n",
      "180 [D loss: 0.002249, acc.: 100.00%] [G loss: 6.550757]\n",
      "181 [D loss: 0.001646, acc.: 100.00%] [G loss: 6.795810]\n",
      "182 [D loss: 0.001211, acc.: 100.00%] [G loss: 6.942601]\n",
      "183 [D loss: 0.001033, acc.: 100.00%] [G loss: 7.276471]\n",
      "184 [D loss: 0.000712, acc.: 100.00%] [G loss: 7.455441]\n",
      "185 [D loss: 0.001244, acc.: 100.00%] [G loss: 7.137244]\n",
      "186 [D loss: 0.003594, acc.: 100.00%] [G loss: 6.418223]\n",
      "187 [D loss: 0.010613, acc.: 100.00%] [G loss: 6.028002]\n",
      "188 [D loss: 0.003542, acc.: 100.00%] [G loss: 6.452731]\n",
      "189 [D loss: 0.000735, acc.: 100.00%] [G loss: 7.921065]\n",
      "190 [D loss: 0.001089, acc.: 100.00%] [G loss: 9.000631]\n",
      "191 [D loss: 0.000266, acc.: 100.00%] [G loss: 9.600168]\n",
      "192 [D loss: 0.000116, acc.: 100.00%] [G loss: 9.701666]\n",
      "193 [D loss: 0.000145, acc.: 100.00%] [G loss: 9.756142]\n",
      "194 [D loss: 0.000262, acc.: 100.00%] [G loss: 9.613543]\n",
      "195 [D loss: 0.000240, acc.: 100.00%] [G loss: 9.066058]\n",
      "196 [D loss: 0.000746, acc.: 100.00%] [G loss: 7.767250]\n",
      "197 [D loss: 0.001183, acc.: 100.00%] [G loss: 6.737316]\n",
      "198 [D loss: 0.001616, acc.: 100.00%] [G loss: 6.163351]\n",
      "199 [D loss: 0.003180, acc.: 100.00%] [G loss: 5.735230]\n",
      "200 [D loss: 0.005003, acc.: 100.00%] [G loss: 5.515393]\n",
      "201 [D loss: 0.003635, acc.: 100.00%] [G loss: 5.829406]\n",
      "202 [D loss: 0.001974, acc.: 100.00%] [G loss: 6.516993]\n",
      "203 [D loss: 0.001265, acc.: 100.00%] [G loss: 6.848156]\n",
      "204 [D loss: 0.001544, acc.: 100.00%] [G loss: 7.180574]\n",
      "205 [D loss: 0.000754, acc.: 100.00%] [G loss: 7.399157]\n",
      "206 [D loss: 0.000912, acc.: 100.00%] [G loss: 7.244082]\n",
      "207 [D loss: 0.001480, acc.: 100.00%] [G loss: 6.661481]\n",
      "208 [D loss: 0.003178, acc.: 100.00%] [G loss: 6.002463]\n",
      "209 [D loss: 0.004315, acc.: 100.00%] [G loss: 5.726175]\n",
      "210 [D loss: 0.002810, acc.: 100.00%] [G loss: 6.192513]\n",
      "211 [D loss: 0.001091, acc.: 100.00%] [G loss: 6.966919]\n",
      "212 [D loss: 0.000587, acc.: 100.00%] [G loss: 7.573411]\n",
      "213 [D loss: 0.000414, acc.: 100.00%] [G loss: 7.957458]\n",
      "214 [D loss: 0.000319, acc.: 100.00%] [G loss: 8.096449]\n",
      "215 [D loss: 0.000306, acc.: 100.00%] [G loss: 8.080667]\n",
      "216 [D loss: 0.000303, acc.: 100.00%] [G loss: 7.984774]\n",
      "217 [D loss: 0.000496, acc.: 100.00%] [G loss: 7.673804]\n",
      "218 [D loss: 0.000436, acc.: 100.00%] [G loss: 7.361795]\n",
      "219 [D loss: 0.000600, acc.: 100.00%] [G loss: 7.484739]\n",
      "220 [D loss: 0.000533, acc.: 100.00%] [G loss: 7.541050]\n",
      "221 [D loss: 0.000437, acc.: 100.00%] [G loss: 7.581996]\n",
      "222 [D loss: 0.000452, acc.: 100.00%] [G loss: 7.331993]\n",
      "223 [D loss: 0.000453, acc.: 100.00%] [G loss: 7.270366]\n",
      "224 [D loss: 0.000396, acc.: 100.00%] [G loss: 7.470406]\n",
      "225 [D loss: 0.000384, acc.: 100.00%] [G loss: 7.632427]\n",
      "226 [D loss: 0.000329, acc.: 100.00%] [G loss: 7.623838]\n",
      "227 [D loss: 0.000390, acc.: 100.00%] [G loss: 7.593740]\n",
      "228 [D loss: 0.000402, acc.: 100.00%] [G loss: 7.670310]\n",
      "229 [D loss: 0.000365, acc.: 100.00%] [G loss: 7.693311]\n",
      "230 [D loss: 0.000442, acc.: 100.00%] [G loss: 7.442200]\n",
      "231 [D loss: 0.000582, acc.: 100.00%] [G loss: 6.994507]\n",
      "232 [D loss: 0.000629, acc.: 100.00%] [G loss: 6.914269]\n",
      "233 [D loss: 0.000604, acc.: 100.00%] [G loss: 6.988189]\n",
      "234 [D loss: 0.000524, acc.: 100.00%] [G loss: 7.179029]\n",
      "235 [D loss: 0.000422, acc.: 100.00%] [G loss: 7.342449]\n",
      "236 [D loss: 0.000379, acc.: 100.00%] [G loss: 7.410273]\n",
      "237 [D loss: 0.000463, acc.: 100.00%] [G loss: 7.300932]\n",
      "238 [D loss: 0.000615, acc.: 100.00%] [G loss: 7.189692]\n",
      "239 [D loss: 0.000449, acc.: 100.00%] [G loss: 7.221693]\n",
      "240 [D loss: 0.000510, acc.: 100.00%] [G loss: 7.319323]\n",
      "241 [D loss: 0.000420, acc.: 100.00%] [G loss: 7.446438]\n",
      "242 [D loss: 0.000468, acc.: 100.00%] [G loss: 7.448224]\n",
      "243 [D loss: 0.000516, acc.: 100.00%] [G loss: 7.403426]\n",
      "244 [D loss: 0.000440, acc.: 100.00%] [G loss: 7.362367]\n",
      "245 [D loss: 0.000466, acc.: 100.00%] [G loss: 7.306608]\n",
      "246 [D loss: 0.000511, acc.: 100.00%] [G loss: 7.290808]\n",
      "247 [D loss: 0.000425, acc.: 100.00%] [G loss: 7.353328]\n",
      "248 [D loss: 0.000403, acc.: 100.00%] [G loss: 7.453579]\n",
      "249 [D loss: 0.000353, acc.: 100.00%] [G loss: 7.612656]\n",
      "250 [D loss: 0.000324, acc.: 100.00%] [G loss: 7.732586]\n",
      "251 [D loss: 0.000283, acc.: 100.00%] [G loss: 7.861143]\n",
      "252 [D loss: 0.000217, acc.: 100.00%] [G loss: 8.012209]\n",
      "253 [D loss: 0.000256, acc.: 100.00%] [G loss: 8.025777]\n",
      "254 [D loss: 0.000252, acc.: 100.00%] [G loss: 7.948757]\n",
      "255 [D loss: 0.000223, acc.: 100.00%] [G loss: 8.059038]\n",
      "256 [D loss: 0.000197, acc.: 100.00%] [G loss: 8.223522]\n",
      "257 [D loss: 0.000173, acc.: 100.00%] [G loss: 8.286568]\n",
      "258 [D loss: 0.000165, acc.: 100.00%] [G loss: 8.272339]\n",
      "259 [D loss: 0.000163, acc.: 100.00%] [G loss: 8.279074]\n",
      "260 [D loss: 0.000195, acc.: 100.00%] [G loss: 8.187721]\n",
      "261 [D loss: 0.000219, acc.: 100.00%] [G loss: 8.220778]\n",
      "262 [D loss: 0.000184, acc.: 100.00%] [G loss: 8.383498]\n",
      "263 [D loss: 0.000148, acc.: 100.00%] [G loss: 8.498505]\n",
      "264 [D loss: 0.000156, acc.: 100.00%] [G loss: 8.411253]\n",
      "265 [D loss: 0.000173, acc.: 100.00%] [G loss: 8.253834]\n",
      "266 [D loss: 0.000215, acc.: 100.00%] [G loss: 8.162350]\n",
      "267 [D loss: 0.000162, acc.: 100.00%] [G loss: 8.239201]\n",
      "268 [D loss: 0.000177, acc.: 100.00%] [G loss: 8.396446]\n",
      "269 [D loss: 0.000128, acc.: 100.00%] [G loss: 8.555824]\n",
      "270 [D loss: 0.000122, acc.: 100.00%] [G loss: 8.702721]\n",
      "271 [D loss: 0.000089, acc.: 100.00%] [G loss: 8.832258]\n",
      "272 [D loss: 0.000092, acc.: 100.00%] [G loss: 8.934257]\n",
      "273 [D loss: 0.000092, acc.: 100.00%] [G loss: 9.047179]\n",
      "274 [D loss: 0.000077, acc.: 100.00%] [G loss: 9.157256]\n",
      "275 [D loss: 0.000090, acc.: 100.00%] [G loss: 9.217854]\n",
      "276 [D loss: 0.000084, acc.: 100.00%] [G loss: 9.124014]\n",
      "277 [D loss: 0.000090, acc.: 100.00%] [G loss: 8.895618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278 [D loss: 0.000108, acc.: 100.00%] [G loss: 8.699661]\n",
      "279 [D loss: 0.000146, acc.: 100.00%] [G loss: 8.564693]\n",
      "280 [D loss: 0.000116, acc.: 100.00%] [G loss: 8.623411]\n",
      "281 [D loss: 0.000106, acc.: 100.00%] [G loss: 8.726044]\n",
      "282 [D loss: 0.000107, acc.: 100.00%] [G loss: 8.814136]\n",
      "283 [D loss: 0.000097, acc.: 100.00%] [G loss: 8.838055]\n",
      "284 [D loss: 0.000134, acc.: 100.00%] [G loss: 8.744215]\n",
      "285 [D loss: 0.000113, acc.: 100.00%] [G loss: 8.610493]\n",
      "286 [D loss: 0.000126, acc.: 100.00%] [G loss: 8.497830]\n",
      "287 [D loss: 0.000144, acc.: 100.00%] [G loss: 8.554782]\n",
      "288 [D loss: 0.000123, acc.: 100.00%] [G loss: 8.452472]\n",
      "289 [D loss: 0.000136, acc.: 100.00%] [G loss: 8.408625]\n",
      "290 [D loss: 0.000182, acc.: 100.00%] [G loss: 8.471846]\n",
      "291 [D loss: 0.000153, acc.: 100.00%] [G loss: 8.531819]\n",
      "292 [D loss: 0.000118, acc.: 100.00%] [G loss: 8.591497]\n",
      "293 [D loss: 0.000137, acc.: 100.00%] [G loss: 8.679623]\n",
      "294 [D loss: 0.000130, acc.: 100.00%] [G loss: 8.721046]\n",
      "295 [D loss: 0.000102, acc.: 100.00%] [G loss: 8.731931]\n",
      "296 [D loss: 0.000114, acc.: 100.00%] [G loss: 8.728331]\n",
      "297 [D loss: 0.000095, acc.: 100.00%] [G loss: 8.741369]\n",
      "298 [D loss: 0.000106, acc.: 100.00%] [G loss: 8.846488]\n",
      "299 [D loss: 0.000095, acc.: 100.00%] [G loss: 8.966879]\n",
      "300 [D loss: 0.000082, acc.: 100.00%] [G loss: 9.083181]\n",
      "301 [D loss: 0.000093, acc.: 100.00%] [G loss: 9.186470]\n",
      "302 [D loss: 0.000077, acc.: 100.00%] [G loss: 9.252914]\n",
      "303 [D loss: 0.000062, acc.: 100.00%] [G loss: 9.262460]\n",
      "304 [D loss: 0.000069, acc.: 100.00%] [G loss: 9.292372]\n",
      "305 [D loss: 0.000077, acc.: 100.00%] [G loss: 9.371202]\n",
      "306 [D loss: 0.000071, acc.: 100.00%] [G loss: 9.436666]\n",
      "307 [D loss: 0.000073, acc.: 100.00%] [G loss: 9.475558]\n",
      "308 [D loss: 0.000067, acc.: 100.00%] [G loss: 9.430626]\n",
      "309 [D loss: 0.000087, acc.: 100.00%] [G loss: 9.420170]\n",
      "310 [D loss: 0.000073, acc.: 100.00%] [G loss: 9.470909]\n",
      "311 [D loss: 0.000052, acc.: 100.00%] [G loss: 9.535982]\n",
      "312 [D loss: 0.000052, acc.: 100.00%] [G loss: 9.599270]\n",
      "313 [D loss: 0.000056, acc.: 100.00%] [G loss: 9.659912]\n",
      "314 [D loss: 0.000064, acc.: 100.00%] [G loss: 9.717793]\n",
      "315 [D loss: 0.000054, acc.: 100.00%] [G loss: 9.773123]\n",
      "316 [D loss: 0.000063, acc.: 100.00%] [G loss: 9.825347]\n",
      "317 [D loss: 0.000061, acc.: 100.00%] [G loss: 9.868914]\n",
      "318 [D loss: 0.000067, acc.: 100.00%] [G loss: 9.874468]\n",
      "319 [D loss: 0.000041, acc.: 100.00%] [G loss: 9.760521]\n",
      "320 [D loss: 0.000064, acc.: 100.00%] [G loss: 9.655627]\n",
      "321 [D loss: 0.000060, acc.: 100.00%] [G loss: 9.688397]\n",
      "322 [D loss: 0.000043, acc.: 100.00%] [G loss: 9.730844]\n",
      "323 [D loss: 0.000039, acc.: 100.00%] [G loss: 9.745955]\n",
      "324 [D loss: 0.000056, acc.: 100.00%] [G loss: 9.724363]\n",
      "325 [D loss: 0.000049, acc.: 100.00%] [G loss: 9.746019]\n",
      "326 [D loss: 0.000044, acc.: 100.00%] [G loss: 9.792199]\n",
      "327 [D loss: 0.000067, acc.: 100.00%] [G loss: 9.838805]\n",
      "328 [D loss: 0.000055, acc.: 100.00%] [G loss: 9.884666]\n",
      "329 [D loss: 0.000041, acc.: 100.00%] [G loss: 9.929787]\n",
      "330 [D loss: 0.000045, acc.: 100.00%] [G loss: 9.973856]\n",
      "331 [D loss: 0.000034, acc.: 100.00%] [G loss: 10.016850]\n",
      "332 [D loss: 0.000049, acc.: 100.00%] [G loss: 10.057386]\n",
      "333 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.096728]\n",
      "334 [D loss: 0.000036, acc.: 100.00%] [G loss: 10.134801]\n",
      "335 [D loss: 0.000038, acc.: 100.00%] [G loss: 10.171610]\n",
      "336 [D loss: 0.000043, acc.: 100.00%] [G loss: 10.207040]\n",
      "337 [D loss: 0.000040, acc.: 100.00%] [G loss: 10.241174]\n",
      "338 [D loss: 0.000036, acc.: 100.00%] [G loss: 10.274137]\n",
      "339 [D loss: 0.000047, acc.: 100.00%] [G loss: 10.305799]\n",
      "340 [D loss: 0.000028, acc.: 100.00%] [G loss: 10.336596]\n",
      "341 [D loss: 0.000039, acc.: 100.00%] [G loss: 10.366390]\n",
      "342 [D loss: 0.000034, acc.: 100.00%] [G loss: 10.395275]\n",
      "343 [D loss: 0.000038, acc.: 100.00%] [G loss: 10.423127]\n",
      "344 [D loss: 0.000032, acc.: 100.00%] [G loss: 10.450146]\n",
      "345 [D loss: 0.000083, acc.: 100.00%] [G loss: 10.475289]\n",
      "346 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.498600]\n",
      "347 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.519294]\n",
      "348 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.519882]\n",
      "349 [D loss: 0.000032, acc.: 100.00%] [G loss: 10.497551]\n",
      "350 [D loss: 0.000046, acc.: 100.00%] [G loss: 10.489966]\n",
      "351 [D loss: 0.000027, acc.: 100.00%] [G loss: 10.497030]\n",
      "352 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.515487]\n",
      "353 [D loss: 0.000035, acc.: 100.00%] [G loss: 10.537043]\n",
      "354 [D loss: 0.000032, acc.: 100.00%] [G loss: 10.558506]\n",
      "355 [D loss: 0.000072, acc.: 100.00%] [G loss: 10.578827]\n",
      "356 [D loss: 0.000021, acc.: 100.00%] [G loss: 10.599292]\n",
      "357 [D loss: 0.000035, acc.: 100.00%] [G loss: 10.619407]\n",
      "358 [D loss: 0.000036, acc.: 100.00%] [G loss: 10.639255]\n",
      "359 [D loss: 0.000026, acc.: 100.00%] [G loss: 10.659021]\n",
      "360 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.678688]\n",
      "361 [D loss: 0.000025, acc.: 100.00%] [G loss: 10.698164]\n",
      "362 [D loss: 0.000026, acc.: 100.00%] [G loss: 10.717371]\n",
      "363 [D loss: 0.000032, acc.: 100.00%] [G loss: 10.736217]\n",
      "364 [D loss: 0.000032, acc.: 100.00%] [G loss: 10.754715]\n",
      "365 [D loss: 0.000051, acc.: 100.00%] [G loss: 10.772431]\n",
      "366 [D loss: 0.000046, acc.: 100.00%] [G loss: 10.789429]\n",
      "367 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.806323]\n",
      "368 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.823092]\n",
      "369 [D loss: 0.000025, acc.: 100.00%] [G loss: 10.839754]\n",
      "370 [D loss: 0.000017, acc.: 100.00%] [G loss: 10.856457]\n",
      "371 [D loss: 0.000018, acc.: 100.00%] [G loss: 10.873081]\n",
      "372 [D loss: 0.000034, acc.: 100.00%] [G loss: 10.889278]\n",
      "373 [D loss: 0.000025, acc.: 100.00%] [G loss: 10.905172]\n",
      "374 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.920938]\n",
      "375 [D loss: 0.000017, acc.: 100.00%] [G loss: 10.936646]\n",
      "376 [D loss: 0.000015, acc.: 100.00%] [G loss: 10.952324]\n",
      "377 [D loss: 0.000055, acc.: 100.00%] [G loss: 10.966816]\n",
      "378 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.981094]\n",
      "379 [D loss: 0.000018, acc.: 100.00%] [G loss: 10.995433]\n",
      "380 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.009678]\n",
      "381 [D loss: 0.000052, acc.: 100.00%] [G loss: 11.022941]\n",
      "382 [D loss: 0.000051, acc.: 100.00%] [G loss: 11.035638]\n",
      "383 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.048656]\n",
      "384 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.061687]\n",
      "385 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.074800]\n",
      "386 [D loss: 0.000023, acc.: 100.00%] [G loss: 11.087915]\n",
      "387 [D loss: 0.000043, acc.: 100.00%] [G loss: 11.100258]\n",
      "388 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.112823]\n",
      "389 [D loss: 0.000059, acc.: 100.00%] [G loss: 11.124906]\n",
      "390 [D loss: 0.000056, acc.: 100.00%] [G loss: 11.136290]\n",
      "391 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.147985]\n",
      "392 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.159793]\n",
      "393 [D loss: 0.000023, acc.: 100.00%] [G loss: 11.171529]\n",
      "394 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.183334]\n",
      "395 [D loss: 0.000036, acc.: 100.00%] [G loss: 11.194715]\n",
      "396 [D loss: 0.000037, acc.: 100.00%] [G loss: 11.205782]\n",
      "397 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.216966]\n",
      "398 [D loss: 0.000032, acc.: 100.00%] [G loss: 11.227874]\n",
      "399 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.239026]\n",
      "400 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.250340]\n",
      "401 [D loss: 0.000024, acc.: 100.00%] [G loss: 11.261460]\n",
      "402 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.272611]\n",
      "403 [D loss: 0.000032, acc.: 100.00%] [G loss: 11.283427]\n",
      "404 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.293760]\n",
      "405 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.293545]\n",
      "406 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.237094]\n",
      "407 [D loss: 0.000038, acc.: 100.00%] [G loss: 11.067022]\n",
      "408 [D loss: 0.000014, acc.: 100.00%] [G loss: 10.993450]\n",
      "409 [D loss: 0.000024, acc.: 100.00%] [G loss: 11.001329]\n",
      "410 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.014351]\n",
      "411 [D loss: 0.000028, acc.: 100.00%] [G loss: 11.027620]\n",
      "412 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.041311]\n",
      "413 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.055262]\n",
      "414 [D loss: 0.000041, acc.: 100.00%] [G loss: 11.068696]\n",
      "415 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.082462]\n",
      "416 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.095919]\n",
      "417 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.108320]\n",
      "418 [D loss: 0.000025, acc.: 100.00%] [G loss: 11.103960]\n",
      "419 [D loss: 0.000032, acc.: 100.00%] [G loss: 11.058489]\n",
      "420 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.005327]\n",
      "421 [D loss: 0.000018, acc.: 100.00%] [G loss: 10.998125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.012309]\n",
      "423 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.027472]\n",
      "424 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.042933]\n",
      "425 [D loss: 0.000034, acc.: 100.00%] [G loss: 11.058044]\n",
      "426 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.073267]\n",
      "427 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.088463]\n",
      "428 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.103693]\n",
      "429 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.119038]\n",
      "430 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.134266]\n",
      "431 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.149232]\n",
      "432 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.164167]\n",
      "433 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.178942]\n",
      "434 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.193684]\n",
      "435 [D loss: 0.000024, acc.: 100.00%] [G loss: 11.208045]\n",
      "436 [D loss: 0.000030, acc.: 100.00%] [G loss: 11.221914]\n",
      "437 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.235670]\n",
      "438 [D loss: 0.000025, acc.: 100.00%] [G loss: 11.248985]\n",
      "439 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.262341]\n",
      "440 [D loss: 0.000031, acc.: 100.00%] [G loss: 11.275243]\n",
      "441 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.288027]\n",
      "442 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.300623]\n",
      "443 [D loss: 0.000030, acc.: 100.00%] [G loss: 11.312729]\n",
      "444 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.324904]\n",
      "445 [D loss: 0.000024, acc.: 100.00%] [G loss: 11.336842]\n",
      "446 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.348927]\n",
      "447 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.361103]\n",
      "448 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.372713]\n",
      "449 [D loss: 0.000029, acc.: 100.00%] [G loss: 11.378607]\n",
      "450 [D loss: 0.000028, acc.: 100.00%] [G loss: 11.366690]\n",
      "451 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.364894]\n",
      "452 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.375545]\n",
      "453 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.387104]\n",
      "454 [D loss: 0.000033, acc.: 100.00%] [G loss: 11.398149]\n",
      "455 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.409302]\n",
      "456 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.420348]\n",
      "457 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.431438]\n",
      "458 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.442482]\n",
      "459 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.453231]\n",
      "460 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.459103]\n",
      "461 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.451172]\n",
      "462 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.436787]\n",
      "463 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.388470]\n",
      "464 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.353909]\n",
      "465 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.358507]\n",
      "466 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.370261]\n",
      "467 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.382315]\n",
      "468 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.394505]\n",
      "469 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.406775]\n",
      "470 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.419022]\n",
      "471 [D loss: 0.000024, acc.: 100.00%] [G loss: 11.430927]\n",
      "472 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.442684]\n",
      "473 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.454531]\n",
      "474 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.458975]\n",
      "475 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.438467]\n",
      "476 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.394050]\n",
      "477 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.382882]\n",
      "478 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.391149]\n",
      "479 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.400232]\n",
      "480 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.403231]\n",
      "481 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.380990]\n",
      "482 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.375319]\n",
      "483 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.386253]\n",
      "484 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.394854]\n",
      "485 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.358823]\n",
      "486 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.152897]\n",
      "487 [D loss: 0.000022, acc.: 100.00%] [G loss: 11.021811]\n",
      "488 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.026103]\n",
      "489 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.041037]\n",
      "490 [D loss: 0.000027, acc.: 100.00%] [G loss: 11.056215]\n",
      "491 [D loss: 0.000028, acc.: 100.00%] [G loss: 11.071518]\n",
      "492 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.087053]\n",
      "493 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.102985]\n",
      "494 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.119101]\n",
      "495 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.135353]\n",
      "496 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.151747]\n",
      "497 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.168142]\n",
      "498 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.184519]\n",
      "499 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.199820]\n",
      "500 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.207613]\n",
      "501 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.180645]\n",
      "502 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.071235]\n",
      "503 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.052811]\n",
      "504 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.069284]\n",
      "505 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.086101]\n",
      "506 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.103091]\n",
      "507 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.120170]\n",
      "508 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.137241]\n",
      "509 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.154255]\n",
      "510 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.171263]\n",
      "511 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.188041]\n",
      "512 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.204762]\n",
      "513 [D loss: 0.000021, acc.: 100.00%] [G loss: 11.221057]\n",
      "514 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.237288]\n",
      "515 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.253071]\n",
      "516 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.268718]\n",
      "517 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.284191]\n",
      "518 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.295572]\n",
      "519 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.272897]\n",
      "520 [D loss: 0.000016, acc.: 100.00%] [G loss: 11.146564]\n",
      "521 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.110859]\n",
      "522 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.126665]\n",
      "523 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.143204]\n",
      "524 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.159913]\n",
      "525 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.176714]\n",
      "526 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.193558]\n",
      "527 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.210292]\n",
      "528 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.226939]\n",
      "529 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.243549]\n",
      "530 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.259884]\n",
      "531 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.276028]\n",
      "532 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.291948]\n",
      "533 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.307729]\n",
      "534 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.323462]\n",
      "535 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.338974]\n",
      "536 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.354249]\n",
      "537 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.369284]\n",
      "538 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.384144]\n",
      "539 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.398899]\n",
      "540 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.413509]\n",
      "541 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.427885]\n",
      "542 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.442159]\n",
      "543 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.456093]\n",
      "544 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.469860]\n",
      "545 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.483507]\n",
      "546 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.497002]\n",
      "547 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.510248]\n",
      "548 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.523390]\n",
      "549 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.536305]\n",
      "550 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.549115]\n",
      "551 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.561799]\n",
      "552 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.574383]\n",
      "553 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.586887]\n",
      "554 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.599261]\n",
      "555 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.611477]\n",
      "556 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.623436]\n",
      "557 [D loss: 0.000020, acc.: 100.00%] [G loss: 11.635103]\n",
      "558 [D loss: 0.000014, acc.: 100.00%] [G loss: 11.646519]\n",
      "559 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.657788]\n",
      "560 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.668919]\n",
      "561 [D loss: 0.000012, acc.: 100.00%] [G loss: 11.679937]\n",
      "562 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.690895]\n",
      "563 [D loss: 0.000018, acc.: 100.00%] [G loss: 11.701670]\n",
      "564 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.712427]\n",
      "565 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.722931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566 [D loss: 0.000007, acc.: 100.00%] [G loss: 11.733463]\n",
      "567 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.743974]\n",
      "568 [D loss: 0.000017, acc.: 100.00%] [G loss: 11.754217]\n",
      "569 [D loss: 0.000019, acc.: 100.00%] [G loss: 11.764277]\n",
      "570 [D loss: 0.000007, acc.: 100.00%] [G loss: 11.774398]\n",
      "571 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.784488]\n",
      "572 [D loss: 0.000013, acc.: 100.00%] [G loss: 11.794421]\n",
      "573 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.804272]\n",
      "574 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.814028]\n",
      "575 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.823761]\n",
      "576 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.833481]\n",
      "577 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.843099]\n",
      "578 [D loss: 0.000007, acc.: 100.00%] [G loss: 11.852702]\n",
      "579 [D loss: 0.000007, acc.: 100.00%] [G loss: 11.862280]\n",
      "580 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.871799]\n",
      "581 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.881229]\n",
      "582 [D loss: 0.000011, acc.: 100.00%] [G loss: 11.890476]\n",
      "583 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.899663]\n",
      "584 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.908738]\n",
      "585 [D loss: 0.000006, acc.: 100.00%] [G loss: 11.917814]\n",
      "586 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.926749]\n",
      "587 [D loss: 0.000015, acc.: 100.00%] [G loss: 11.935465]\n",
      "588 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.944148]\n",
      "589 [D loss: 0.000006, acc.: 100.00%] [G loss: 11.952870]\n",
      "590 [D loss: 0.000007, acc.: 100.00%] [G loss: 11.961569]\n",
      "591 [D loss: 0.000008, acc.: 100.00%] [G loss: 11.970218]\n",
      "592 [D loss: 0.000010, acc.: 100.00%] [G loss: 11.978711]\n",
      "593 [D loss: 0.000009, acc.: 100.00%] [G loss: 11.987129]\n",
      "594 [D loss: 0.000007, acc.: 100.00%] [G loss: 11.995535]\n",
      "595 [D loss: 0.000011, acc.: 100.00%] [G loss: 12.003832]\n",
      "596 [D loss: 0.000009, acc.: 100.00%] [G loss: 12.012016]\n",
      "597 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.020227]\n",
      "598 [D loss: 0.000009, acc.: 100.00%] [G loss: 12.028387]\n",
      "599 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.036470]\n",
      "600 [D loss: 0.000009, acc.: 100.00%] [G loss: 12.044497]\n",
      "601 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.052491]\n",
      "602 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.060485]\n",
      "603 [D loss: 0.000011, acc.: 100.00%] [G loss: 12.068293]\n",
      "604 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.076104]\n",
      "605 [D loss: 0.000012, acc.: 100.00%] [G loss: 12.083703]\n",
      "606 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.091381]\n",
      "607 [D loss: 0.000017, acc.: 100.00%] [G loss: 12.098707]\n",
      "608 [D loss: 0.000010, acc.: 100.00%] [G loss: 12.106028]\n",
      "609 [D loss: 0.000009, acc.: 100.00%] [G loss: 12.113298]\n",
      "610 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.120644]\n",
      "611 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.128055]\n",
      "612 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.135495]\n",
      "613 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.142928]\n",
      "614 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.150328]\n",
      "615 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.157750]\n",
      "616 [D loss: 0.000015, acc.: 100.00%] [G loss: 12.164918]\n",
      "617 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.172041]\n",
      "618 [D loss: 0.000010, acc.: 100.00%] [G loss: 12.179008]\n",
      "619 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.186031]\n",
      "620 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.193065]\n",
      "621 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.200058]\n",
      "622 [D loss: 0.000009, acc.: 100.00%] [G loss: 12.207003]\n",
      "623 [D loss: 0.000013, acc.: 100.00%] [G loss: 12.213793]\n",
      "624 [D loss: 0.000009, acc.: 100.00%] [G loss: 12.220543]\n",
      "625 [D loss: 0.000010, acc.: 100.00%] [G loss: 12.227219]\n",
      "626 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.233870]\n",
      "627 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.240555]\n",
      "628 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.247254]\n",
      "629 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.253986]\n",
      "630 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.260689]\n",
      "631 [D loss: 0.000014, acc.: 100.00%] [G loss: 12.267163]\n",
      "632 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.273686]\n",
      "633 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.280301]\n",
      "634 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.286921]\n",
      "635 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.293594]\n",
      "636 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.300224]\n",
      "637 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.306831]\n",
      "638 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.313461]\n",
      "639 [D loss: 0.000011, acc.: 100.00%] [G loss: 12.319939]\n",
      "640 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.326379]\n",
      "641 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.332769]\n",
      "642 [D loss: 0.000014, acc.: 100.00%] [G loss: 12.338961]\n",
      "643 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.345244]\n",
      "644 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.351561]\n",
      "645 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.357821]\n",
      "646 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.364037]\n",
      "647 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.367393]\n",
      "648 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.359374]\n",
      "649 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.349524]\n",
      "650 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.354790]\n",
      "651 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.361176]\n",
      "652 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.367546]\n",
      "653 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.373995]\n",
      "654 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.380437]\n",
      "655 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.386816]\n",
      "656 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.393145]\n",
      "657 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.399469]\n",
      "658 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.405794]\n",
      "659 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.412155]\n",
      "660 [D loss: 0.000014, acc.: 100.00%] [G loss: 12.418344]\n",
      "661 [D loss: 0.000014, acc.: 100.00%] [G loss: 12.424265]\n",
      "662 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.430111]\n",
      "663 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.436039]\n",
      "664 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.442062]\n",
      "665 [D loss: 0.000009, acc.: 100.00%] [G loss: 12.447939]\n",
      "666 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.453733]\n",
      "667 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.459509]\n",
      "668 [D loss: 0.000009, acc.: 100.00%] [G loss: 12.465196]\n",
      "669 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.470932]\n",
      "670 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.476690]\n",
      "671 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.482447]\n",
      "672 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.488068]\n",
      "673 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.493752]\n",
      "674 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.499451]\n",
      "675 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.505140]\n",
      "676 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.510902]\n",
      "677 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.516632]\n",
      "678 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.522354]\n",
      "679 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.527964]\n",
      "680 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.533592]\n",
      "681 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.539207]\n",
      "682 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.544714]\n",
      "683 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.550140]\n",
      "684 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.555576]\n",
      "685 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.560993]\n",
      "686 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.566320]\n",
      "687 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.571731]\n",
      "688 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.577118]\n",
      "689 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.582407]\n",
      "690 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.587727]\n",
      "691 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.592928]\n",
      "692 [D loss: 0.000012, acc.: 100.00%] [G loss: 12.598005]\n",
      "693 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.603111]\n",
      "694 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.608282]\n",
      "695 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.613487]\n",
      "696 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.618587]\n",
      "697 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.623657]\n",
      "698 [D loss: 0.000011, acc.: 100.00%] [G loss: 12.628496]\n",
      "699 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.633410]\n",
      "700 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.638132]\n",
      "701 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.630367]\n",
      "702 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.564482]\n",
      "703 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.416207]\n",
      "704 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.397796]\n",
      "705 [D loss: 0.000012, acc.: 100.00%] [G loss: 12.403003]\n",
      "706 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.408529]\n",
      "707 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.414189]\n",
      "708 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.419964]\n",
      "709 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.425820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "710 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.431799]\n",
      "711 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.437815]\n",
      "712 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.443857]\n",
      "713 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.449972]\n",
      "714 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.456144]\n",
      "715 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.462295]\n",
      "716 [D loss: 0.000010, acc.: 100.00%] [G loss: 12.468410]\n",
      "717 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.474571]\n",
      "718 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.480762]\n",
      "719 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.486830]\n",
      "720 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.492950]\n",
      "721 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.499076]\n",
      "722 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.505175]\n",
      "723 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.511221]\n",
      "724 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.517282]\n",
      "725 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.523239]\n",
      "726 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.529190]\n",
      "727 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.534687]\n",
      "728 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.538016]\n",
      "729 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.532190]\n",
      "730 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.503432]\n",
      "731 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.482899]\n",
      "732 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.487336]\n",
      "733 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.492941]\n",
      "734 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.499070]\n",
      "735 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.505233]\n",
      "736 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.511393]\n",
      "737 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.517570]\n",
      "738 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.523776]\n",
      "739 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.529961]\n",
      "740 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.536119]\n",
      "741 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.542292]\n",
      "742 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.548359]\n",
      "743 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.554474]\n",
      "744 [D loss: 0.000010, acc.: 100.00%] [G loss: 12.560318]\n",
      "745 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.566223]\n",
      "746 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.572051]\n",
      "747 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.577890]\n",
      "748 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.583770]\n",
      "749 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.589617]\n",
      "750 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.595391]\n",
      "751 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.601212]\n",
      "752 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.607067]\n",
      "753 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.612923]\n",
      "754 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.618769]\n",
      "755 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.624551]\n",
      "756 [D loss: 0.000011, acc.: 100.00%] [G loss: 12.630141]\n",
      "757 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.635773]\n",
      "758 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.641392]\n",
      "759 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.646925]\n",
      "760 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.652444]\n",
      "761 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.657988]\n",
      "762 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.663511]\n",
      "763 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.669049]\n",
      "764 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.674633]\n",
      "765 [D loss: 0.000010, acc.: 100.00%] [G loss: 12.680079]\n",
      "766 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.685525]\n",
      "767 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.690968]\n",
      "768 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.696358]\n",
      "769 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.701715]\n",
      "770 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.707039]\n",
      "771 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.712339]\n",
      "772 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.717602]\n",
      "773 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.722909]\n",
      "774 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.728172]\n",
      "775 [D loss: 0.000012, acc.: 100.00%] [G loss: 12.733081]\n",
      "776 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.738079]\n",
      "777 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.743130]\n",
      "778 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.748198]\n",
      "779 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.753150]\n",
      "780 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.758062]\n",
      "781 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.762886]\n",
      "782 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.766467]\n",
      "783 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.741604]\n",
      "784 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.698796]\n",
      "785 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.697932]\n",
      "786 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.703018]\n",
      "787 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.708143]\n",
      "788 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.713314]\n",
      "789 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.718511]\n",
      "790 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.723778]\n",
      "791 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.729029]\n",
      "792 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.734310]\n",
      "793 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.739586]\n",
      "794 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.744871]\n",
      "795 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.750153]\n",
      "796 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.755445]\n",
      "797 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.760714]\n",
      "798 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.766005]\n",
      "799 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.771297]\n",
      "800 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.776463]\n",
      "801 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.781652]\n",
      "802 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.786863]\n",
      "803 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.792040]\n",
      "804 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.797238]\n",
      "805 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.802401]\n",
      "806 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.807570]\n",
      "807 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.812633]\n",
      "808 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.817689]\n",
      "809 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.822649]\n",
      "810 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.827606]\n",
      "811 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.832563]\n",
      "812 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.837534]\n",
      "813 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.842468]\n",
      "814 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.847435]\n",
      "815 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.852365]\n",
      "816 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.857241]\n",
      "817 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.862133]\n",
      "818 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.867056]\n",
      "819 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.871912]\n",
      "820 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.876692]\n",
      "821 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.881473]\n",
      "822 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.886269]\n",
      "823 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.891080]\n",
      "824 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.895739]\n",
      "825 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.900421]\n",
      "826 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.905113]\n",
      "827 [D loss: 0.000008, acc.: 100.00%] [G loss: 12.909697]\n",
      "828 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.914312]\n",
      "829 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.918946]\n",
      "830 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.923502]\n",
      "831 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.927956]\n",
      "832 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.932426]\n",
      "833 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.936923]\n",
      "834 [D loss: 0.000007, acc.: 100.00%] [G loss: 12.941326]\n",
      "835 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.945786]\n",
      "836 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.950178]\n",
      "837 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.954542]\n",
      "838 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.958944]\n",
      "839 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.963327]\n",
      "840 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.967760]\n",
      "841 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.972199]\n",
      "842 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.976518]\n",
      "843 [D loss: 0.000006, acc.: 100.00%] [G loss: 12.980736]\n",
      "844 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.985008]\n",
      "845 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.989252]\n",
      "846 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.993534]\n",
      "847 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.997818]\n",
      "848 [D loss: 0.000009, acc.: 100.00%] [G loss: 13.001993]\n",
      "849 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.006221]\n",
      "850 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.010498]\n",
      "851 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.014819]\n",
      "852 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.019114]\n",
      "853 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.023439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.027767]\n",
      "855 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.032124]\n",
      "856 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.036502]\n",
      "857 [D loss: 0.000005, acc.: 100.00%] [G loss: 13.040788]\n",
      "858 [D loss: 0.000005, acc.: 100.00%] [G loss: 13.045000]\n",
      "859 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.049166]\n",
      "860 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.053356]\n",
      "861 [D loss: 0.000006, acc.: 100.00%] [G loss: 13.057394]\n",
      "862 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.061480]\n",
      "863 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.065536]\n",
      "864 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.069626]\n",
      "865 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.073702]\n",
      "866 [D loss: 0.000005, acc.: 100.00%] [G loss: 13.077714]\n",
      "867 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.081749]\n",
      "868 [D loss: 0.000006, acc.: 100.00%] [G loss: 13.085670]\n",
      "869 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.089624]\n",
      "870 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.093585]\n",
      "871 [D loss: 0.000009, acc.: 100.00%] [G loss: 13.097307]\n",
      "872 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.101120]\n",
      "873 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.104967]\n",
      "874 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.108797]\n",
      "875 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.112661]\n",
      "876 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.116592]\n",
      "877 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.120508]\n",
      "878 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.124453]\n",
      "879 [D loss: 0.000009, acc.: 100.00%] [G loss: 13.128141]\n",
      "880 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.131857]\n",
      "881 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.135664]\n",
      "882 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.139503]\n",
      "883 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.143385]\n",
      "884 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.147296]\n",
      "885 [D loss: 0.000005, acc.: 100.00%] [G loss: 13.151113]\n",
      "886 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.154960]\n",
      "887 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.158774]\n",
      "888 [D loss: 0.000007, acc.: 100.00%] [G loss: 13.162371]\n",
      "889 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.121444]\n",
      "890 [D loss: 0.000006, acc.: 100.00%] [G loss: 13.005178]\n",
      "891 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.897221]\n",
      "892 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.898983]\n",
      "893 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.903433]\n",
      "894 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.908091]\n",
      "895 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.912932]\n",
      "896 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.917852]\n",
      "897 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.922797]\n",
      "898 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.927804]\n",
      "899 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.932869]\n",
      "900 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.937971]\n",
      "901 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.943102]\n",
      "902 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.948189]\n",
      "903 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.953289]\n",
      "904 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.958407]\n",
      "905 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.963552]\n",
      "906 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.968736]\n",
      "907 [D loss: 0.000004, acc.: 100.00%] [G loss: 12.973896]\n",
      "908 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.979088]\n",
      "909 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.984247]\n",
      "910 [D loss: 0.000002, acc.: 100.00%] [G loss: 12.989418]\n",
      "911 [D loss: 0.000003, acc.: 100.00%] [G loss: 12.994581]\n",
      "912 [D loss: 0.000005, acc.: 100.00%] [G loss: 12.999663]\n",
      "913 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.004738]\n",
      "914 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.009834]\n",
      "915 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.014961]\n",
      "916 [D loss: 0.000006, acc.: 100.00%] [G loss: 13.019805]\n",
      "917 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.024620]\n",
      "918 [D loss: 0.000005, acc.: 100.00%] [G loss: 13.029299]\n",
      "919 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.034006]\n",
      "920 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.038721]\n",
      "921 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.043488]\n",
      "922 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.048265]\n",
      "923 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.053034]\n",
      "924 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.057750]\n",
      "925 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.062408]\n",
      "926 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.067065]\n",
      "927 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.071745]\n",
      "928 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.076424]\n",
      "929 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.081116]\n",
      "930 [D loss: 0.000010, acc.: 100.00%] [G loss: 13.085613]\n",
      "931 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.090103]\n",
      "932 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.094620]\n",
      "933 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.099169]\n",
      "934 [D loss: 0.000005, acc.: 100.00%] [G loss: 13.103578]\n",
      "935 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.108034]\n",
      "936 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.112467]\n",
      "937 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.116837]\n",
      "938 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.121233]\n",
      "939 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.125650]\n",
      "940 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.130106]\n",
      "941 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.134497]\n",
      "942 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.138887]\n",
      "943 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.143299]\n",
      "944 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.147680]\n",
      "945 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.152031]\n",
      "946 [D loss: 0.000006, acc.: 100.00%] [G loss: 13.156263]\n",
      "947 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.160461]\n",
      "948 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.164703]\n",
      "949 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.168953]\n",
      "950 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.173204]\n",
      "951 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.177507]\n",
      "952 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.181786]\n",
      "953 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.186081]\n",
      "954 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.190401]\n",
      "955 [D loss: 0.000005, acc.: 100.00%] [G loss: 13.194639]\n",
      "956 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.198879]\n",
      "957 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.203126]\n",
      "958 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.207360]\n",
      "959 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.211588]\n",
      "960 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.215837]\n",
      "961 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.220097]\n",
      "962 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.224352]\n",
      "963 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.228518]\n",
      "964 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.232691]\n",
      "965 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.236892]\n",
      "966 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.241098]\n",
      "967 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.245308]\n",
      "968 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.249441]\n",
      "969 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.253580]\n",
      "970 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.257724]\n",
      "971 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.261803]\n",
      "972 [D loss: 0.000006, acc.: 100.00%] [G loss: 13.265745]\n",
      "973 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.269669]\n",
      "974 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.273619]\n",
      "975 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.277590]\n",
      "976 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.281580]\n",
      "977 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.285601]\n",
      "978 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.289543]\n",
      "979 [D loss: 0.000001, acc.: 100.00%] [G loss: 13.293538]\n",
      "980 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.297525]\n",
      "981 [D loss: 0.000006, acc.: 100.00%] [G loss: 13.301348]\n",
      "982 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.305200]\n",
      "983 [D loss: 0.000005, acc.: 100.00%] [G loss: 13.308983]\n",
      "984 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.312782]\n",
      "985 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.316562]\n",
      "986 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.320329]\n",
      "987 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.324097]\n",
      "988 [D loss: 0.000005, acc.: 100.00%] [G loss: 13.327834]\n",
      "989 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.331590]\n",
      "990 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.335352]\n",
      "991 [D loss: 0.000001, acc.: 100.00%] [G loss: 13.339167]\n",
      "992 [D loss: 0.000001, acc.: 100.00%] [G loss: 13.343027]\n",
      "993 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.346901]\n",
      "994 [D loss: 0.000001, acc.: 100.00%] [G loss: 13.350806]\n",
      "995 [D loss: 0.000003, acc.: 100.00%] [G loss: 13.354643]\n",
      "996 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.358480]\n",
      "997 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.362309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998 [D loss: 0.000001, acc.: 100.00%] [G loss: 13.366163]\n",
      "999 [D loss: 0.000002, acc.: 100.00%] [G loss: 13.369995]\n",
      "1000 [D loss: 0.000004, acc.: 100.00%] [G loss: 13.373755]\n"
     ]
    }
   ],
   "source": [
    "z_dim = 100\n",
    "\n",
    "discriminator, generator, gan = build_compile(data_word, z_dim)\n",
    "\n",
    "iterations =1000\n",
    "batch_size = 128\n",
    "sample_interval = 1000\n",
    "\n",
    "generator = train(data_word,labels_word,iterations,batch_size,sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
