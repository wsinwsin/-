{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# similarity_unseen_docsの内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_unseen_docs(self, doc_words1, doc_words2, alpha=None, min_alpha=None, steps=None):\n",
    "        \"\"\"Compute cosine similarity between two post-bulk out of training documents.\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
    "            An instance of a trained `Doc2Vec` model.\n",
    "        doc_words1 : list of str\n",
    "            Input document.\n",
    "        doc_words2 : list of str\n",
    "            Input document.\n",
    "        alpha : float, optional\n",
    "            The initial learning rate.\n",
    "        min_alpha : float, optional\n",
    "            Learning rate will linearly drop to `min_alpha` as training progresses.\n",
    "        steps : int, optional\n",
    "            Number of epoch to train the new document.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The cosine similarity between `doc_words1` and `doc_words2`.\n",
    "        \"\"\"\n",
    "        d1 = self.infer_vector(doc_words=doc_words1, alpha=alpha, min_alpha=min_alpha, steps=steps)\n",
    "        d2 = self.infer_vector(doc_words=doc_words2, alpha=alpha, min_alpha=min_alpha, steps=steps)\n",
    "        return np.dot(matutils.unitvec(d1), matutils.unitvec(d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unitvec(vec, norm='l2', return_norm=False):\n",
    "    \"\"\"Scale a vector to unit length.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : {numpy.ndarray, scipy.sparse, list of (int, float)}\n",
    "        Input vector in any format\n",
    "    norm : {'l1', 'l2', 'unique'}, optional\n",
    "        Metric to normalize in.\n",
    "    return_norm : bool, optional\n",
    "        Return the length of vector `vec`, in addition to the normalized vector itself?\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray, scipy.sparse, list of (int, float)}\n",
    "        Normalized vector in same format as `vec`.\n",
    "    float\n",
    "        Length of `vec` before normalization, if `return_norm` is set.\n",
    "    Notes\n",
    "    -----\n",
    "    Zero-vector will be unchanged.\n",
    "    \"\"\"\n",
    "    supported_norms = ('l1', 'l2', 'unique')\n",
    "    if norm not in supported_norms:\n",
    "        raise ValueError(\"'%s' is not a supported norm. Currently supported norms are %s.\" % (norm, supported_norms))\n",
    "\n",
    "    if scipy.sparse.issparse(vec): #vecが疎行列だったらTrue  #false\n",
    "        vec = vec.tocsr()  #vecをcsr_matrixの形式に変換\n",
    "        if norm == 'l1':\n",
    "            veclen = np.sum(np.abs(vec.data))\n",
    "        if norm == 'l2':\n",
    "            veclen = np.sqrt(np.sum(vec.data ** 2))\n",
    "        if norm == 'unique':\n",
    "            veclen = vec.nnz\n",
    "        if veclen > 0.0:\n",
    "            if np.issubdtype(vec.dtype, np.integer):\n",
    "                vec = vec.astype(np.float)\n",
    "            vec /= veclen\n",
    "            if return_norm:\n",
    "                return vec, veclen\n",
    "            else:\n",
    "                return vec\n",
    "        else:\n",
    "            if return_norm:\n",
    "                return vec, 1.0\n",
    "            else:\n",
    "                return vec\n",
    "\n",
    "    if isinstance(vec, np.ndarray): #vecがnd.ndarrayの型だったらTrue #true\n",
    "        if norm == 'l1':\n",
    "            veclen = np.sum(np.abs(vec)) #vecの絶対値の合計をveclenに代入\n",
    "        if norm == 'l2': #ture\n",
    "            if vec.size == 0:\n",
    "                veclen = 0.0\n",
    "            else:\n",
    "                veclen = blas_nrm2(vec)\n",
    "                print('veclen:',veclen)\n",
    "        if norm == 'unique':\n",
    "            veclen = np.count_nonzero(vec)\n",
    "        if veclen > 0.0:\n",
    "            if np.issubdtype(vec.dtype, np.integer):  #false\n",
    "                vec = vec.astype(np.float)\n",
    "            if return_norm: #false\n",
    "                return blas_scal(1.0 / veclen, vec).astype(vec.dtype), veclen\n",
    "            else:\n",
    "                return blas_scal(1.0 / veclen, vec).astype(vec.dtype)\n",
    "        else:\n",
    "            if return_norm:\n",
    "                return vec, 1.0\n",
    "            else:\n",
    "                return vec\n",
    "\n",
    "    try:\n",
    "        first = next(iter(vec))  # is there at least one element?\n",
    "    except StopIteration:\n",
    "        if return_norm:\n",
    "            return vec, 1.0\n",
    "        else:\n",
    "            return vec\n",
    "\n",
    "    if isinstance(first, (tuple, list)) and len(first) == 2:  # gensim sparse format\n",
    "        if norm == 'l1':\n",
    "            length = float(sum(abs(val) for _, val in vec))\n",
    "        if norm == 'l2':\n",
    "            length = 1.0 * math.sqrt(sum(val ** 2 for _, val in vec))\n",
    "        if norm == 'unique':\n",
    "            length = 1.0 * len(vec)\n",
    "        assert length > 0.0, \"sparse documents must not contain any explicit zero entries\"\n",
    "        if return_norm:\n",
    "            return ret_normalized_vec(vec, length), length\n",
    "        else:\n",
    "            return ret_normalized_vec(vec, length)\n",
    "    else:\n",
    "        raise ValueError(\"unknown input type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blas(name, ndarray):\n",
    "    \"\"\"Helper for getting the appropriate BLAS function, using :func:`scipy.linalg.get_blas_funcs`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Name(s) of BLAS functions, without the type prefix.\n",
    "    ndarray : numpy.ndarray\n",
    "        Arrays can be given to determine optimal prefix of BLAS routines.\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        BLAS function for the needed operation on the given data type.\n",
    "    \"\"\"\n",
    "    return scipy.linalg.get_blas_funcs((name,), (ndarray,))[0] #名前から使用可能なBLAS関数オブジェクトを返します。\n",
    "blas_nrm2 = blas('nrm2', np.array([], dtype=float))\n",
    "blas_scal = blas('scal', np.array([], dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "疎行列（そぎょうれつ、英: sparse matrix）とは、成分のほとんどが零である行列のことをいう。スパース行列とも言う。 有限差分法、有限体積法、有限要素法などで離散化された偏微分方程式は一般に疎行列を係数行列とした連立一次方程式となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "model_doc = Doc2Vec.load(\"jawiki.doc2vec.dbow300d.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model_doc.infer_vector(['車'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veclen: 0.7907881717694959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.08972666, -0.0092322 , -0.04256337,  0.05482487,  0.10632727,\n",
       "        0.02062949, -0.0560747 ,  0.06268494, -0.0562828 ,  0.02607832,\n",
       "       -0.03893636, -0.04824613,  0.01007342, -0.00947893,  0.0035056 ,\n",
       "        0.02899972, -0.02408149,  0.03620663, -0.00597921,  0.01082833,\n",
       "        0.05455308,  0.01392252,  0.04128278,  0.03102316, -0.07902006,\n",
       "        0.02307848, -0.04648721, -0.11421375, -0.05605326,  0.0289881 ,\n",
       "       -0.14291629, -0.08959747,  0.04422378,  0.07865747, -0.06083259,\n",
       "        0.01160509, -0.08184171, -0.01392379,  0.04465479,  0.01285873,\n",
       "        0.02368837,  0.01367635, -0.09993049, -0.11114352,  0.02746045,\n",
       "        0.01767351,  0.02436406, -0.06267492, -0.05237596,  0.14076704,\n",
       "       -0.01748081, -0.01877484, -0.04558789, -0.142673  , -0.05778985,\n",
       "        0.0200805 ,  0.00302748,  0.00614667,  0.05105071,  0.02074428,\n",
       "        0.07087641,  0.02082591,  0.07308887,  0.02439552, -0.13460341,\n",
       "       -0.05065057, -0.03573213,  0.05318211,  0.03366949,  0.00249451,\n",
       "       -0.0060955 ,  0.01836406,  0.04629673,  0.01456484, -0.05228858,\n",
       "       -0.02132644, -0.0687071 , -0.01256952,  0.0827577 , -0.07274462,\n",
       "        0.03964791, -0.05697401,  0.06977038,  0.10655961, -0.00723194,\n",
       "       -0.05845588, -0.0090021 ,  0.01950937,  0.0323458 ,  0.0365798 ,\n",
       "        0.0684543 , -0.03401664, -0.13135895, -0.06649767, -0.02413141,\n",
       "       -0.00553963, -0.11619277, -0.04508212,  0.04743119, -0.00273875,\n",
       "       -0.06933369, -0.03911523,  0.05129944,  0.11558697,  0.01068555,\n",
       "       -0.00969939, -0.10134343, -0.09736136, -0.01355378, -0.05507425,\n",
       "        0.04878614,  0.17942871,  0.0479466 ,  0.06936306,  0.00209962,\n",
       "        0.0061823 , -0.01113752, -0.01013641, -0.00236185, -0.04556535,\n",
       "       -0.01360698, -0.09877957,  0.03555369, -0.06133393,  0.02588133,\n",
       "        0.00380331,  0.03637108,  0.01497461, -0.01302595, -0.00757908,\n",
       "        0.03729983, -0.00439701, -0.0251673 ,  0.01068256,  0.00656572,\n",
       "        0.03579216,  0.02418571, -0.07037644,  0.08042087, -0.06059545,\n",
       "        0.01988534, -0.04965787, -0.03248081,  0.02939424,  0.06021314,\n",
       "        0.04808516,  0.01569291, -0.0030863 ,  0.11293639,  0.01243154,\n",
       "        0.0910136 ,  0.02355534, -0.01797625, -0.05243497,  0.01004795,\n",
       "        0.04416169,  0.02479562, -0.03723859,  0.02439924,  0.06300233,\n",
       "       -0.04176599, -0.0298796 ,  0.0831568 , -0.04565256,  0.03061315,\n",
       "        0.08309343, -0.03247567,  0.06232424,  0.02440577, -0.0017838 ,\n",
       "        0.16603102, -0.03713371, -0.03589789,  0.01199933,  0.07290979,\n",
       "        0.00113833,  0.00362443, -0.02586994, -0.01103101,  0.00288493,\n",
       "        0.04226032, -0.02661184, -0.01058803, -0.00027249, -0.00825463,\n",
       "        0.03116243,  0.04641757, -0.00640185,  0.07153127, -0.09890315,\n",
       "       -0.074431  , -0.01932839,  0.09435154, -0.08938903,  0.05811155,\n",
       "        0.0328597 ,  0.04528093, -0.04598277, -0.0104209 ,  0.15171045,\n",
       "        0.00620178, -0.05537411,  0.0259045 , -0.0146111 , -0.09363618,\n",
       "       -0.04362801, -0.00107715, -0.01413449,  0.0446903 , -0.01736013,\n",
       "       -0.05986515,  0.10484503, -0.01839113, -0.1074559 ,  0.12005583,\n",
       "        0.00071026, -0.05057134, -0.00714692,  0.08115268,  0.05833627,\n",
       "        0.06383894,  0.05744456,  0.02541564,  0.04790474,  0.00662141,\n",
       "        0.04337414, -0.04795161,  0.04822432, -0.01917786,  0.08345926,\n",
       "       -0.06595404,  0.00031956, -0.01610104,  0.03816779, -0.03386281,\n",
       "        0.0908744 ,  0.03049199,  0.10885615, -0.00233923, -0.07684739,\n",
       "        0.01863109,  0.03116683, -0.00157035,  0.0767144 , -0.0240596 ,\n",
       "       -0.02374835, -0.06995974, -0.00896965,  0.01667344, -0.03052988,\n",
       "        0.00928648,  0.07411538,  0.00452134,  0.03933224,  0.055659  ,\n",
       "       -0.12684217, -0.08637406,  0.00165659,  0.08549922, -0.04672493,\n",
       "       -0.04589462,  0.07272147, -0.06215105,  0.06516939,  0.04592765,\n",
       "       -0.03312872, -0.02901184,  0.01180789,  0.0012687 , -0.00744452,\n",
       "       -0.05335481, -0.07213582,  0.03634054, -0.01504843,  0.00068316,\n",
       "       -0.03677261,  0.06879438,  0.04424821,  0.05168883, -0.0181126 ,\n",
       "       -0.11026699, -0.00997337, -0.04724916, -0.12262858, -0.08122272,\n",
       "       -0.08631854,  0.05570557,  0.02422351, -0.11595752,  0.08001738,\n",
       "        0.00047429,  0.06439953,  0.04577772,  0.04769183,  0.11155348,\n",
       "        0.1155782 ,  0.09869153,  0.01414903, -0.08287943, -0.09316536],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitvec(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.issubdtype(z.dtype, np.integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_vector(self, doc_words, alpha=None, min_alpha=None, epochs=None, steps=None):\n",
    "        \"\"\"Infer a vector for given post-bulk training document.\n",
    "        Notes\n",
    "        -----\n",
    "        Subsequent calls to this function may infer different representations for the same document.\n",
    "        For a more stable representation, increase the number of steps to assert a stricket convergence.\n",
    "        Parameters\n",
    "        ----------\n",
    "        doc_words : list of str\n",
    "            A document for which the vector representation will be inferred.\n",
    "        alpha : float, optional\n",
    "            The initial learning rate. If unspecified, value from model initialization will be reused.\n",
    "        min_alpha : float, optional\n",
    "            Learning rate will linearly drop to `min_alpha` over all inference epochs. If unspecified,\n",
    "            value from model initialization will be reused.\n",
    "        epochs : int, optional\n",
    "            Number of times to train the new document. Larger values take more time, but may improve\n",
    "            quality and run-to-run stability of inferred vectors. If unspecified, the `epochs` value\n",
    "            from model initialization will be reused.\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The inferred paragraph vector for the new document.\n",
    "        \"\"\"\n",
    "        if isinstance(doc_words, str):  # a common mistake; fail with a nicer error\n",
    "            raise TypeError(\"Parameter doc_words of infer_vector() must be a list of strings (not a single string).\")\n",
    "\n",
    "        alpha = alpha or self.alpha\n",
    "        min_alpha = min_alpha or self.min_alpha\n",
    "        epochs = epochs or self.epochs\n",
    "\n",
    "        doctag_vectors = pseudorandom_weak_vector(self.dv.vector_size, seed_string=' '.join(doc_words))\n",
    "        doctag_vectors = doctag_vectors.reshape(1, self.dv.vector_size)\n",
    "\n",
    "        doctags_lockf = np.ones(1, dtype=REAL)\n",
    "        doctag_indexes = [0]\n",
    "        work = zeros(self.layer1_size, dtype=REAL)   #layer1_size = vector_size\n",
    "        if not self.sg:    #sg= (1 + dm) % 2\n",
    "                                #dm : {1,0}, optional\n",
    "                                #Defines the training algorithm. If `dm=1`, 'distributed memory' (PV-DM) is used.\n",
    "                                #Otherwise, `distributed bag of words` (PV-DBOW) is employed.\n",
    "            neu1 = matutils.zeros_aligned(self.layer1_size, dtype=REAL)\n",
    "\n",
    "        alpha_delta = (alpha - min_alpha) / max(epochs - 1, 1)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            if self.sg:\n",
    "                train_document_dbow(\n",
    "                    self, doc_words, doctag_indexes, alpha, work,\n",
    "                    learn_words=False, learn_hidden=False, doctag_vectors=doctag_vectors, doctags_lockf=doctags_lockf\n",
    "                )\n",
    "            elif self.dm_concat:\n",
    "                train_document_dm_concat(\n",
    "                    self, doc_words, doctag_indexes, alpha, work, neu1,\n",
    "                    learn_words=False, learn_hidden=False, doctag_vectors=doctag_vectors, doctags_lockf=doctags_lockf\n",
    "                )\n",
    "            else:\n",
    "                train_document_dm(\n",
    "                    self, doc_words, doctag_indexes, alpha, work, neu1,\n",
    "                    learn_words=False, learn_hidden=False, doctag_vectors=doctag_vectors, doctags_lockf=doctags_lockf\n",
    "                )\n",
    "            alpha -= alpha_delta\n",
    "\n",
    "        return doctag_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudorandom_weak_vector(size, seed_string=None, hashfxn=hash):\n",
    "    \"\"\"Get a random vector, derived deterministically from `seed_string` if supplied.\n",
    "    Useful for initializing KeyedVectors that will be the starting projection/input layers of _2Vec models.\n",
    "    \"\"\"\n",
    "    if seed_string:\n",
    "        once = np.random.Generator(np.random.SFC64(hashfxn(seed_string) & 0xffffffff))\n",
    "    else:\n",
    "        once = utils.default_prng\n",
    "    return (once.random(size).astype(REAL) - 0.5) / size  #REAL = np.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_aligned(shape, dtype, order='C', align=128):\n",
    "    \"\"\"Get array aligned at `align` byte boundary in memory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape : int or (int, int)\n",
    "        Shape of array.\n",
    "    dtype : data-type\n",
    "        Data type of array.\n",
    "    order : {'C', 'F'}, optional\n",
    "        Whether to store multidimensional data in C- or Fortran-contiguous (row- or column-wise) order in memory.\n",
    "    align : int, optional\n",
    "        Boundary for alignment in bytes.\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Aligned array.\n",
    "    \"\"\"\n",
    "    nbytes = np.prod(shape, dtype=np.int64) * np.dtype(dtype).itemsize #np.prod 全要素を対象に要素の積を算出\n",
    "    buffer = np.zeros(nbytes + align, dtype=np.uint8)  # problematic on win64 (\"maximum allowed dimension exceeded\")\n",
    "    start_index = -buffer.ctypes.data % align\n",
    "    return buffer[start_index: start_index + nbytes].view(dtype).reshape(shape, order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
