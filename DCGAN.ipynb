{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten, Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv1D, Conv2D,Conv2DTranspose\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers.convolutional import Conv1DTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import MeCab\n",
    "import json\n",
    "import hashlib\n",
    "from googletrans import Translator\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import torch\n",
    "from transformers.modeling_bert import BertModel\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "mt = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "mt.parse('')\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "\n",
    "model_doc = Doc2Vec.load(\"jawiki.doc2vec.dbow300d.model\")\n",
    "model_word = word2vec.Word2Vec.load(\"wiki_plus.model\")\n",
    "model_bert = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(text):\n",
    "    word = {}\n",
    "    node = mt.parseToNode(text)\n",
    "    while node:\n",
    "        fields = node.feature.split(\",\")\n",
    "        if (fields[0] == '名詞' or fields[0] == '動詞' or fields[0] == '形容詞') and node.surface in model_word.wv:\n",
    "            w = node.surface\n",
    "            word[w] = word.get(w, 0) + 1\n",
    "        node = node.next\n",
    "    return word\n",
    "\n",
    "def weighted_mean_vec(text):\n",
    "    v = np.zeros(model_word.vector_size)\n",
    "    s = 1.0\n",
    "    for w,weight in get_tags(text).items():\n",
    "        v += weight * model_word.wv[w]  #Eventクラスeの単語wの個数＊単語wのベクトル\n",
    "        s += weight\n",
    "    return v / s\n",
    "\n",
    "def get_tags_for_doc2vec(text):\n",
    "    word = []\n",
    "    node = mt.parseToNode(text)\n",
    "    while node:\n",
    "        fields = node.feature.split(\",\")\n",
    "        if node.surface in model_doc.wv and node.surface !='':\n",
    "            w = node.surface\n",
    "            word.append(w)\n",
    "        node = node.next\n",
    "    return word\n",
    "\n",
    "#bertのベクトル化\n",
    "def get_vector_cls(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt') \n",
    "    result = model_bert(input_ids)\n",
    "    tensor_result = result[0][0][0]\n",
    "    numpy_result = tensor_result.to('cpu').detach().numpy().copy()\n",
    "    return numpy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event:\n",
    "    def __init__(self, id, type, score, desc, links):\n",
    "        self.id = id\n",
    "        self.type = type\n",
    "        self.score = score\n",
    "        self.desc = desc\n",
    "        self.links = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON ファイルから event set をロード\n",
    "def load_events(jsonfile):\n",
    "    with open(jsonfile) as f:\n",
    "        df = json.load(f)\n",
    "    events = {x['id']: Event(x['id'], x['type'], x['score'], x['desc'], x['links']) for x in df} #eventsにidをkeyとしそのオブジェクトをvalueとした辞書を生成\n",
    "    for k,x in events.items():\n",
    "        x.links = [events[e] for e in x.links] #Event.linkの中身をidの配列からEventの配列に変更\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = load_events('sesaku2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "labels = []\n",
    "columns=[]\n",
    "index=[]\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='部品':\n",
    "        index.append(v1.desc)\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='対策':\n",
    "        if not v1.desc in columns:\n",
    "            columns.append(v1.desc)\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='部品':\n",
    "        for k2, v2 in events.items():\n",
    "            if v2.type[-1] == '対策':\n",
    "                    if v2 in v1.links:\n",
    "                        df.at[v1.desc, v2.desc] = 1\n",
    "                    else:\n",
    "                        df.at[v1.desc, v2.desc] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "taisaku_vec_word = {}\n",
    "for i in df:\n",
    "    taisaku_vec_word[i]=weighted_mean_vec(i)\n",
    "\n",
    "taisaku_vec_doc = {}\n",
    "for i in df:\n",
    "    taisaku_vec_doc[i]=model_doc.infer_vector(get_tags_for_doc2vec(i))\n",
    "\n",
    "taisaku_vec_bert = {}\n",
    "for i in df:\n",
    "    taisaku_vec_bert[i]=get_vector_cls(i)\n",
    "\n",
    "class Label:\n",
    "    TAISAKU = 1\n",
    "    NASI = 0\n",
    "\n",
    "data_word = []\n",
    "labels_word = []\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    x1 = weighted_mean_vec(index)\n",
    "    for i in df:#対策\n",
    "        x2 =  taisaku_vec_word[i]#対策\n",
    "        data_word.extend([np.append(x1, x2)])\n",
    "        if row[i] ==1:\n",
    "            labels_word.append(Label.TAISAKU)\n",
    "        else:\n",
    "            labels_word.append(Label.NASI)\n",
    "\n",
    "data_doc = []\n",
    "labels_doc = []\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    x1 = model_doc.infer_vector(get_tags_for_doc2vec(index))\n",
    "    for i in df:#対策\n",
    "        x2 =  taisaku_vec_doc[i]#対策\n",
    "        data_doc.extend([np.append(x1, x2)])\n",
    "        if row[i] ==1:\n",
    "            labels_doc.append(Label.TAISAKU)\n",
    "        else:\n",
    "            labels_doc.append(Label.NASI)\n",
    "\n",
    "data_BERT_cls = []\n",
    "labels_BERT_cls = []\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    x1 = get_vector_cls(index) #部品\n",
    "    for i in df:#対策\n",
    "        x2=taisaku_vec_bert[i]\n",
    "        data_BERT_cls.extend([np.append(x1, x2)])\n",
    "        if row[i] ==1:\n",
    "            labels_BERT_cls.append(Label.TAISAKU)\n",
    "        else:\n",
    "            labels_BERT_cls.append(Label.NASI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " #生成器\n",
    "def build_generator(data_size, z_dim):\n",
    "        model = Sequential()\n",
    "        \n",
    "        #全結合層によって、100×16のテンソルに変換  テンソル＝配列\n",
    "        model.add(Dense(100*32, input_dim = z_dim))\n",
    "        model.add(Reshape((100,32)))\n",
    "        \n",
    "        #転置畳み込み層により,100×16を200×16のテンソルに変換\n",
    "        model.add(Conv1DTranspose(16, kernel_size=3, strides=2, padding='same'))\n",
    "        \n",
    "        #バッチ正規化\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "        \n",
    "        #転置畳み込み層により,200×16を200×8のテンソルに変換\n",
    "        model.add(Conv1DTranspose(8, kernel_size=3, strides=1, padding='same'))\n",
    "        \n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "        \n",
    "        #転置畳み込み層により,200×64を400×1をのテンソルに変換\n",
    "        model.add(Conv1DTranspose(1, kernel_size=3, strides=2, padding='same'))\n",
    "        \n",
    "        model.add(Activation('tanh'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 3200)              323200    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose (Conv1DTran (None, 200, 16)           1552      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 200, 16)           64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 200, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_1 (Conv1DTr (None, 200, 8)            392       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200, 8)            32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 200, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_2 (Conv1DTr (None, 400, 1)            25        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 400, 1)            0         \n",
      "=================================================================\n",
      "Total params: 325,265\n",
      "Trainable params: 325,217\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_generator(200,100)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#識別器\n",
    "\n",
    "def build_discriminator(data_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    #400×1を200×32のテンソルにする畳み込み層\n",
    "\n",
    "    model.add(Conv1D(32, kernel_size=3, strides=2,input_shape=(data_size,1), padding='same'))\n",
    "    \n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    #200×32を100×64のテンソルにする畳み込み層\n",
    "    model.add(Conv1D(64, kernel_size=3, strides=2, input_shape=(data_size,1), padding='same'))\n",
    "    \n",
    "    #バッチ正規化\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    #7×7×64を3×3×128のテンソルにする畳み込み層\n",
    "    #model.add(Conv2D(128, kernel_size=3, strides=2, imput_shape=(data_size,), padding='same'))\n",
    "    model.add(Conv1D(128, kernel_size=3, strides=2, input_shape=(data_size,1), padding='same'))\n",
    "    \n",
    "    #バッチ正規化\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "        \n",
    "    #出力にシグモイド関数を適用\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #生成器と識別器のモデルを組み合わせる\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_compile(data, z_dim):\n",
    "    data_size = len(data[0])\n",
    "    #識別器の構築とコンパイル\n",
    "    discriminator = build_discriminator(data_size)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    #生成器の構築\n",
    "    generator = build_generator(data_size, z_dim)\n",
    "\n",
    "    #生成器の構築中は識別器のパラメータを固定\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    #生成器の訓練のため、識別器は固定し、GANモデルの構築とコンパイルを行う\n",
    "    gan = build_gan(generator, discriminator)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "    \n",
    "    return discriminator, generator, gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_y_train(data,labels):\n",
    "    index_1 = [i for i, x in enumerate(labels) if x == 1]\n",
    "    index_0 = [i for i, x in enumerate(labels) if x == 0]\n",
    "    index_0 = random.sample(index_0, len(index_1))\n",
    "    data_1 = [data[i] for i in index_1]\n",
    "    data_0 = [data[i] for i in index_0]\n",
    "\n",
    "    labels_1 = [Label.TAISAKU]*len(data_1)\n",
    "    labels_0 = [Label.NASI]*len(data_0) \n",
    "    data_0 = np.array(data_0)\n",
    "    labels_0 = np.array(labels_0)\n",
    "    data_1 = np.array(data_1)\n",
    "    labels_1 = np.array(labels_1)\n",
    "    return data_0, labels_0, data_1, labels_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "iteration_checkpoints = []\n",
    "\n",
    "\n",
    "def train(data, labels, iterations, batch_size, sample_interval):\n",
    "    \n",
    "    data_0, labels_0, data_1, lables_1 = x_y_train(data,labels)\n",
    "    #1の数7303\n",
    "    #0の数7303\n",
    "\n",
    "    data_0 = np.reshape(data_0, (-1, data_0.shape[1], 1))\n",
    "    data_1 = np.reshape(data_1, (-1, data_1.shape[1], 1))\n",
    "    \n",
    "    #ラベル1\n",
    "    real = np.ones((batch_size,1))\n",
    "    #ラベル0\n",
    "    fake = np.zeros((batch_size,1))\n",
    "    \n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        #-------------------\n",
    "        #識別器の訓練\n",
    "        #-------------------\n",
    "        \n",
    "        #ランダムに関係があるベクトルをとる\n",
    "        idx = np.random.randint(0,len(data_1),batch_size)\n",
    "        vecs = data_1[idx]\n",
    "        \n",
    "        \n",
    "        z = np.random.normal(0, 1,(batch_size, 100))\n",
    "        \n",
    "        gen_vec = generator.predict(z)\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(vecs, real)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_vec, fake)\n",
    "        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        #-------------------\n",
    "        #生成器の訓練\n",
    "        #-------------------\n",
    "\n",
    "        z = np.random.normal(0, 1,(batch_size, 100))\n",
    "            \n",
    "        gen_vec = generator.predict(z)\n",
    "        \n",
    "        g_loss = gan.train_on_batch(z , fake)  #real→fake\n",
    "        \n",
    "        if(iteration +1) % sample_interval ==0:\n",
    "            losses.append((d_loss, g_loss))\n",
    "            accuracies.append(100.0 * accuracy)\n",
    "            iteration_checkpoints.append(iteration +1)\n",
    "        \n",
    "        print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %(iteration +1,d_loss, 100.0*accuracy,g_loss))\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.798666, acc.: 46.90%] [G loss: 0.704229]\n",
      "2 [D loss: 0.751533, acc.: 51.23%] [G loss: 0.701942]\n",
      "3 [D loss: 0.729438, acc.: 51.64%] [G loss: 0.697114]\n",
      "4 [D loss: 0.707229, acc.: 54.36%] [G loss: 0.694444]\n",
      "5 [D loss: 0.690716, acc.: 54.93%] [G loss: 0.693570]\n",
      "6 [D loss: 0.670387, acc.: 57.04%] [G loss: 0.693211]\n",
      "7 [D loss: 0.653779, acc.: 59.80%] [G loss: 0.692172]\n",
      "8 [D loss: 0.637908, acc.: 61.73%] [G loss: 0.690932]\n",
      "9 [D loss: 0.625756, acc.: 63.94%] [G loss: 0.689723]\n",
      "10 [D loss: 0.609733, acc.: 67.12%] [G loss: 0.688601]\n",
      "11 [D loss: 0.596043, acc.: 70.75%] [G loss: 0.687069]\n",
      "12 [D loss: 0.583942, acc.: 73.30%] [G loss: 0.684661]\n",
      "13 [D loss: 0.567541, acc.: 76.98%] [G loss: 0.681941]\n",
      "14 [D loss: 0.556607, acc.: 79.55%] [G loss: 0.679264]\n",
      "15 [D loss: 0.542930, acc.: 82.63%] [G loss: 0.676641]\n",
      "16 [D loss: 0.526616, acc.: 86.04%] [G loss: 0.674018]\n",
      "17 [D loss: 0.516362, acc.: 87.45%] [G loss: 0.670906]\n",
      "18 [D loss: 0.500471, acc.: 89.79%] [G loss: 0.667483]\n",
      "19 [D loss: 0.481801, acc.: 92.15%] [G loss: 0.664237]\n",
      "20 [D loss: 0.468733, acc.: 93.20%] [G loss: 0.661058]\n",
      "21 [D loss: 0.451837, acc.: 94.61%] [G loss: 0.657656]\n",
      "22 [D loss: 0.434670, acc.: 95.87%] [G loss: 0.654242]\n",
      "23 [D loss: 0.418488, acc.: 96.76%] [G loss: 0.650874]\n",
      "24 [D loss: 0.398401, acc.: 97.73%] [G loss: 0.647916]\n",
      "25 [D loss: 0.383062, acc.: 98.31%] [G loss: 0.644720]\n",
      "26 [D loss: 0.367289, acc.: 98.43%] [G loss: 0.641851]\n",
      "27 [D loss: 0.352056, acc.: 98.95%] [G loss: 0.638785]\n",
      "28 [D loss: 0.331409, acc.: 99.27%] [G loss: 0.635782]\n",
      "29 [D loss: 0.315340, acc.: 99.43%] [G loss: 0.633566]\n",
      "30 [D loss: 0.304881, acc.: 99.42%] [G loss: 0.631442]\n",
      "31 [D loss: 0.292265, acc.: 99.60%] [G loss: 0.629157]\n",
      "32 [D loss: 0.283236, acc.: 99.63%] [G loss: 0.627396]\n",
      "33 [D loss: 0.266745, acc.: 99.75%] [G loss: 0.625189]\n",
      "34 [D loss: 0.261513, acc.: 99.64%] [G loss: 0.623524]\n",
      "35 [D loss: 0.253514, acc.: 99.80%] [G loss: 0.621507]\n",
      "36 [D loss: 0.244288, acc.: 99.95%] [G loss: 0.619701]\n",
      "37 [D loss: 0.233635, acc.: 99.96%] [G loss: 0.618013]\n",
      "38 [D loss: 0.227929, acc.: 99.94%] [G loss: 0.616598]\n",
      "39 [D loss: 0.217082, acc.: 99.96%] [G loss: 0.615113]\n",
      "40 [D loss: 0.212322, acc.: 99.96%] [G loss: 0.614241]\n",
      "41 [D loss: 0.205355, acc.: 99.91%] [G loss: 0.612981]\n",
      "42 [D loss: 0.197355, acc.: 99.98%] [G loss: 0.612081]\n",
      "43 [D loss: 0.193962, acc.: 99.95%] [G loss: 0.610708]\n",
      "44 [D loss: 0.185474, acc.: 99.98%] [G loss: 0.610016]\n",
      "45 [D loss: 0.181002, acc.: 99.99%] [G loss: 0.609463]\n",
      "46 [D loss: 0.176248, acc.: 99.99%] [G loss: 0.608772]\n",
      "47 [D loss: 0.173628, acc.: 99.98%] [G loss: 0.607919]\n",
      "48 [D loss: 0.167132, acc.: 99.99%] [G loss: 0.607412]\n",
      "49 [D loss: 0.162396, acc.: 100.00%] [G loss: 0.607136]\n",
      "50 [D loss: 0.159304, acc.: 100.00%] [G loss: 0.607506]\n",
      "51 [D loss: 0.156381, acc.: 100.00%] [G loss: 0.607721]\n",
      "52 [D loss: 0.152585, acc.: 99.98%] [G loss: 0.607703]\n",
      "53 [D loss: 0.149104, acc.: 100.00%] [G loss: 0.608203]\n",
      "54 [D loss: 0.144922, acc.: 100.00%] [G loss: 0.608831]\n",
      "55 [D loss: 0.141839, acc.: 100.00%] [G loss: 0.609135]\n",
      "56 [D loss: 0.137110, acc.: 99.99%] [G loss: 0.609689]\n",
      "57 [D loss: 0.135365, acc.: 100.00%] [G loss: 0.610567]\n",
      "58 [D loss: 0.133191, acc.: 100.00%] [G loss: 0.610898]\n",
      "59 [D loss: 0.130122, acc.: 99.99%] [G loss: 0.611516]\n",
      "60 [D loss: 0.128551, acc.: 99.99%] [G loss: 0.612580]\n",
      "61 [D loss: 0.125399, acc.: 100.00%] [G loss: 0.613547]\n",
      "62 [D loss: 0.124275, acc.: 99.98%] [G loss: 0.614539]\n",
      "63 [D loss: 0.123338, acc.: 100.00%] [G loss: 0.615571]\n",
      "64 [D loss: 0.121267, acc.: 100.00%] [G loss: 0.616266]\n",
      "65 [D loss: 0.116867, acc.: 99.99%] [G loss: 0.617525]\n",
      "66 [D loss: 0.117543, acc.: 99.99%] [G loss: 0.619043]\n",
      "67 [D loss: 0.117122, acc.: 99.99%] [G loss: 0.620432]\n",
      "68 [D loss: 0.113903, acc.: 100.00%] [G loss: 0.622118]\n",
      "69 [D loss: 0.112321, acc.: 100.00%] [G loss: 0.624085]\n",
      "70 [D loss: 0.112360, acc.: 100.00%] [G loss: 0.626192]\n",
      "71 [D loss: 0.108559, acc.: 99.99%] [G loss: 0.628687]\n",
      "72 [D loss: 0.108303, acc.: 99.98%] [G loss: 0.630744]\n",
      "73 [D loss: 0.107078, acc.: 99.99%] [G loss: 0.633023]\n",
      "74 [D loss: 0.106632, acc.: 99.99%] [G loss: 0.635709]\n",
      "75 [D loss: 0.104425, acc.: 100.00%] [G loss: 0.638448]\n",
      "76 [D loss: 0.104947, acc.: 100.00%] [G loss: 0.641050]\n",
      "77 [D loss: 0.103911, acc.: 100.00%] [G loss: 0.643433]\n",
      "78 [D loss: 0.102062, acc.: 99.99%] [G loss: 0.645932]\n",
      "79 [D loss: 0.098846, acc.: 100.00%] [G loss: 0.648281]\n",
      "80 [D loss: 0.099645, acc.: 100.00%] [G loss: 0.650490]\n",
      "81 [D loss: 0.100473, acc.: 99.99%] [G loss: 0.652228]\n",
      "82 [D loss: 0.099327, acc.: 100.00%] [G loss: 0.654061]\n",
      "83 [D loss: 0.096409, acc.: 99.99%] [G loss: 0.655653]\n",
      "84 [D loss: 0.095532, acc.: 100.00%] [G loss: 0.656841]\n",
      "85 [D loss: 0.095011, acc.: 99.98%] [G loss: 0.658136]\n",
      "86 [D loss: 0.096199, acc.: 99.98%] [G loss: 0.659036]\n",
      "87 [D loss: 0.094595, acc.: 99.98%] [G loss: 0.660544]\n",
      "88 [D loss: 0.092665, acc.: 100.00%] [G loss: 0.662182]\n",
      "89 [D loss: 0.093390, acc.: 99.98%] [G loss: 0.662736]\n",
      "90 [D loss: 0.093494, acc.: 99.98%] [G loss: 0.662539]\n",
      "91 [D loss: 0.093337, acc.: 99.99%] [G loss: 0.662176]\n",
      "92 [D loss: 0.094030, acc.: 99.97%] [G loss: 0.662681]\n",
      "93 [D loss: 0.094313, acc.: 99.95%] [G loss: 0.662351]\n",
      "94 [D loss: 0.095466, acc.: 99.98%] [G loss: 0.661602]\n",
      "95 [D loss: 0.095650, acc.: 99.95%] [G loss: 0.660758]\n",
      "96 [D loss: 0.096933, acc.: 99.98%] [G loss: 0.659432]\n",
      "97 [D loss: 0.101143, acc.: 99.98%] [G loss: 0.657396]\n",
      "98 [D loss: 0.099903, acc.: 99.98%] [G loss: 0.656263]\n",
      "99 [D loss: 0.101091, acc.: 99.97%] [G loss: 0.655898]\n",
      "100 [D loss: 0.106965, acc.: 99.95%] [G loss: 0.654259]\n",
      "101 [D loss: 0.107743, acc.: 99.97%] [G loss: 0.653069]\n",
      "102 [D loss: 0.111034, acc.: 99.93%] [G loss: 0.652344]\n",
      "103 [D loss: 0.110853, acc.: 99.98%] [G loss: 0.650771]\n",
      "104 [D loss: 0.115737, acc.: 99.93%] [G loss: 0.649843]\n",
      "105 [D loss: 0.115840, acc.: 99.98%] [G loss: 0.651682]\n",
      "106 [D loss: 0.117150, acc.: 99.95%] [G loss: 0.652989]\n",
      "107 [D loss: 0.112778, acc.: 99.98%] [G loss: 0.653664]\n",
      "108 [D loss: 0.111412, acc.: 100.00%] [G loss: 0.656186]\n",
      "109 [D loss: 0.110558, acc.: 99.99%] [G loss: 0.658606]\n",
      "110 [D loss: 0.107453, acc.: 100.00%] [G loss: 0.662961]\n",
      "111 [D loss: 0.106106, acc.: 99.97%] [G loss: 0.668242]\n",
      "112 [D loss: 0.101661, acc.: 99.98%] [G loss: 0.676160]\n",
      "113 [D loss: 0.101317, acc.: 100.00%] [G loss: 0.684799]\n",
      "114 [D loss: 0.101817, acc.: 100.00%] [G loss: 0.693766]\n",
      "115 [D loss: 0.103282, acc.: 99.98%] [G loss: 0.699341]\n",
      "116 [D loss: 0.103391, acc.: 100.00%] [G loss: 0.704764]\n",
      "117 [D loss: 0.099059, acc.: 99.98%] [G loss: 0.712743]\n",
      "118 [D loss: 0.099769, acc.: 99.99%] [G loss: 0.720869]\n",
      "119 [D loss: 0.096539, acc.: 99.99%] [G loss: 0.726538]\n",
      "120 [D loss: 0.094181, acc.: 100.00%] [G loss: 0.731217]\n",
      "121 [D loss: 0.093953, acc.: 99.98%] [G loss: 0.734406]\n",
      "122 [D loss: 0.091664, acc.: 99.98%] [G loss: 0.735842]\n",
      "123 [D loss: 0.088441, acc.: 99.99%] [G loss: 0.738725]\n",
      "124 [D loss: 0.086117, acc.: 99.99%] [G loss: 0.742102]\n",
      "125 [D loss: 0.083179, acc.: 100.00%] [G loss: 0.746501]\n",
      "126 [D loss: 0.081946, acc.: 100.00%] [G loss: 0.749329]\n",
      "127 [D loss: 0.080306, acc.: 100.00%] [G loss: 0.752104]\n",
      "128 [D loss: 0.079140, acc.: 100.00%] [G loss: 0.755020]\n",
      "129 [D loss: 0.076001, acc.: 100.00%] [G loss: 0.757419]\n",
      "130 [D loss: 0.075170, acc.: 100.00%] [G loss: 0.759774]\n",
      "131 [D loss: 0.073221, acc.: 99.99%] [G loss: 0.762513]\n",
      "132 [D loss: 0.071788, acc.: 100.00%] [G loss: 0.765186]\n",
      "133 [D loss: 0.068859, acc.: 100.00%] [G loss: 0.768831]\n",
      "134 [D loss: 0.068686, acc.: 100.00%] [G loss: 0.772433]\n",
      "135 [D loss: 0.068309, acc.: 100.00%] [G loss: 0.777609]\n",
      "136 [D loss: 0.066668, acc.: 100.00%] [G loss: 0.784790]\n",
      "137 [D loss: 0.066220, acc.: 100.00%] [G loss: 0.794130]\n",
      "138 [D loss: 0.065877, acc.: 100.00%] [G loss: 0.802497]\n",
      "139 [D loss: 0.062328, acc.: 100.00%] [G loss: 0.810156]\n",
      "140 [D loss: 0.064065, acc.: 100.00%] [G loss: 0.819932]\n",
      "141 [D loss: 0.062767, acc.: 100.00%] [G loss: 0.828553]\n",
      "142 [D loss: 0.060324, acc.: 100.00%] [G loss: 0.837620]\n",
      "143 [D loss: 0.060190, acc.: 100.00%] [G loss: 0.847874]\n",
      "144 [D loss: 0.058488, acc.: 100.00%] [G loss: 0.857184]\n",
      "145 [D loss: 0.057675, acc.: 100.00%] [G loss: 0.865263]\n",
      "146 [D loss: 0.057123, acc.: 100.00%] [G loss: 0.872440]\n",
      "147 [D loss: 0.056362, acc.: 100.00%] [G loss: 0.880284]\n",
      "148 [D loss: 0.056655, acc.: 100.00%] [G loss: 0.887089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 0.057206, acc.: 100.00%] [G loss: 0.892993]\n",
      "150 [D loss: 0.055491, acc.: 100.00%] [G loss: 0.897211]\n",
      "151 [D loss: 0.054246, acc.: 100.00%] [G loss: 0.901840]\n",
      "152 [D loss: 0.053887, acc.: 100.00%] [G loss: 0.907368]\n",
      "153 [D loss: 0.053568, acc.: 100.00%] [G loss: 0.912024]\n",
      "154 [D loss: 0.053130, acc.: 100.00%] [G loss: 0.918216]\n",
      "155 [D loss: 0.052036, acc.: 100.00%] [G loss: 0.922428]\n",
      "156 [D loss: 0.051704, acc.: 100.00%] [G loss: 0.926845]\n",
      "157 [D loss: 0.051911, acc.: 100.00%] [G loss: 0.927650]\n",
      "158 [D loss: 0.049817, acc.: 100.00%] [G loss: 0.924920]\n",
      "159 [D loss: 0.050550, acc.: 100.00%] [G loss: 0.926183]\n",
      "160 [D loss: 0.050290, acc.: 100.00%] [G loss: 0.925262]\n",
      "161 [D loss: 0.050828, acc.: 100.00%] [G loss: 0.927368]\n",
      "162 [D loss: 0.051035, acc.: 100.00%] [G loss: 0.934911]\n",
      "163 [D loss: 0.050872, acc.: 100.00%] [G loss: 0.938515]\n",
      "164 [D loss: 0.050100, acc.: 100.00%] [G loss: 0.942398]\n",
      "165 [D loss: 0.052167, acc.: 100.00%] [G loss: 0.943060]\n",
      "166 [D loss: 0.052625, acc.: 100.00%] [G loss: 0.942107]\n",
      "167 [D loss: 0.052406, acc.: 100.00%] [G loss: 0.943319]\n",
      "168 [D loss: 0.053351, acc.: 100.00%] [G loss: 0.942380]\n",
      "169 [D loss: 0.055516, acc.: 100.00%] [G loss: 0.946331]\n",
      "170 [D loss: 0.056308, acc.: 100.00%] [G loss: 0.959848]\n",
      "171 [D loss: 0.058974, acc.: 100.00%] [G loss: 0.977908]\n",
      "172 [D loss: 0.059779, acc.: 100.00%] [G loss: 0.990725]\n",
      "173 [D loss: 0.063441, acc.: 100.00%] [G loss: 1.000567]\n",
      "174 [D loss: 0.064879, acc.: 100.00%] [G loss: 1.012416]\n",
      "175 [D loss: 0.067019, acc.: 100.00%] [G loss: 1.023430]\n",
      "176 [D loss: 0.070999, acc.: 100.00%] [G loss: 1.037752]\n",
      "177 [D loss: 0.074046, acc.: 100.00%] [G loss: 1.054044]\n",
      "178 [D loss: 0.078036, acc.: 100.00%] [G loss: 1.073265]\n",
      "179 [D loss: 0.082492, acc.: 100.00%] [G loss: 1.089141]\n",
      "180 [D loss: 0.082875, acc.: 99.99%] [G loss: 1.102320]\n",
      "181 [D loss: 0.082574, acc.: 100.00%] [G loss: 1.106469]\n",
      "182 [D loss: 0.084203, acc.: 99.99%] [G loss: 1.107170]\n",
      "183 [D loss: 0.082656, acc.: 100.00%] [G loss: 1.112839]\n",
      "184 [D loss: 0.082155, acc.: 99.99%] [G loss: 1.126548]\n",
      "185 [D loss: 0.080294, acc.: 100.00%] [G loss: 1.141130]\n",
      "186 [D loss: 0.078573, acc.: 99.99%] [G loss: 1.150368]\n",
      "187 [D loss: 0.079514, acc.: 100.00%] [G loss: 1.153975]\n",
      "188 [D loss: 0.077788, acc.: 99.98%] [G loss: 1.157910]\n",
      "189 [D loss: 0.077772, acc.: 100.00%] [G loss: 1.165043]\n",
      "190 [D loss: 0.073980, acc.: 99.99%] [G loss: 1.178450]\n",
      "191 [D loss: 0.076166, acc.: 100.00%] [G loss: 1.201109]\n",
      "192 [D loss: 0.075703, acc.: 100.00%] [G loss: 1.224099]\n",
      "193 [D loss: 0.073834, acc.: 100.00%] [G loss: 1.243386]\n",
      "194 [D loss: 0.074388, acc.: 100.00%] [G loss: 1.256903]\n",
      "195 [D loss: 0.072059, acc.: 100.00%] [G loss: 1.267773]\n",
      "196 [D loss: 0.071777, acc.: 100.00%] [G loss: 1.272318]\n",
      "197 [D loss: 0.069054, acc.: 100.00%] [G loss: 1.275044]\n",
      "198 [D loss: 0.068871, acc.: 100.00%] [G loss: 1.273690]\n",
      "199 [D loss: 0.066918, acc.: 100.00%] [G loss: 1.275068]\n",
      "200 [D loss: 0.067266, acc.: 99.99%] [G loss: 1.277696]\n",
      "201 [D loss: 0.065729, acc.: 99.99%] [G loss: 1.284673]\n",
      "202 [D loss: 0.065404, acc.: 100.00%] [G loss: 1.296325]\n",
      "203 [D loss: 0.064500, acc.: 99.99%] [G loss: 1.304700]\n",
      "204 [D loss: 0.062040, acc.: 100.00%] [G loss: 1.307368]\n",
      "205 [D loss: 0.061371, acc.: 100.00%] [G loss: 1.308049]\n",
      "206 [D loss: 0.060573, acc.: 100.00%] [G loss: 1.308452]\n",
      "207 [D loss: 0.059714, acc.: 100.00%] [G loss: 1.299773]\n",
      "208 [D loss: 0.059709, acc.: 100.00%] [G loss: 1.307663]\n",
      "209 [D loss: 0.059121, acc.: 100.00%] [G loss: 1.328041]\n",
      "210 [D loss: 0.057324, acc.: 100.00%] [G loss: 1.341736]\n",
      "211 [D loss: 0.056115, acc.: 100.00%] [G loss: 1.351190]\n",
      "212 [D loss: 0.056112, acc.: 100.00%] [G loss: 1.353371]\n",
      "213 [D loss: 0.056004, acc.: 100.00%] [G loss: 1.351420]\n",
      "214 [D loss: 0.057881, acc.: 100.00%] [G loss: 1.359385]\n",
      "215 [D loss: 0.057080, acc.: 100.00%] [G loss: 1.370850]\n",
      "216 [D loss: 0.057566, acc.: 100.00%] [G loss: 1.372186]\n",
      "217 [D loss: 0.057096, acc.: 100.00%] [G loss: 1.367340]\n",
      "218 [D loss: 0.056226, acc.: 100.00%] [G loss: 1.366380]\n",
      "219 [D loss: 0.056593, acc.: 100.00%] [G loss: 1.372924]\n",
      "220 [D loss: 0.058407, acc.: 100.00%] [G loss: 1.379714]\n",
      "221 [D loss: 0.061963, acc.: 100.00%] [G loss: 1.392520]\n",
      "222 [D loss: 0.061121, acc.: 100.00%] [G loss: 1.411717]\n",
      "223 [D loss: 0.066000, acc.: 100.00%] [G loss: 1.426299]\n",
      "224 [D loss: 0.061578, acc.: 100.00%] [G loss: 1.437578]\n",
      "225 [D loss: 0.060891, acc.: 99.98%] [G loss: 1.458162]\n",
      "226 [D loss: 0.059828, acc.: 100.00%] [G loss: 1.486784]\n",
      "227 [D loss: 0.064310, acc.: 99.98%] [G loss: 1.500134]\n",
      "228 [D loss: 0.062812, acc.: 100.00%] [G loss: 1.512267]\n",
      "229 [D loss: 0.067994, acc.: 100.00%] [G loss: 1.532248]\n",
      "230 [D loss: 0.065344, acc.: 100.00%] [G loss: 1.550403]\n",
      "231 [D loss: 0.065278, acc.: 99.99%] [G loss: 1.570457]\n",
      "232 [D loss: 0.060953, acc.: 100.00%] [G loss: 1.590205]\n",
      "233 [D loss: 0.058209, acc.: 100.00%] [G loss: 1.600040]\n",
      "234 [D loss: 0.055436, acc.: 100.00%] [G loss: 1.585170]\n",
      "235 [D loss: 0.054463, acc.: 99.98%] [G loss: 1.566981]\n",
      "236 [D loss: 0.050523, acc.: 99.98%] [G loss: 1.547192]\n",
      "237 [D loss: 0.049825, acc.: 99.99%] [G loss: 1.532047]\n",
      "238 [D loss: 0.048601, acc.: 99.98%] [G loss: 1.528591]\n",
      "239 [D loss: 0.047657, acc.: 99.97%] [G loss: 1.528262]\n",
      "240 [D loss: 0.045736, acc.: 100.00%] [G loss: 1.532655]\n",
      "241 [D loss: 0.047143, acc.: 99.98%] [G loss: 1.531985]\n",
      "242 [D loss: 0.045067, acc.: 99.99%] [G loss: 1.531962]\n",
      "243 [D loss: 0.043525, acc.: 100.00%] [G loss: 1.537649]\n",
      "244 [D loss: 0.042028, acc.: 100.00%] [G loss: 1.546729]\n",
      "245 [D loss: 0.041133, acc.: 100.00%] [G loss: 1.547325]\n",
      "246 [D loss: 0.039517, acc.: 99.97%] [G loss: 1.542068]\n",
      "247 [D loss: 0.039241, acc.: 99.98%] [G loss: 1.531226]\n",
      "248 [D loss: 0.039673, acc.: 99.96%] [G loss: 1.514367]\n",
      "249 [D loss: 0.041270, acc.: 99.98%] [G loss: 1.502463]\n",
      "250 [D loss: 0.048655, acc.: 99.92%] [G loss: 1.488362]\n",
      "251 [D loss: 0.053714, acc.: 100.00%] [G loss: 1.440206]\n",
      "252 [D loss: 0.071148, acc.: 99.98%] [G loss: 1.379735]\n",
      "253 [D loss: 0.103277, acc.: 99.66%] [G loss: 1.284646]\n",
      "254 [D loss: 0.200650, acc.: 95.95%] [G loss: 1.205591]\n",
      "255 [D loss: 0.485255, acc.: 85.85%] [G loss: 1.107570]\n",
      "256 [D loss: 0.328499, acc.: 81.70%] [G loss: 1.057317]\n",
      "257 [D loss: 0.188402, acc.: 95.43%] [G loss: 1.066358]\n",
      "258 [D loss: 0.113191, acc.: 99.92%] [G loss: 1.115733]\n",
      "259 [D loss: 0.135296, acc.: 99.97%] [G loss: 1.181511]\n",
      "260 [D loss: 0.131457, acc.: 99.98%] [G loss: 1.213408]\n",
      "261 [D loss: 0.117047, acc.: 99.95%] [G loss: 1.234612]\n",
      "262 [D loss: 0.104773, acc.: 99.95%] [G loss: 1.235586]\n",
      "263 [D loss: 0.098625, acc.: 99.84%] [G loss: 1.190088]\n",
      "264 [D loss: 0.089451, acc.: 99.82%] [G loss: 1.122858]\n",
      "265 [D loss: 0.085618, acc.: 99.99%] [G loss: 1.070179]\n",
      "266 [D loss: 0.078540, acc.: 99.97%] [G loss: 1.042222]\n",
      "267 [D loss: 0.073670, acc.: 99.97%] [G loss: 1.031952]\n",
      "268 [D loss: 0.068463, acc.: 100.00%] [G loss: 1.033436]\n",
      "269 [D loss: 0.064149, acc.: 100.00%] [G loss: 1.042756]\n",
      "270 [D loss: 0.061446, acc.: 99.99%] [G loss: 1.056845]\n",
      "271 [D loss: 0.056687, acc.: 100.00%] [G loss: 1.071857]\n",
      "272 [D loss: 0.055055, acc.: 99.99%] [G loss: 1.085197]\n",
      "273 [D loss: 0.054552, acc.: 99.98%] [G loss: 1.093575]\n",
      "274 [D loss: 0.053813, acc.: 99.99%] [G loss: 1.102483]\n",
      "275 [D loss: 0.051524, acc.: 100.00%] [G loss: 1.109257]\n",
      "276 [D loss: 0.049063, acc.: 100.00%] [G loss: 1.118965]\n",
      "277 [D loss: 0.045810, acc.: 100.00%] [G loss: 1.136890]\n",
      "278 [D loss: 0.047670, acc.: 100.00%] [G loss: 1.153036]\n",
      "279 [D loss: 0.048121, acc.: 100.00%] [G loss: 1.170152]\n",
      "280 [D loss: 0.055204, acc.: 100.00%] [G loss: 1.196842]\n",
      "281 [D loss: 0.057962, acc.: 100.00%] [G loss: 1.240816]\n",
      "282 [D loss: 0.057109, acc.: 100.00%] [G loss: 1.280366]\n",
      "283 [D loss: 0.054003, acc.: 100.00%] [G loss: 1.313068]\n",
      "284 [D loss: 0.049927, acc.: 100.00%] [G loss: 1.350708]\n",
      "285 [D loss: 0.047909, acc.: 100.00%] [G loss: 1.390695]\n",
      "286 [D loss: 0.046408, acc.: 100.00%] [G loss: 1.421355]\n",
      "287 [D loss: 0.045353, acc.: 100.00%] [G loss: 1.441363]\n",
      "288 [D loss: 0.040911, acc.: 99.99%] [G loss: 1.460811]\n",
      "289 [D loss: 0.038904, acc.: 99.98%] [G loss: 1.480002]\n",
      "290 [D loss: 0.037970, acc.: 99.99%] [G loss: 1.501890]\n",
      "291 [D loss: 0.035897, acc.: 99.98%] [G loss: 1.522526]\n",
      "292 [D loss: 0.035586, acc.: 99.95%] [G loss: 1.540443]\n",
      "293 [D loss: 0.033513, acc.: 99.94%] [G loss: 1.560813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 [D loss: 0.033903, acc.: 99.93%] [G loss: 1.584532]\n",
      "295 [D loss: 0.031989, acc.: 99.94%] [G loss: 1.608773]\n",
      "296 [D loss: 0.031227, acc.: 99.94%] [G loss: 1.632663]\n",
      "297 [D loss: 0.031446, acc.: 99.91%] [G loss: 1.654647]\n",
      "298 [D loss: 0.030295, acc.: 99.90%] [G loss: 1.671344]\n",
      "299 [D loss: 0.031308, acc.: 99.91%] [G loss: 1.698087]\n",
      "300 [D loss: 0.030250, acc.: 99.88%] [G loss: 1.734038]\n",
      "301 [D loss: 0.028519, acc.: 99.95%] [G loss: 1.766835]\n",
      "302 [D loss: 0.029075, acc.: 99.93%] [G loss: 1.796423]\n",
      "303 [D loss: 0.027907, acc.: 99.93%] [G loss: 1.821559]\n",
      "304 [D loss: 0.027092, acc.: 99.97%] [G loss: 1.860124]\n",
      "305 [D loss: 0.027601, acc.: 99.92%] [G loss: 1.892839]\n",
      "306 [D loss: 0.026511, acc.: 99.97%] [G loss: 1.923232]\n",
      "307 [D loss: 0.026042, acc.: 99.99%] [G loss: 1.954529]\n",
      "308 [D loss: 0.025645, acc.: 99.98%] [G loss: 1.985574]\n",
      "309 [D loss: 0.026216, acc.: 99.97%] [G loss: 2.015791]\n",
      "310 [D loss: 0.026568, acc.: 99.97%] [G loss: 2.053775]\n",
      "311 [D loss: 0.025916, acc.: 99.98%] [G loss: 2.100908]\n",
      "312 [D loss: 0.026175, acc.: 99.98%] [G loss: 2.150147]\n",
      "313 [D loss: 0.025992, acc.: 99.95%] [G loss: 2.202267]\n",
      "314 [D loss: 0.025410, acc.: 99.97%] [G loss: 2.258157]\n",
      "315 [D loss: 0.026835, acc.: 99.98%] [G loss: 2.311127]\n",
      "316 [D loss: 0.026794, acc.: 99.95%] [G loss: 2.364686]\n",
      "317 [D loss: 0.025456, acc.: 99.99%] [G loss: 2.416367]\n",
      "318 [D loss: 0.025543, acc.: 99.96%] [G loss: 2.463894]\n",
      "319 [D loss: 0.025921, acc.: 99.98%] [G loss: 2.504946]\n",
      "320 [D loss: 0.025055, acc.: 99.96%] [G loss: 2.530347]\n",
      "321 [D loss: 0.025338, acc.: 99.98%] [G loss: 2.560115]\n",
      "322 [D loss: 0.025847, acc.: 99.98%] [G loss: 2.583704]\n",
      "323 [D loss: 0.025802, acc.: 99.98%] [G loss: 2.610516]\n",
      "324 [D loss: 0.026248, acc.: 99.98%] [G loss: 2.635650]\n",
      "325 [D loss: 0.025654, acc.: 99.98%] [G loss: 2.662196]\n",
      "326 [D loss: 0.026550, acc.: 99.96%] [G loss: 2.683071]\n",
      "327 [D loss: 0.026796, acc.: 99.98%] [G loss: 2.705533]\n",
      "328 [D loss: 0.025793, acc.: 99.99%] [G loss: 2.724640]\n",
      "329 [D loss: 0.024619, acc.: 99.98%] [G loss: 2.736614]\n",
      "330 [D loss: 0.024257, acc.: 100.00%] [G loss: 2.742743]\n",
      "331 [D loss: 0.024351, acc.: 100.00%] [G loss: 2.756488]\n",
      "332 [D loss: 0.024254, acc.: 100.00%] [G loss: 2.770324]\n",
      "333 [D loss: 0.022402, acc.: 99.99%] [G loss: 2.772585]\n",
      "334 [D loss: 0.021589, acc.: 100.00%] [G loss: 2.768208]\n",
      "335 [D loss: 0.020832, acc.: 99.98%] [G loss: 2.781859]\n",
      "336 [D loss: 0.020262, acc.: 100.00%] [G loss: 2.808261]\n",
      "337 [D loss: 0.019639, acc.: 100.00%] [G loss: 2.833208]\n",
      "338 [D loss: 0.019043, acc.: 100.00%] [G loss: 2.847983]\n",
      "339 [D loss: 0.018589, acc.: 100.00%] [G loss: 2.858387]\n",
      "340 [D loss: 0.018011, acc.: 100.00%] [G loss: 2.869746]\n",
      "341 [D loss: 0.017651, acc.: 100.00%] [G loss: 2.875736]\n",
      "342 [D loss: 0.017177, acc.: 100.00%] [G loss: 2.886648]\n",
      "343 [D loss: 0.016598, acc.: 100.00%] [G loss: 2.898270]\n",
      "344 [D loss: 0.016418, acc.: 100.00%] [G loss: 2.909038]\n",
      "345 [D loss: 0.016483, acc.: 100.00%] [G loss: 2.916069]\n",
      "346 [D loss: 0.015705, acc.: 100.00%] [G loss: 2.930843]\n",
      "347 [D loss: 0.015431, acc.: 100.00%] [G loss: 2.947459]\n",
      "348 [D loss: 0.014999, acc.: 100.00%] [G loss: 2.969136]\n",
      "349 [D loss: 0.015502, acc.: 100.00%] [G loss: 2.977271]\n",
      "350 [D loss: 0.014756, acc.: 100.00%] [G loss: 2.993118]\n",
      "351 [D loss: 0.014608, acc.: 100.00%] [G loss: 3.016814]\n",
      "352 [D loss: 0.014071, acc.: 100.00%] [G loss: 3.043595]\n",
      "353 [D loss: 0.014152, acc.: 100.00%] [G loss: 3.071831]\n",
      "354 [D loss: 0.014116, acc.: 100.00%] [G loss: 3.085474]\n",
      "355 [D loss: 0.013739, acc.: 100.00%] [G loss: 3.090767]\n",
      "356 [D loss: 0.013582, acc.: 100.00%] [G loss: 3.092685]\n",
      "357 [D loss: 0.013358, acc.: 100.00%] [G loss: 3.084279]\n",
      "358 [D loss: 0.013424, acc.: 100.00%] [G loss: 3.078169]\n",
      "359 [D loss: 0.013478, acc.: 100.00%] [G loss: 3.074311]\n",
      "360 [D loss: 0.013786, acc.: 100.00%] [G loss: 3.069051]\n",
      "361 [D loss: 0.013539, acc.: 99.99%] [G loss: 3.066258]\n",
      "362 [D loss: 0.013331, acc.: 100.00%] [G loss: 3.068562]\n",
      "363 [D loss: 0.013782, acc.: 100.00%] [G loss: 3.081442]\n",
      "364 [D loss: 0.014043, acc.: 100.00%] [G loss: 3.094932]\n",
      "365 [D loss: 0.013960, acc.: 100.00%] [G loss: 3.103314]\n",
      "366 [D loss: 0.014040, acc.: 100.00%] [G loss: 3.105218]\n",
      "367 [D loss: 0.014078, acc.: 100.00%] [G loss: 3.103420]\n",
      "368 [D loss: 0.014201, acc.: 100.00%] [G loss: 3.101403]\n",
      "369 [D loss: 0.014229, acc.: 100.00%] [G loss: 3.122792]\n",
      "370 [D loss: 0.014453, acc.: 100.00%] [G loss: 3.119547]\n",
      "371 [D loss: 0.014467, acc.: 100.00%] [G loss: 3.124612]\n",
      "372 [D loss: 0.014542, acc.: 100.00%] [G loss: 3.118208]\n",
      "373 [D loss: 0.014666, acc.: 100.00%] [G loss: 3.107962]\n",
      "374 [D loss: 0.014532, acc.: 100.00%] [G loss: 3.095524]\n",
      "375 [D loss: 0.014783, acc.: 100.00%] [G loss: 3.095922]\n",
      "376 [D loss: 0.014829, acc.: 100.00%] [G loss: 3.099047]\n",
      "377 [D loss: 0.014420, acc.: 100.00%] [G loss: 3.104383]\n",
      "378 [D loss: 0.014565, acc.: 100.00%] [G loss: 3.104962]\n",
      "379 [D loss: 0.013800, acc.: 100.00%] [G loss: 3.118145]\n",
      "380 [D loss: 0.014046, acc.: 100.00%] [G loss: 3.125343]\n",
      "381 [D loss: 0.014211, acc.: 100.00%] [G loss: 3.127440]\n",
      "382 [D loss: 0.013832, acc.: 100.00%] [G loss: 3.122383]\n",
      "383 [D loss: 0.014270, acc.: 100.00%] [G loss: 3.118597]\n",
      "384 [D loss: 0.013633, acc.: 100.00%] [G loss: 3.097714]\n",
      "385 [D loss: 0.013687, acc.: 100.00%] [G loss: 3.106298]\n",
      "386 [D loss: 0.013160, acc.: 100.00%] [G loss: 3.101416]\n",
      "387 [D loss: 0.013186, acc.: 100.00%] [G loss: 3.103941]\n",
      "388 [D loss: 0.013556, acc.: 100.00%] [G loss: 3.087099]\n",
      "389 [D loss: 0.013024, acc.: 100.00%] [G loss: 3.090777]\n",
      "390 [D loss: 0.012584, acc.: 100.00%] [G loss: 3.093920]\n",
      "391 [D loss: 0.012661, acc.: 100.00%] [G loss: 3.111297]\n",
      "392 [D loss: 0.012117, acc.: 100.00%] [G loss: 3.146824]\n",
      "393 [D loss: 0.012099, acc.: 100.00%] [G loss: 3.164482]\n",
      "394 [D loss: 0.012216, acc.: 100.00%] [G loss: 3.178849]\n",
      "395 [D loss: 0.011828, acc.: 100.00%] [G loss: 3.180739]\n",
      "396 [D loss: 0.011661, acc.: 100.00%] [G loss: 3.187197]\n",
      "397 [D loss: 0.011308, acc.: 100.00%] [G loss: 3.192125]\n",
      "398 [D loss: 0.011052, acc.: 100.00%] [G loss: 3.188971]\n",
      "399 [D loss: 0.010958, acc.: 100.00%] [G loss: 3.193190]\n",
      "400 [D loss: 0.011002, acc.: 100.00%] [G loss: 3.194327]\n",
      "401 [D loss: 0.011048, acc.: 100.00%] [G loss: 3.169648]\n",
      "402 [D loss: 0.010735, acc.: 100.00%] [G loss: 3.165327]\n",
      "403 [D loss: 0.010439, acc.: 100.00%] [G loss: 3.177050]\n",
      "404 [D loss: 0.010673, acc.: 100.00%] [G loss: 3.177388]\n",
      "405 [D loss: 0.010779, acc.: 100.00%] [G loss: 3.160230]\n",
      "406 [D loss: 0.010504, acc.: 100.00%] [G loss: 3.161796]\n",
      "407 [D loss: 0.010231, acc.: 100.00%] [G loss: 3.151830]\n",
      "408 [D loss: 0.010106, acc.: 100.00%] [G loss: 3.143991]\n",
      "409 [D loss: 0.010176, acc.: 100.00%] [G loss: 3.122888]\n",
      "410 [D loss: 0.009936, acc.: 100.00%] [G loss: 3.112355]\n",
      "411 [D loss: 0.010034, acc.: 100.00%] [G loss: 3.107936]\n",
      "412 [D loss: 0.010232, acc.: 100.00%] [G loss: 3.084512]\n",
      "413 [D loss: 0.010389, acc.: 100.00%] [G loss: 3.039321]\n",
      "414 [D loss: 0.010156, acc.: 100.00%] [G loss: 2.996429]\n",
      "415 [D loss: 0.010615, acc.: 100.00%] [G loss: 2.971318]\n",
      "416 [D loss: 0.010269, acc.: 100.00%] [G loss: 2.950433]\n",
      "417 [D loss: 0.010375, acc.: 100.00%] [G loss: 2.917600]\n",
      "418 [D loss: 0.010716, acc.: 100.00%] [G loss: 2.901736]\n",
      "419 [D loss: 0.010803, acc.: 100.00%] [G loss: 2.876551]\n",
      "420 [D loss: 0.010965, acc.: 100.00%] [G loss: 2.847230]\n",
      "421 [D loss: 0.011563, acc.: 100.00%] [G loss: 2.823712]\n",
      "422 [D loss: 0.011708, acc.: 100.00%] [G loss: 2.786934]\n",
      "423 [D loss: 0.012012, acc.: 100.00%] [G loss: 2.741468]\n",
      "424 [D loss: 0.012425, acc.: 100.00%] [G loss: 2.729656]\n",
      "425 [D loss: 0.012750, acc.: 100.00%] [G loss: 2.725465]\n",
      "426 [D loss: 0.013088, acc.: 100.00%] [G loss: 2.692689]\n",
      "427 [D loss: 0.014230, acc.: 100.00%] [G loss: 2.683011]\n",
      "428 [D loss: 0.014203, acc.: 100.00%] [G loss: 2.664002]\n",
      "429 [D loss: 0.015004, acc.: 100.00%] [G loss: 2.632692]\n",
      "430 [D loss: 0.015623, acc.: 99.99%] [G loss: 2.616086]\n",
      "431 [D loss: 0.016014, acc.: 99.99%] [G loss: 2.619193]\n",
      "432 [D loss: 0.016879, acc.: 100.00%] [G loss: 2.630885]\n",
      "433 [D loss: 0.016916, acc.: 100.00%] [G loss: 2.645172]\n",
      "434 [D loss: 0.017091, acc.: 100.00%] [G loss: 2.671912]\n",
      "435 [D loss: 0.017437, acc.: 100.00%] [G loss: 2.689890]\n",
      "436 [D loss: 0.016685, acc.: 99.99%] [G loss: 2.739562]\n",
      "437 [D loss: 0.017683, acc.: 99.98%] [G loss: 2.768860]\n",
      "438 [D loss: 0.016394, acc.: 100.00%] [G loss: 2.789983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439 [D loss: 0.016439, acc.: 100.00%] [G loss: 2.812070]\n",
      "440 [D loss: 0.015347, acc.: 100.00%] [G loss: 2.835972]\n",
      "441 [D loss: 0.014984, acc.: 99.99%] [G loss: 2.843692]\n",
      "442 [D loss: 0.013621, acc.: 100.00%] [G loss: 2.844773]\n",
      "443 [D loss: 0.013402, acc.: 100.00%] [G loss: 2.883596]\n",
      "444 [D loss: 0.012467, acc.: 100.00%] [G loss: 2.902140]\n",
      "445 [D loss: 0.011996, acc.: 100.00%] [G loss: 2.931898]\n",
      "446 [D loss: 0.011569, acc.: 100.00%] [G loss: 2.980362]\n",
      "447 [D loss: 0.011298, acc.: 99.99%] [G loss: 3.019136]\n",
      "448 [D loss: 0.010848, acc.: 100.00%] [G loss: 3.039509]\n",
      "449 [D loss: 0.010436, acc.: 99.99%] [G loss: 3.030984]\n",
      "450 [D loss: 0.010044, acc.: 100.00%] [G loss: 3.020078]\n",
      "451 [D loss: 0.009959, acc.: 100.00%] [G loss: 2.997826]\n",
      "452 [D loss: 0.009899, acc.: 100.00%] [G loss: 2.968577]\n",
      "453 [D loss: 0.009936, acc.: 100.00%] [G loss: 2.908720]\n",
      "454 [D loss: 0.009660, acc.: 100.00%] [G loss: 2.844471]\n",
      "455 [D loss: 0.009461, acc.: 100.00%] [G loss: 2.793481]\n",
      "456 [D loss: 0.009513, acc.: 100.00%] [G loss: 2.763322]\n",
      "457 [D loss: 0.009668, acc.: 100.00%] [G loss: 2.733317]\n",
      "458 [D loss: 0.009326, acc.: 100.00%] [G loss: 2.698983]\n",
      "459 [D loss: 0.009241, acc.: 100.00%] [G loss: 2.654282]\n",
      "460 [D loss: 0.009632, acc.: 100.00%] [G loss: 2.619587]\n",
      "461 [D loss: 0.009992, acc.: 100.00%] [G loss: 2.583432]\n",
      "462 [D loss: 0.010241, acc.: 100.00%] [G loss: 2.552229]\n",
      "463 [D loss: 0.010550, acc.: 100.00%] [G loss: 2.524144]\n",
      "464 [D loss: 0.010529, acc.: 100.00%] [G loss: 2.504446]\n",
      "465 [D loss: 0.011138, acc.: 100.00%] [G loss: 2.468222]\n",
      "466 [D loss: 0.011011, acc.: 100.00%] [G loss: 2.471949]\n",
      "467 [D loss: 0.012172, acc.: 100.00%] [G loss: 2.465566]\n",
      "468 [D loss: 0.012418, acc.: 100.00%] [G loss: 2.462173]\n",
      "469 [D loss: 0.013132, acc.: 100.00%] [G loss: 2.441386]\n",
      "470 [D loss: 0.013485, acc.: 100.00%] [G loss: 2.393739]\n",
      "471 [D loss: 0.014414, acc.: 100.00%] [G loss: 2.340063]\n",
      "472 [D loss: 0.015523, acc.: 100.00%] [G loss: 2.324522]\n",
      "473 [D loss: 0.016979, acc.: 100.00%] [G loss: 2.372313]\n",
      "474 [D loss: 0.017658, acc.: 100.00%] [G loss: 2.458515]\n",
      "475 [D loss: 0.018843, acc.: 99.98%] [G loss: 2.587481]\n",
      "476 [D loss: 0.017919, acc.: 100.00%] [G loss: 2.693760]\n",
      "477 [D loss: 0.018270, acc.: 100.00%] [G loss: 2.767143]\n",
      "478 [D loss: 0.020340, acc.: 99.99%] [G loss: 2.788615]\n",
      "479 [D loss: 0.021943, acc.: 99.99%] [G loss: 2.846052]\n",
      "480 [D loss: 0.030886, acc.: 100.00%] [G loss: 3.183305]\n",
      "481 [D loss: 0.045450, acc.: 99.96%] [G loss: 3.384480]\n",
      "482 [D loss: 0.045044, acc.: 99.83%] [G loss: 3.542301]\n",
      "483 [D loss: 0.039365, acc.: 99.96%] [G loss: 3.783658]\n",
      "484 [D loss: 0.033972, acc.: 99.96%] [G loss: 3.823447]\n",
      "485 [D loss: 0.029186, acc.: 99.98%] [G loss: 3.822398]\n",
      "486 [D loss: 0.026492, acc.: 99.98%] [G loss: 3.854356]\n",
      "487 [D loss: 0.021462, acc.: 100.00%] [G loss: 3.777173]\n",
      "488 [D loss: 0.021500, acc.: 100.00%] [G loss: 3.634426]\n",
      "489 [D loss: 0.020115, acc.: 100.00%] [G loss: 3.529888]\n",
      "490 [D loss: 0.018089, acc.: 100.00%] [G loss: 3.435786]\n",
      "491 [D loss: 0.017246, acc.: 100.00%] [G loss: 3.377469]\n",
      "492 [D loss: 0.017298, acc.: 100.00%] [G loss: 3.334886]\n",
      "493 [D loss: 0.015412, acc.: 100.00%] [G loss: 3.298960]\n",
      "494 [D loss: 0.015322, acc.: 100.00%] [G loss: 3.266279]\n",
      "495 [D loss: 0.014959, acc.: 100.00%] [G loss: 3.215567]\n",
      "496 [D loss: 0.014352, acc.: 100.00%] [G loss: 3.160453]\n",
      "497 [D loss: 0.014545, acc.: 99.99%] [G loss: 3.106932]\n",
      "498 [D loss: 0.014642, acc.: 100.00%] [G loss: 3.055497]\n",
      "499 [D loss: 0.017211, acc.: 100.00%] [G loss: 3.016499]\n",
      "500 [D loss: 0.019751, acc.: 100.00%] [G loss: 2.992461]\n",
      "501 [D loss: 0.026421, acc.: 99.99%] [G loss: 2.911451]\n",
      "502 [D loss: 0.032015, acc.: 99.98%] [G loss: 2.795991]\n",
      "503 [D loss: 0.045407, acc.: 100.00%] [G loss: 2.636585]\n",
      "504 [D loss: 0.042342, acc.: 100.00%] [G loss: 2.548835]\n",
      "505 [D loss: 0.057015, acc.: 98.90%] [G loss: 2.404023]\n",
      "506 [D loss: 0.069936, acc.: 100.00%] [G loss: 2.184781]\n",
      "507 [D loss: 0.093435, acc.: 99.48%] [G loss: 1.746333]\n",
      "508 [D loss: 0.059821, acc.: 99.90%] [G loss: 1.455246]\n",
      "509 [D loss: 0.041652, acc.: 100.00%] [G loss: 1.288193]\n",
      "510 [D loss: 0.036775, acc.: 100.00%] [G loss: 1.306762]\n",
      "511 [D loss: 0.036284, acc.: 100.00%] [G loss: 1.374048]\n",
      "512 [D loss: 0.029903, acc.: 100.00%] [G loss: 1.430789]\n",
      "513 [D loss: 0.027406, acc.: 100.00%] [G loss: 1.468234]\n",
      "514 [D loss: 0.025685, acc.: 100.00%] [G loss: 1.467115]\n",
      "515 [D loss: 0.024410, acc.: 100.00%] [G loss: 1.447946]\n",
      "516 [D loss: 0.023445, acc.: 99.97%] [G loss: 1.398214]\n",
      "517 [D loss: 0.022726, acc.: 100.00%] [G loss: 1.343064]\n",
      "518 [D loss: 0.021227, acc.: 100.00%] [G loss: 1.284983]\n",
      "519 [D loss: 0.020987, acc.: 100.00%] [G loss: 1.240832]\n",
      "520 [D loss: 0.019251, acc.: 100.00%] [G loss: 1.242323]\n",
      "521 [D loss: 0.019224, acc.: 100.00%] [G loss: 1.257953]\n",
      "522 [D loss: 0.018629, acc.: 100.00%] [G loss: 1.286150]\n",
      "523 [D loss: 0.017591, acc.: 100.00%] [G loss: 1.319490]\n",
      "524 [D loss: 0.017447, acc.: 100.00%] [G loss: 1.351957]\n",
      "525 [D loss: 0.017275, acc.: 100.00%] [G loss: 1.388350]\n",
      "526 [D loss: 0.016657, acc.: 100.00%] [G loss: 1.424274]\n",
      "527 [D loss: 0.016366, acc.: 100.00%] [G loss: 1.450238]\n",
      "528 [D loss: 0.015798, acc.: 100.00%] [G loss: 1.472953]\n",
      "529 [D loss: 0.015785, acc.: 100.00%] [G loss: 1.511143]\n",
      "530 [D loss: 0.015027, acc.: 100.00%] [G loss: 1.554260]\n",
      "531 [D loss: 0.014855, acc.: 100.00%] [G loss: 1.617907]\n",
      "532 [D loss: 0.013957, acc.: 100.00%] [G loss: 1.699807]\n",
      "533 [D loss: 0.013552, acc.: 100.00%] [G loss: 1.788476]\n",
      "534 [D loss: 0.013659, acc.: 100.00%] [G loss: 1.872850]\n",
      "535 [D loss: 0.013439, acc.: 100.00%] [G loss: 1.953212]\n",
      "536 [D loss: 0.012930, acc.: 100.00%] [G loss: 2.013579]\n",
      "537 [D loss: 0.012975, acc.: 100.00%] [G loss: 2.050762]\n",
      "538 [D loss: 0.013144, acc.: 100.00%] [G loss: 2.079082]\n",
      "539 [D loss: 0.012997, acc.: 100.00%] [G loss: 2.153531]\n",
      "540 [D loss: 0.013385, acc.: 100.00%] [G loss: 2.257254]\n",
      "541 [D loss: 0.013384, acc.: 100.00%] [G loss: 2.363339]\n",
      "542 [D loss: 0.012941, acc.: 100.00%] [G loss: 2.437859]\n",
      "543 [D loss: 0.012926, acc.: 100.00%] [G loss: 2.502393]\n",
      "544 [D loss: 0.013210, acc.: 100.00%] [G loss: 2.583987]\n",
      "545 [D loss: 0.013133, acc.: 100.00%] [G loss: 2.666507]\n",
      "546 [D loss: 0.013763, acc.: 100.00%] [G loss: 2.725819]\n",
      "547 [D loss: 0.013926, acc.: 100.00%] [G loss: 2.780406]\n",
      "548 [D loss: 0.015056, acc.: 100.00%] [G loss: 2.817569]\n",
      "549 [D loss: 0.014896, acc.: 100.00%] [G loss: 2.857913]\n",
      "550 [D loss: 0.014999, acc.: 100.00%] [G loss: 2.895266]\n",
      "551 [D loss: 0.015500, acc.: 100.00%] [G loss: 2.925251]\n",
      "552 [D loss: 0.015800, acc.: 100.00%] [G loss: 2.962834]\n",
      "553 [D loss: 0.016951, acc.: 100.00%] [G loss: 3.023117]\n",
      "554 [D loss: 0.017383, acc.: 100.00%] [G loss: 3.076691]\n",
      "555 [D loss: 0.017237, acc.: 100.00%] [G loss: 3.096019]\n",
      "556 [D loss: 0.018205, acc.: 100.00%] [G loss: 3.092000]\n",
      "557 [D loss: 0.018629, acc.: 100.00%] [G loss: 3.089082]\n",
      "558 [D loss: 0.018760, acc.: 100.00%] [G loss: 3.108297]\n",
      "559 [D loss: 0.020647, acc.: 100.00%] [G loss: 3.084119]\n",
      "560 [D loss: 0.022507, acc.: 100.00%] [G loss: 3.100006]\n",
      "561 [D loss: 0.024189, acc.: 100.00%] [G loss: 3.175035]\n",
      "562 [D loss: 0.021900, acc.: 100.00%] [G loss: 3.206735]\n",
      "563 [D loss: 0.022391, acc.: 100.00%] [G loss: 3.240488]\n",
      "564 [D loss: 0.022947, acc.: 100.00%] [G loss: 3.313920]\n",
      "565 [D loss: 0.029472, acc.: 100.00%] [G loss: 3.234832]\n",
      "566 [D loss: 0.029720, acc.: 100.00%] [G loss: 3.001053]\n",
      "567 [D loss: 0.030177, acc.: 100.00%] [G loss: 2.890805]\n",
      "568 [D loss: 0.027531, acc.: 100.00%] [G loss: 2.861049]\n",
      "569 [D loss: 0.030724, acc.: 100.00%] [G loss: 2.869328]\n",
      "570 [D loss: 0.030565, acc.: 100.00%] [G loss: 2.885155]\n",
      "571 [D loss: 0.030319, acc.: 100.00%] [G loss: 2.880451]\n",
      "572 [D loss: 0.029677, acc.: 99.99%] [G loss: 2.847949]\n",
      "573 [D loss: 0.029120, acc.: 99.99%] [G loss: 2.806234]\n",
      "574 [D loss: 0.027448, acc.: 100.00%] [G loss: 2.774849]\n",
      "575 [D loss: 0.027319, acc.: 100.00%] [G loss: 2.733318]\n",
      "576 [D loss: 0.026240, acc.: 100.00%] [G loss: 2.644983]\n",
      "577 [D loss: 0.025744, acc.: 100.00%] [G loss: 2.565367]\n",
      "578 [D loss: 0.025549, acc.: 100.00%] [G loss: 2.530910]\n",
      "579 [D loss: 0.025515, acc.: 100.00%] [G loss: 2.562808]\n",
      "580 [D loss: 0.024379, acc.: 100.00%] [G loss: 2.619378]\n",
      "581 [D loss: 0.023887, acc.: 100.00%] [G loss: 2.663654]\n",
      "582 [D loss: 0.023847, acc.: 99.99%] [G loss: 2.692351]\n",
      "583 [D loss: 0.022834, acc.: 100.00%] [G loss: 2.734936]\n",
      "584 [D loss: 0.022786, acc.: 100.00%] [G loss: 2.749157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585 [D loss: 0.021812, acc.: 100.00%] [G loss: 2.755517]\n",
      "586 [D loss: 0.021468, acc.: 100.00%] [G loss: 2.731687]\n",
      "587 [D loss: 0.020664, acc.: 100.00%] [G loss: 2.733606]\n",
      "588 [D loss: 0.021200, acc.: 100.00%] [G loss: 2.742759]\n",
      "589 [D loss: 0.020431, acc.: 100.00%] [G loss: 2.743729]\n",
      "590 [D loss: 0.020376, acc.: 100.00%] [G loss: 2.739049]\n",
      "591 [D loss: 0.020385, acc.: 100.00%] [G loss: 2.755546]\n",
      "592 [D loss: 0.020807, acc.: 100.00%] [G loss: 2.780550]\n",
      "593 [D loss: 0.019890, acc.: 100.00%] [G loss: 2.808397]\n",
      "594 [D loss: 0.019864, acc.: 100.00%] [G loss: 2.818349]\n",
      "595 [D loss: 0.019496, acc.: 100.00%] [G loss: 2.822618]\n",
      "596 [D loss: 0.019099, acc.: 100.00%] [G loss: 2.842066]\n",
      "597 [D loss: 0.018869, acc.: 100.00%] [G loss: 2.861638]\n",
      "598 [D loss: 0.018009, acc.: 100.00%] [G loss: 2.898223]\n",
      "599 [D loss: 0.018268, acc.: 99.99%] [G loss: 2.930706]\n",
      "600 [D loss: 0.017918, acc.: 100.00%] [G loss: 2.967257]\n",
      "601 [D loss: 0.017720, acc.: 100.00%] [G loss: 2.986391]\n",
      "602 [D loss: 0.018245, acc.: 100.00%] [G loss: 2.986443]\n",
      "603 [D loss: 0.017158, acc.: 100.00%] [G loss: 2.979268]\n",
      "604 [D loss: 0.017071, acc.: 99.99%] [G loss: 2.977018]\n",
      "605 [D loss: 0.017556, acc.: 100.00%] [G loss: 2.973833]\n",
      "606 [D loss: 0.016357, acc.: 100.00%] [G loss: 2.964146]\n",
      "607 [D loss: 0.016646, acc.: 100.00%] [G loss: 2.952355]\n",
      "608 [D loss: 0.016170, acc.: 100.00%] [G loss: 2.937845]\n",
      "609 [D loss: 0.016002, acc.: 100.00%] [G loss: 2.901151]\n",
      "610 [D loss: 0.016096, acc.: 99.99%] [G loss: 2.872346]\n",
      "611 [D loss: 0.016379, acc.: 100.00%] [G loss: 2.847540]\n",
      "612 [D loss: 0.015879, acc.: 100.00%] [G loss: 2.846899]\n",
      "613 [D loss: 0.015342, acc.: 100.00%] [G loss: 2.870138]\n",
      "614 [D loss: 0.014719, acc.: 100.00%] [G loss: 2.896470]\n",
      "615 [D loss: 0.015280, acc.: 100.00%] [G loss: 2.907990]\n",
      "616 [D loss: 0.014947, acc.: 100.00%] [G loss: 2.891975]\n",
      "617 [D loss: 0.014829, acc.: 100.00%] [G loss: 2.864173]\n",
      "618 [D loss: 0.015000, acc.: 100.00%] [G loss: 2.829778]\n",
      "619 [D loss: 0.014430, acc.: 100.00%] [G loss: 2.800681]\n",
      "620 [D loss: 0.014456, acc.: 100.00%] [G loss: 2.777087]\n",
      "621 [D loss: 0.014150, acc.: 100.00%] [G loss: 2.783990]\n",
      "622 [D loss: 0.014566, acc.: 100.00%] [G loss: 2.792643]\n",
      "623 [D loss: 0.014326, acc.: 100.00%] [G loss: 2.804412]\n",
      "624 [D loss: 0.014142, acc.: 100.00%] [G loss: 2.809606]\n",
      "625 [D loss: 0.013631, acc.: 100.00%] [G loss: 2.812867]\n",
      "626 [D loss: 0.013475, acc.: 100.00%] [G loss: 2.813717]\n",
      "627 [D loss: 0.013120, acc.: 100.00%] [G loss: 2.807817]\n",
      "628 [D loss: 0.013101, acc.: 100.00%] [G loss: 2.801350]\n",
      "629 [D loss: 0.013122, acc.: 100.00%] [G loss: 2.821551]\n",
      "630 [D loss: 0.012711, acc.: 100.00%] [G loss: 2.835463]\n",
      "631 [D loss: 0.013341, acc.: 99.99%] [G loss: 2.828450]\n",
      "632 [D loss: 0.013122, acc.: 99.99%] [G loss: 2.812640]\n",
      "633 [D loss: 0.012914, acc.: 100.00%] [G loss: 2.804118]\n",
      "634 [D loss: 0.012880, acc.: 100.00%] [G loss: 2.802036]\n",
      "635 [D loss: 0.012320, acc.: 100.00%] [G loss: 2.805927]\n",
      "636 [D loss: 0.012392, acc.: 100.00%] [G loss: 2.810364]\n",
      "637 [D loss: 0.012490, acc.: 100.00%] [G loss: 2.809495]\n",
      "638 [D loss: 0.012552, acc.: 100.00%] [G loss: 2.805709]\n",
      "639 [D loss: 0.012074, acc.: 100.00%] [G loss: 2.813252]\n",
      "640 [D loss: 0.012217, acc.: 100.00%] [G loss: 2.829691]\n",
      "641 [D loss: 0.012094, acc.: 100.00%] [G loss: 2.860579]\n",
      "642 [D loss: 0.012246, acc.: 100.00%] [G loss: 2.888321]\n",
      "643 [D loss: 0.011347, acc.: 100.00%] [G loss: 2.905041]\n",
      "644 [D loss: 0.012199, acc.: 100.00%] [G loss: 2.910108]\n",
      "645 [D loss: 0.011456, acc.: 100.00%] [G loss: 2.916095]\n",
      "646 [D loss: 0.012119, acc.: 100.00%] [G loss: 2.927402]\n",
      "647 [D loss: 0.011266, acc.: 100.00%] [G loss: 2.947728]\n",
      "648 [D loss: 0.011471, acc.: 100.00%] [G loss: 2.967289]\n",
      "649 [D loss: 0.011520, acc.: 100.00%] [G loss: 2.978207]\n",
      "650 [D loss: 0.011192, acc.: 100.00%] [G loss: 2.967493]\n",
      "651 [D loss: 0.010921, acc.: 100.00%] [G loss: 2.970984]\n",
      "652 [D loss: 0.011091, acc.: 100.00%] [G loss: 2.974109]\n",
      "653 [D loss: 0.010977, acc.: 100.00%] [G loss: 2.990009]\n",
      "654 [D loss: 0.011233, acc.: 100.00%] [G loss: 3.018311]\n",
      "655 [D loss: 0.010772, acc.: 100.00%] [G loss: 3.037712]\n",
      "656 [D loss: 0.010526, acc.: 100.00%] [G loss: 3.063108]\n",
      "657 [D loss: 0.010691, acc.: 100.00%] [G loss: 3.074579]\n",
      "658 [D loss: 0.010758, acc.: 100.00%] [G loss: 3.071022]\n",
      "659 [D loss: 0.010437, acc.: 100.00%] [G loss: 3.064592]\n",
      "660 [D loss: 0.010377, acc.: 100.00%] [G loss: 3.065404]\n",
      "661 [D loss: 0.010105, acc.: 100.00%] [G loss: 3.076971]\n",
      "662 [D loss: 0.010200, acc.: 100.00%] [G loss: 3.086030]\n",
      "663 [D loss: 0.010199, acc.: 100.00%] [G loss: 3.098413]\n",
      "664 [D loss: 0.010089, acc.: 100.00%] [G loss: 3.095180]\n",
      "665 [D loss: 0.009863, acc.: 100.00%] [G loss: 3.062673]\n",
      "666 [D loss: 0.009989, acc.: 100.00%] [G loss: 3.030897]\n",
      "667 [D loss: 0.010358, acc.: 100.00%] [G loss: 3.003366]\n",
      "668 [D loss: 0.009794, acc.: 99.99%] [G loss: 2.990075]\n",
      "669 [D loss: 0.009917, acc.: 100.00%] [G loss: 2.988377]\n",
      "670 [D loss: 0.009817, acc.: 100.00%] [G loss: 2.994407]\n",
      "671 [D loss: 0.010154, acc.: 99.99%] [G loss: 3.008469]\n",
      "672 [D loss: 0.009427, acc.: 100.00%] [G loss: 3.018860]\n",
      "673 [D loss: 0.010461, acc.: 99.99%] [G loss: 3.006765]\n",
      "674 [D loss: 0.010110, acc.: 99.98%] [G loss: 2.988956]\n",
      "675 [D loss: 0.009486, acc.: 100.00%] [G loss: 2.985321]\n",
      "676 [D loss: 0.009644, acc.: 99.99%] [G loss: 2.986898]\n",
      "677 [D loss: 0.009541, acc.: 100.00%] [G loss: 2.975640]\n",
      "678 [D loss: 0.009525, acc.: 99.98%] [G loss: 2.969127]\n",
      "679 [D loss: 0.009567, acc.: 99.98%] [G loss: 2.955149]\n",
      "680 [D loss: 0.010157, acc.: 99.97%] [G loss: 2.950341]\n",
      "681 [D loss: 0.010090, acc.: 99.98%] [G loss: 2.959719]\n",
      "682 [D loss: 0.009954, acc.: 100.00%] [G loss: 2.922097]\n",
      "683 [D loss: 0.009530, acc.: 99.98%] [G loss: 2.926519]\n",
      "684 [D loss: 0.009815, acc.: 99.99%] [G loss: 2.919820]\n",
      "685 [D loss: 0.009782, acc.: 99.97%] [G loss: 2.918802]\n",
      "686 [D loss: 0.009713, acc.: 100.00%] [G loss: 2.920364]\n",
      "687 [D loss: 0.009731, acc.: 100.00%] [G loss: 2.960768]\n",
      "688 [D loss: 0.009200, acc.: 99.98%] [G loss: 3.003140]\n",
      "689 [D loss: 0.009190, acc.: 99.99%] [G loss: 3.051147]\n",
      "690 [D loss: 0.009632, acc.: 99.98%] [G loss: 3.090791]\n",
      "691 [D loss: 0.009455, acc.: 99.98%] [G loss: 3.124996]\n",
      "692 [D loss: 0.009346, acc.: 100.00%] [G loss: 3.155531]\n",
      "693 [D loss: 0.009070, acc.: 100.00%] [G loss: 3.178756]\n",
      "694 [D loss: 0.009046, acc.: 99.98%] [G loss: 3.216865]\n",
      "695 [D loss: 0.008783, acc.: 99.99%] [G loss: 3.275255]\n",
      "696 [D loss: 0.009095, acc.: 100.00%] [G loss: 3.347656]\n",
      "697 [D loss: 0.008650, acc.: 100.00%] [G loss: 3.406317]\n",
      "698 [D loss: 0.009097, acc.: 99.99%] [G loss: 3.451727]\n",
      "699 [D loss: 0.009164, acc.: 99.99%] [G loss: 3.476145]\n",
      "700 [D loss: 0.008931, acc.: 100.00%] [G loss: 3.489000]\n",
      "701 [D loss: 0.008516, acc.: 100.00%] [G loss: 3.505596]\n",
      "702 [D loss: 0.008934, acc.: 100.00%] [G loss: 3.531433]\n",
      "703 [D loss: 0.008723, acc.: 100.00%] [G loss: 3.577671]\n",
      "704 [D loss: 0.008345, acc.: 100.00%] [G loss: 3.618206]\n",
      "705 [D loss: 0.008237, acc.: 100.00%] [G loss: 3.646459]\n",
      "706 [D loss: 0.008164, acc.: 100.00%] [G loss: 3.659445]\n",
      "707 [D loss: 0.008659, acc.: 100.00%] [G loss: 3.647635]\n",
      "708 [D loss: 0.008312, acc.: 100.00%] [G loss: 3.625204]\n",
      "709 [D loss: 0.008189, acc.: 100.00%] [G loss: 3.594093]\n",
      "710 [D loss: 0.008291, acc.: 99.99%] [G loss: 3.592672]\n",
      "711 [D loss: 0.008315, acc.: 99.99%] [G loss: 3.603589]\n",
      "712 [D loss: 0.008319, acc.: 100.00%] [G loss: 3.617810]\n",
      "713 [D loss: 0.007954, acc.: 100.00%] [G loss: 3.636060]\n",
      "714 [D loss: 0.008060, acc.: 99.98%] [G loss: 3.654507]\n",
      "715 [D loss: 0.007984, acc.: 99.99%] [G loss: 3.679189]\n",
      "716 [D loss: 0.007790, acc.: 99.98%] [G loss: 3.711636]\n",
      "717 [D loss: 0.007841, acc.: 99.99%] [G loss: 3.783321]\n",
      "718 [D loss: 0.008002, acc.: 100.00%] [G loss: 3.830471]\n",
      "719 [D loss: 0.007367, acc.: 100.00%] [G loss: 3.861675]\n",
      "720 [D loss: 0.008230, acc.: 100.00%] [G loss: 3.909738]\n",
      "721 [D loss: 0.007911, acc.: 100.00%] [G loss: 3.946598]\n",
      "722 [D loss: 0.007752, acc.: 100.00%] [G loss: 3.999275]\n",
      "723 [D loss: 0.007953, acc.: 100.00%] [G loss: 4.054019]\n",
      "724 [D loss: 0.008157, acc.: 100.00%] [G loss: 4.090775]\n",
      "725 [D loss: 0.007793, acc.: 100.00%] [G loss: 4.134630]\n",
      "726 [D loss: 0.007459, acc.: 100.00%] [G loss: 4.161203]\n",
      "727 [D loss: 0.007273, acc.: 100.00%] [G loss: 4.185142]\n",
      "728 [D loss: 0.007295, acc.: 100.00%] [G loss: 4.193457]\n",
      "729 [D loss: 0.007631, acc.: 100.00%] [G loss: 4.167448]\n",
      "730 [D loss: 0.007402, acc.: 100.00%] [G loss: 4.139638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731 [D loss: 0.007007, acc.: 100.00%] [G loss: 4.122212]\n",
      "732 [D loss: 0.007026, acc.: 100.00%] [G loss: 4.122414]\n",
      "733 [D loss: 0.006968, acc.: 100.00%] [G loss: 4.149658]\n",
      "734 [D loss: 0.007242, acc.: 100.00%] [G loss: 4.179526]\n",
      "735 [D loss: 0.007217, acc.: 100.00%] [G loss: 4.210777]\n",
      "736 [D loss: 0.006992, acc.: 100.00%] [G loss: 4.225337]\n",
      "737 [D loss: 0.006925, acc.: 100.00%] [G loss: 4.240183]\n",
      "738 [D loss: 0.007075, acc.: 100.00%] [G loss: 4.237346]\n",
      "739 [D loss: 0.007020, acc.: 100.00%] [G loss: 4.222518]\n",
      "740 [D loss: 0.007331, acc.: 99.98%] [G loss: 4.220695]\n",
      "741 [D loss: 0.006773, acc.: 100.00%] [G loss: 4.243931]\n",
      "742 [D loss: 0.006989, acc.: 100.00%] [G loss: 4.262623]\n",
      "743 [D loss: 0.006849, acc.: 100.00%] [G loss: 4.269698]\n",
      "744 [D loss: 0.006973, acc.: 100.00%] [G loss: 4.261486]\n",
      "745 [D loss: 0.007037, acc.: 100.00%] [G loss: 4.266458]\n",
      "746 [D loss: 0.006730, acc.: 100.00%] [G loss: 4.271847]\n",
      "747 [D loss: 0.006755, acc.: 99.99%] [G loss: 4.323784]\n",
      "748 [D loss: 0.006640, acc.: 100.00%] [G loss: 4.370984]\n",
      "749 [D loss: 0.006451, acc.: 100.00%] [G loss: 4.398489]\n",
      "750 [D loss: 0.006531, acc.: 100.00%] [G loss: 4.420566]\n",
      "751 [D loss: 0.006207, acc.: 100.00%] [G loss: 4.429036]\n",
      "752 [D loss: 0.006418, acc.: 100.00%] [G loss: 4.439660]\n",
      "753 [D loss: 0.006475, acc.: 100.00%] [G loss: 4.436859]\n",
      "754 [D loss: 0.006296, acc.: 100.00%] [G loss: 4.448534]\n",
      "755 [D loss: 0.006364, acc.: 100.00%] [G loss: 4.459387]\n",
      "756 [D loss: 0.006521, acc.: 99.98%] [G loss: 4.489093]\n",
      "757 [D loss: 0.006307, acc.: 100.00%] [G loss: 4.534199]\n",
      "758 [D loss: 0.006293, acc.: 100.00%] [G loss: 4.554327]\n",
      "759 [D loss: 0.006425, acc.: 100.00%] [G loss: 4.574877]\n",
      "760 [D loss: 0.006114, acc.: 100.00%] [G loss: 4.580657]\n",
      "761 [D loss: 0.006222, acc.: 100.00%] [G loss: 4.556795]\n",
      "762 [D loss: 0.006046, acc.: 100.00%] [G loss: 4.524691]\n",
      "763 [D loss: 0.006103, acc.: 100.00%] [G loss: 4.507808]\n",
      "764 [D loss: 0.006074, acc.: 100.00%] [G loss: 4.518480]\n",
      "765 [D loss: 0.006298, acc.: 99.99%] [G loss: 4.535945]\n",
      "766 [D loss: 0.005896, acc.: 100.00%] [G loss: 4.557385]\n",
      "767 [D loss: 0.006112, acc.: 100.00%] [G loss: 4.572650]\n",
      "768 [D loss: 0.005874, acc.: 100.00%] [G loss: 4.586031]\n",
      "769 [D loss: 0.005711, acc.: 100.00%] [G loss: 4.608793]\n",
      "770 [D loss: 0.005818, acc.: 100.00%] [G loss: 4.627231]\n",
      "771 [D loss: 0.006089, acc.: 100.00%] [G loss: 4.628346]\n",
      "772 [D loss: 0.005802, acc.: 100.00%] [G loss: 4.623273]\n",
      "773 [D loss: 0.005979, acc.: 100.00%] [G loss: 4.618011]\n",
      "774 [D loss: 0.006089, acc.: 100.00%] [G loss: 4.583909]\n",
      "775 [D loss: 0.005831, acc.: 100.00%] [G loss: 4.540490]\n",
      "776 [D loss: 0.005794, acc.: 100.00%] [G loss: 4.528227]\n",
      "777 [D loss: 0.005941, acc.: 100.00%] [G loss: 4.534072]\n",
      "778 [D loss: 0.006070, acc.: 100.00%] [G loss: 4.534383]\n",
      "779 [D loss: 0.005802, acc.: 100.00%] [G loss: 4.552746]\n",
      "780 [D loss: 0.005716, acc.: 100.00%] [G loss: 4.567463]\n",
      "781 [D loss: 0.005607, acc.: 100.00%] [G loss: 4.586021]\n",
      "782 [D loss: 0.005752, acc.: 100.00%] [G loss: 4.583063]\n",
      "783 [D loss: 0.005645, acc.: 100.00%] [G loss: 4.561416]\n",
      "784 [D loss: 0.005521, acc.: 100.00%] [G loss: 4.540370]\n",
      "785 [D loss: 0.005672, acc.: 100.00%] [G loss: 4.496504]\n",
      "786 [D loss: 0.005506, acc.: 100.00%] [G loss: 4.477214]\n",
      "787 [D loss: 0.005406, acc.: 100.00%] [G loss: 4.450489]\n",
      "788 [D loss: 0.005711, acc.: 100.00%] [G loss: 4.443503]\n",
      "789 [D loss: 0.005539, acc.: 100.00%] [G loss: 4.440481]\n",
      "790 [D loss: 0.005484, acc.: 100.00%] [G loss: 4.426896]\n",
      "791 [D loss: 0.005547, acc.: 100.00%] [G loss: 4.412976]\n",
      "792 [D loss: 0.005202, acc.: 100.00%] [G loss: 4.399568]\n",
      "793 [D loss: 0.005334, acc.: 100.00%] [G loss: 4.374659]\n",
      "794 [D loss: 0.005586, acc.: 100.00%] [G loss: 4.358643]\n",
      "795 [D loss: 0.005164, acc.: 100.00%] [G loss: 4.354807]\n",
      "796 [D loss: 0.005342, acc.: 100.00%] [G loss: 4.343852]\n",
      "797 [D loss: 0.005269, acc.: 100.00%] [G loss: 4.355321]\n",
      "798 [D loss: 0.005262, acc.: 100.00%] [G loss: 4.355264]\n",
      "799 [D loss: 0.005372, acc.: 100.00%] [G loss: 4.341675]\n",
      "800 [D loss: 0.005219, acc.: 100.00%] [G loss: 4.327313]\n",
      "801 [D loss: 0.005298, acc.: 100.00%] [G loss: 4.318943]\n",
      "802 [D loss: 0.005511, acc.: 100.00%] [G loss: 4.308902]\n",
      "803 [D loss: 0.005207, acc.: 100.00%] [G loss: 4.282691]\n",
      "804 [D loss: 0.005257, acc.: 100.00%] [G loss: 4.257828]\n",
      "805 [D loss: 0.005192, acc.: 100.00%] [G loss: 4.253718]\n",
      "806 [D loss: 0.005303, acc.: 100.00%] [G loss: 4.255674]\n",
      "807 [D loss: 0.005275, acc.: 100.00%] [G loss: 4.254060]\n",
      "808 [D loss: 0.005009, acc.: 100.00%] [G loss: 4.251945]\n",
      "809 [D loss: 0.005101, acc.: 100.00%] [G loss: 4.243976]\n",
      "810 [D loss: 0.004988, acc.: 100.00%] [G loss: 4.223841]\n",
      "811 [D loss: 0.004967, acc.: 100.00%] [G loss: 4.221115]\n",
      "812 [D loss: 0.004904, acc.: 100.00%] [G loss: 4.226674]\n",
      "813 [D loss: 0.004838, acc.: 100.00%] [G loss: 4.230346]\n",
      "814 [D loss: 0.004911, acc.: 100.00%] [G loss: 4.234037]\n",
      "815 [D loss: 0.004854, acc.: 100.00%] [G loss: 4.233783]\n",
      "816 [D loss: 0.004870, acc.: 100.00%] [G loss: 4.238187]\n",
      "817 [D loss: 0.005205, acc.: 100.00%] [G loss: 4.211750]\n",
      "818 [D loss: 0.004948, acc.: 100.00%] [G loss: 4.187146]\n",
      "819 [D loss: 0.004851, acc.: 100.00%] [G loss: 4.179173]\n",
      "820 [D loss: 0.004866, acc.: 100.00%] [G loss: 4.174093]\n",
      "821 [D loss: 0.004864, acc.: 100.00%] [G loss: 4.186854]\n",
      "822 [D loss: 0.004856, acc.: 100.00%] [G loss: 4.216885]\n",
      "823 [D loss: 0.004753, acc.: 100.00%] [G loss: 4.222651]\n",
      "824 [D loss: 0.004848, acc.: 100.00%] [G loss: 4.221460]\n",
      "825 [D loss: 0.004630, acc.: 100.00%] [G loss: 4.233019]\n",
      "826 [D loss: 0.005000, acc.: 99.98%] [G loss: 4.249523]\n",
      "827 [D loss: 0.004593, acc.: 100.00%] [G loss: 4.256993]\n",
      "828 [D loss: 0.004782, acc.: 100.00%] [G loss: 4.255177]\n",
      "829 [D loss: 0.004518, acc.: 99.99%] [G loss: 4.254500]\n",
      "830 [D loss: 0.004547, acc.: 100.00%] [G loss: 4.267738]\n",
      "831 [D loss: 0.004539, acc.: 100.00%] [G loss: 4.277693]\n",
      "832 [D loss: 0.004676, acc.: 100.00%] [G loss: 4.281286]\n",
      "833 [D loss: 0.004473, acc.: 100.00%] [G loss: 4.276331]\n",
      "834 [D loss: 0.004517, acc.: 100.00%] [G loss: 4.255353]\n",
      "835 [D loss: 0.004577, acc.: 100.00%] [G loss: 4.238599]\n",
      "836 [D loss: 0.004567, acc.: 100.00%] [G loss: 4.223639]\n",
      "837 [D loss: 0.004602, acc.: 100.00%] [G loss: 4.238595]\n",
      "838 [D loss: 0.004540, acc.: 100.00%] [G loss: 4.221781]\n",
      "839 [D loss: 0.004503, acc.: 100.00%] [G loss: 4.201715]\n",
      "840 [D loss: 0.004253, acc.: 100.00%] [G loss: 4.158379]\n",
      "841 [D loss: 0.004526, acc.: 99.99%] [G loss: 4.110582]\n",
      "842 [D loss: 0.004451, acc.: 100.00%] [G loss: 3.960666]\n",
      "843 [D loss: 0.004431, acc.: 100.00%] [G loss: 3.785465]\n",
      "844 [D loss: 0.004907, acc.: 99.98%] [G loss: 3.784459]\n",
      "845 [D loss: 0.005185, acc.: 100.00%] [G loss: 3.766556]\n",
      "846 [D loss: 0.005432, acc.: 99.98%] [G loss: 3.534650]\n",
      "847 [D loss: 0.004865, acc.: 100.00%] [G loss: 3.056927]\n",
      "848 [D loss: 0.004351, acc.: 100.00%] [G loss: 2.653290]\n",
      "849 [D loss: 0.004514, acc.: 99.99%] [G loss: 2.397075]\n",
      "850 [D loss: 0.004214, acc.: 100.00%] [G loss: 2.226113]\n",
      "851 [D loss: 0.004113, acc.: 100.00%] [G loss: 2.102140]\n",
      "852 [D loss: 0.004216, acc.: 100.00%] [G loss: 1.971770]\n",
      "853 [D loss: 0.004109, acc.: 100.00%] [G loss: 1.809960]\n",
      "854 [D loss: 0.004205, acc.: 100.00%] [G loss: 1.713531]\n",
      "855 [D loss: 0.004294, acc.: 100.00%] [G loss: 1.608382]\n",
      "856 [D loss: 0.004417, acc.: 100.00%] [G loss: 1.547931]\n",
      "857 [D loss: 0.004458, acc.: 100.00%] [G loss: 1.495777]\n",
      "858 [D loss: 0.004444, acc.: 100.00%] [G loss: 1.428418]\n",
      "859 [D loss: 0.004280, acc.: 100.00%] [G loss: 1.391559]\n",
      "860 [D loss: 0.004735, acc.: 100.00%] [G loss: 1.363334]\n",
      "861 [D loss: 0.004355, acc.: 100.00%] [G loss: 1.321422]\n",
      "862 [D loss: 0.004887, acc.: 100.00%] [G loss: 1.297058]\n",
      "863 [D loss: 0.004381, acc.: 100.00%] [G loss: 1.286077]\n",
      "864 [D loss: 0.004722, acc.: 100.00%] [G loss: 1.295048]\n",
      "865 [D loss: 0.004526, acc.: 100.00%] [G loss: 1.298502]\n",
      "866 [D loss: 0.004748, acc.: 100.00%] [G loss: 1.314800]\n",
      "867 [D loss: 0.004699, acc.: 100.00%] [G loss: 1.322730]\n",
      "868 [D loss: 0.004899, acc.: 100.00%] [G loss: 1.336286]\n",
      "869 [D loss: 0.004776, acc.: 100.00%] [G loss: 1.349489]\n",
      "870 [D loss: 0.004766, acc.: 100.00%] [G loss: 1.343861]\n",
      "871 [D loss: 0.004741, acc.: 100.00%] [G loss: 1.339282]\n",
      "872 [D loss: 0.004663, acc.: 100.00%] [G loss: 1.341761]\n",
      "873 [D loss: 0.004723, acc.: 100.00%] [G loss: 1.336397]\n",
      "874 [D loss: 0.004792, acc.: 100.00%] [G loss: 1.322417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875 [D loss: 0.004631, acc.: 100.00%] [G loss: 1.306555]\n",
      "876 [D loss: 0.004818, acc.: 100.00%] [G loss: 1.315861]\n",
      "877 [D loss: 0.004636, acc.: 100.00%] [G loss: 1.341340]\n",
      "878 [D loss: 0.004637, acc.: 100.00%] [G loss: 1.375164]\n",
      "879 [D loss: 0.004907, acc.: 100.00%] [G loss: 1.418127]\n",
      "880 [D loss: 0.004727, acc.: 100.00%] [G loss: 1.464967]\n",
      "881 [D loss: 0.004834, acc.: 100.00%] [G loss: 1.509828]\n",
      "882 [D loss: 0.004505, acc.: 100.00%] [G loss: 1.545273]\n",
      "883 [D loss: 0.004540, acc.: 100.00%] [G loss: 1.559448]\n",
      "884 [D loss: 0.004384, acc.: 100.00%] [G loss: 1.558810]\n",
      "885 [D loss: 0.004665, acc.: 100.00%] [G loss: 1.557967]\n",
      "886 [D loss: 0.004468, acc.: 100.00%] [G loss: 1.562344]\n",
      "887 [D loss: 0.004657, acc.: 100.00%] [G loss: 1.569088]\n",
      "888 [D loss: 0.004088, acc.: 100.00%] [G loss: 1.590155]\n",
      "889 [D loss: 0.004698, acc.: 100.00%] [G loss: 1.632544]\n",
      "890 [D loss: 0.004393, acc.: 100.00%] [G loss: 1.675312]\n",
      "891 [D loss: 0.004678, acc.: 99.99%] [G loss: 1.721993]\n",
      "892 [D loss: 0.004278, acc.: 100.00%] [G loss: 1.764533]\n",
      "893 [D loss: 0.004594, acc.: 100.00%] [G loss: 1.809814]\n",
      "894 [D loss: 0.004573, acc.: 100.00%] [G loss: 1.856740]\n",
      "895 [D loss: 0.004225, acc.: 100.00%] [G loss: 1.895085]\n",
      "896 [D loss: 0.004465, acc.: 100.00%] [G loss: 1.934283]\n",
      "897 [D loss: 0.004876, acc.: 100.00%] [G loss: 1.959357]\n",
      "898 [D loss: 0.004272, acc.: 100.00%] [G loss: 1.975488]\n",
      "899 [D loss: 0.004324, acc.: 100.00%] [G loss: 2.000062]\n",
      "900 [D loss: 0.004500, acc.: 100.00%] [G loss: 2.011336]\n",
      "901 [D loss: 0.004402, acc.: 100.00%] [G loss: 2.033597]\n",
      "902 [D loss: 0.004565, acc.: 100.00%] [G loss: 2.049815]\n",
      "903 [D loss: 0.004422, acc.: 100.00%] [G loss: 2.055463]\n",
      "904 [D loss: 0.004157, acc.: 100.00%] [G loss: 2.066411]\n",
      "905 [D loss: 0.004342, acc.: 99.99%] [G loss: 2.070156]\n",
      "906 [D loss: 0.004130, acc.: 100.00%] [G loss: 2.076795]\n",
      "907 [D loss: 0.004445, acc.: 100.00%] [G loss: 2.086498]\n",
      "908 [D loss: 0.004180, acc.: 100.00%] [G loss: 2.093636]\n",
      "909 [D loss: 0.004294, acc.: 100.00%] [G loss: 2.100996]\n",
      "910 [D loss: 0.004010, acc.: 100.00%] [G loss: 2.110437]\n",
      "911 [D loss: 0.004218, acc.: 100.00%] [G loss: 2.102584]\n",
      "912 [D loss: 0.004218, acc.: 100.00%] [G loss: 2.103252]\n",
      "913 [D loss: 0.004001, acc.: 100.00%] [G loss: 2.099027]\n",
      "914 [D loss: 0.004021, acc.: 100.00%] [G loss: 2.100776]\n",
      "915 [D loss: 0.004290, acc.: 100.00%] [G loss: 2.105184]\n",
      "916 [D loss: 0.003963, acc.: 100.00%] [G loss: 2.103252]\n",
      "917 [D loss: 0.004034, acc.: 100.00%] [G loss: 2.102344]\n",
      "918 [D loss: 0.004260, acc.: 100.00%] [G loss: 2.110224]\n",
      "919 [D loss: 0.003948, acc.: 100.00%] [G loss: 2.125194]\n",
      "920 [D loss: 0.004156, acc.: 100.00%] [G loss: 2.136327]\n",
      "921 [D loss: 0.003907, acc.: 100.00%] [G loss: 2.167289]\n",
      "922 [D loss: 0.004115, acc.: 100.00%] [G loss: 2.190562]\n",
      "923 [D loss: 0.003820, acc.: 100.00%] [G loss: 2.221337]\n",
      "924 [D loss: 0.004309, acc.: 100.00%] [G loss: 2.193234]\n",
      "925 [D loss: 0.003863, acc.: 100.00%] [G loss: 2.183698]\n",
      "926 [D loss: 0.003920, acc.: 100.00%] [G loss: 2.202204]\n",
      "927 [D loss: 0.004009, acc.: 100.00%] [G loss: 2.224137]\n",
      "928 [D loss: 0.004037, acc.: 100.00%] [G loss: 2.241482]\n",
      "929 [D loss: 0.004066, acc.: 100.00%] [G loss: 2.278483]\n",
      "930 [D loss: 0.004112, acc.: 100.00%] [G loss: 2.321367]\n",
      "931 [D loss: 0.003975, acc.: 100.00%] [G loss: 2.380875]\n",
      "932 [D loss: 0.004054, acc.: 100.00%] [G loss: 2.434388]\n",
      "933 [D loss: 0.003782, acc.: 100.00%] [G loss: 2.461533]\n",
      "934 [D loss: 0.003659, acc.: 100.00%] [G loss: 2.471042]\n",
      "935 [D loss: 0.004029, acc.: 100.00%] [G loss: 2.461097]\n",
      "936 [D loss: 0.004300, acc.: 100.00%] [G loss: 2.451417]\n",
      "937 [D loss: 0.003672, acc.: 100.00%] [G loss: 2.443419]\n",
      "938 [D loss: 0.003761, acc.: 100.00%] [G loss: 2.431723]\n",
      "939 [D loss: 0.003785, acc.: 100.00%] [G loss: 2.433062]\n",
      "940 [D loss: 0.003718, acc.: 100.00%] [G loss: 2.437984]\n",
      "941 [D loss: 0.003844, acc.: 100.00%] [G loss: 2.456548]\n",
      "942 [D loss: 0.003699, acc.: 100.00%] [G loss: 2.468926]\n",
      "943 [D loss: 0.003662, acc.: 100.00%] [G loss: 2.491516]\n",
      "944 [D loss: 0.003937, acc.: 100.00%] [G loss: 2.500534]\n",
      "945 [D loss: 0.003739, acc.: 100.00%] [G loss: 2.496227]\n",
      "946 [D loss: 0.003552, acc.: 100.00%] [G loss: 2.498969]\n",
      "947 [D loss: 0.003711, acc.: 100.00%] [G loss: 2.502595]\n",
      "948 [D loss: 0.003548, acc.: 100.00%] [G loss: 2.509985]\n",
      "949 [D loss: 0.003534, acc.: 100.00%] [G loss: 2.511895]\n",
      "950 [D loss: 0.003589, acc.: 100.00%] [G loss: 2.516915]\n",
      "951 [D loss: 0.003483, acc.: 100.00%] [G loss: 2.523792]\n",
      "952 [D loss: 0.003501, acc.: 100.00%] [G loss: 2.536391]\n",
      "953 [D loss: 0.003545, acc.: 100.00%] [G loss: 2.569152]\n",
      "954 [D loss: 0.003683, acc.: 100.00%] [G loss: 2.578950]\n",
      "955 [D loss: 0.003701, acc.: 100.00%] [G loss: 2.588028]\n",
      "956 [D loss: 0.003390, acc.: 100.00%] [G loss: 2.607692]\n",
      "957 [D loss: 0.003482, acc.: 100.00%] [G loss: 2.626244]\n",
      "958 [D loss: 0.003749, acc.: 100.00%] [G loss: 2.640751]\n",
      "959 [D loss: 0.003389, acc.: 100.00%] [G loss: 2.661749]\n",
      "960 [D loss: 0.003579, acc.: 100.00%] [G loss: 2.692563]\n",
      "961 [D loss: 0.003377, acc.: 100.00%] [G loss: 2.721127]\n",
      "962 [D loss: 0.003605, acc.: 100.00%] [G loss: 2.741785]\n",
      "963 [D loss: 0.003654, acc.: 100.00%] [G loss: 2.757740]\n",
      "964 [D loss: 0.003479, acc.: 100.00%] [G loss: 2.772628]\n",
      "965 [D loss: 0.003507, acc.: 100.00%] [G loss: 2.780318]\n",
      "966 [D loss: 0.003391, acc.: 100.00%] [G loss: 2.799102]\n",
      "967 [D loss: 0.003410, acc.: 100.00%] [G loss: 2.817041]\n",
      "968 [D loss: 0.003171, acc.: 100.00%] [G loss: 2.845991]\n",
      "969 [D loss: 0.003420, acc.: 100.00%] [G loss: 2.875208]\n",
      "970 [D loss: 0.003524, acc.: 100.00%] [G loss: 2.910913]\n",
      "971 [D loss: 0.003433, acc.: 100.00%] [G loss: 2.948881]\n",
      "972 [D loss: 0.003472, acc.: 100.00%] [G loss: 2.955097]\n",
      "973 [D loss: 0.003313, acc.: 100.00%] [G loss: 2.974873]\n",
      "974 [D loss: 0.003439, acc.: 100.00%] [G loss: 2.984342]\n",
      "975 [D loss: 0.003492, acc.: 100.00%] [G loss: 2.994795]\n",
      "976 [D loss: 0.003346, acc.: 100.00%] [G loss: 3.007026]\n",
      "977 [D loss: 0.003267, acc.: 100.00%] [G loss: 3.023167]\n",
      "978 [D loss: 0.003566, acc.: 100.00%] [G loss: 3.041423]\n",
      "979 [D loss: 0.003308, acc.: 100.00%] [G loss: 3.050119]\n",
      "980 [D loss: 0.003463, acc.: 100.00%] [G loss: 3.067295]\n",
      "981 [D loss: 0.003335, acc.: 100.00%] [G loss: 3.087780]\n",
      "982 [D loss: 0.003261, acc.: 100.00%] [G loss: 3.100717]\n",
      "983 [D loss: 0.003555, acc.: 100.00%] [G loss: 3.120179]\n",
      "984 [D loss: 0.003236, acc.: 100.00%] [G loss: 3.135973]\n",
      "985 [D loss: 0.003255, acc.: 100.00%] [G loss: 3.147346]\n",
      "986 [D loss: 0.003238, acc.: 100.00%] [G loss: 3.154716]\n",
      "987 [D loss: 0.003439, acc.: 100.00%] [G loss: 3.169657]\n",
      "988 [D loss: 0.003497, acc.: 100.00%] [G loss: 3.190603]\n",
      "989 [D loss: 0.003345, acc.: 100.00%] [G loss: 3.211891]\n",
      "990 [D loss: 0.003293, acc.: 100.00%] [G loss: 3.232907]\n",
      "991 [D loss: 0.003280, acc.: 100.00%] [G loss: 3.252827]\n",
      "992 [D loss: 0.003278, acc.: 100.00%] [G loss: 3.258981]\n",
      "993 [D loss: 0.003201, acc.: 100.00%] [G loss: 3.274207]\n",
      "994 [D loss: 0.003208, acc.: 100.00%] [G loss: 3.290216]\n",
      "995 [D loss: 0.003373, acc.: 100.00%] [G loss: 3.303295]\n",
      "996 [D loss: 0.003251, acc.: 100.00%] [G loss: 3.310743]\n",
      "997 [D loss: 0.003377, acc.: 100.00%] [G loss: 3.326675]\n",
      "998 [D loss: 0.003348, acc.: 100.00%] [G loss: 3.339342]\n",
      "999 [D loss: 0.003375, acc.: 100.00%] [G loss: 3.377728]\n",
      "1000 [D loss: 0.003302, acc.: 100.00%] [G loss: 3.428512]\n"
     ]
    }
   ],
   "source": [
    "z_dim = 100\n",
    "\n",
    "discriminator, generator, gan = build_compile(data_word, z_dim)\n",
    "\n",
    "iterations =1000\n",
    "batch_size = 128\n",
    "sample_interval = 1000\n",
    "\n",
    "generator = train(data_word,labels_word,iterations,batch_size,sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.random.normal(0,1,(1, 100))\n",
    "vec = generator.predict(z)\n",
    "vec = vec[0].reshape(1,400)\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.112584877604407"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ ,  _, data_1, _ = x_y_train(data_word,labels_word)\n",
    "data_1.shape\n",
    "result = 0\n",
    "for i in range(7303):\n",
    "    if result < cos_sim(vec[0],data_1[i]):\n",
    "        result = cos_sim(vec[0],data_1[i])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
