{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.layers import Activation, BatchNormalization, Concatenate, Dense, Embedding, Flatten, Reshape, Input, Multiply,Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import MeCab\n",
    "import json\n",
    "import hashlib\n",
    "from googletrans import Translator\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import torch\n",
    "from transformers.modeling_bert import BertModel\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "mt = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "mt.parse('')\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "model_bert = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_cls(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt') \n",
    "    result = model_bert(input_ids)\n",
    "    tensor_result = result[0][0][0]\n",
    "    numpy_result = tensor_result.to('cpu').detach().numpy().copy()\n",
    "    return numpy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event:\n",
    "    def __init__(self, id, type, score, desc, links):\n",
    "        self.id = id\n",
    "        self.type = type\n",
    "        self.score = score\n",
    "        self.desc = desc\n",
    "        self.links = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON ファイルから event set をロード\n",
    "def load_events(jsonfile):\n",
    "    with open(jsonfile) as f:\n",
    "        df = json.load(f)\n",
    "    events = {x['id']: Event(x['id'], x['type'], x['score'], x['desc'], x['links']) for x in df} #eventsにidをkeyとしそのオブジェクトをvalueとした辞書を生成\n",
    "    for k,x in events.items():\n",
    "        x.links = [events[e] for e in x.links] #Event.linkの中身をidの配列からEventの配列に変更\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = load_events('sesaku2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "labels = []\n",
    "columns=[]\n",
    "index=[]\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='部品':\n",
    "        index.append(v1.desc)\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='対策':\n",
    "        if not v1.desc in columns:\n",
    "            columns.append(v1.desc)\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='部品':\n",
    "        for k2, v2 in events.items():\n",
    "            if v2.type[-1] == '対策':\n",
    "                    if v2 in v1.links:\n",
    "                        df.at[v1.desc, v2.desc] = 1\n",
    "                    else:\n",
    "                        df.at[v1.desc, v2.desc] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 319)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n",
      "さ\n"
     ]
    }
   ],
   "source": [
    "taisaku_vec_word = {}\n",
    "taisaku_count ={}\n",
    "for i in df:\n",
    "    taisaku_vec_word[i]=get_vector_cls(i)\n",
    "    taisaku_count[i] = 0\n",
    "\n",
    "#label →部品　生成→対策\n",
    "data = []\n",
    "buhin = []\n",
    "buhin2 = []\n",
    "taisaku = []\n",
    "buhin_dict = {}\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    print('さ')\n",
    "    x1 = get_vector_cls(index)\n",
    "    buhin.append(x1)\n",
    "    buhin_dict[index] = tuple(x1)\n",
    "    for i in df:#対策\n",
    "        x2 =  taisaku_vec_word[i]#対策\n",
    "        if row[i] ==1:\n",
    "            data.extend([np.append(x1, x2)])\n",
    "            buhin2.append(x1)\n",
    "            taisaku_count[i] += 1\n",
    "            \n",
    "for k,v in taisaku_vec_word.items():\n",
    "    taisaku.append(v)\n",
    "data = np.array(data)\n",
    "buhin = np.array(buhin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7303, 1536)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#関係がある部品と対策の文書ベクトル\n",
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "buhim = []\n",
    "for i in data:\n",
    "    buhim.append(i[:768])\n",
    "buhim  = np.array(buhim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7303, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buhim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#部品の文書ベクトルだけ\n",
    "buhin = np.array(buhin)\n",
    "buhin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7303, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buhin2 = np.array(buhin2)\n",
    "buhin2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.473694"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buhin.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#対策の文書ベクトルだけ\n",
    "taisaku = np.array(taisaku)\n",
    "taisaku.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('多能工化を設計者に適用。メカ・電気・ソフトの各スキルマップを作成し統合。', 152),\n",
       " ('定性的なアウトプットである個人の振り返りを実施。個人の成長・マネジメント軸を強化する。', 152),\n",
       " ('６ヶ月に一回のサイクルで実施。', 152),\n",
       " ('会社目標から製品、各機能組織ごとの、目標展開を実施。メンバー目標とも連携している', 152),\n",
       " ('背景・目的・目標・KGI/KPI・大日程、現状の課題までが繋がっている', 152),\n",
       " ('関連部門が参加した課題解決の仕組みがあり、PLMを使ったデジタル化によるグローバル会議ができる', 152),\n",
       " ('機能と役割分担表があり、リーダとサブリーダが決まっている。', 152),\n",
       " ('PLMを使ったグローバルリアルタイムコラボレーション会議を実施。', 152),\n",
       " ('5M (人・道具・原料・方法・測定・環境)の基準で変化点を定義し、最新の状態を関係者で共有している', 152),\n",
       " ('5M変動時の作業指導、品質確認等ができている', 152),\n",
       " ('製造部門だけでなく開発・設計などホワイトカラー業務にも適用できれいる。', 152),\n",
       " ('モジュール化の定義、進め方が決められており、原価まで繋がった評価ができる', 152),\n",
       " ('コンフィグレータが、製品構成、評価、生産まで考慮した仕様になっている。', 152),\n",
       " ('KGI/KPIに標準化の指標がある', 152),\n",
       " ('製品開発ゲートの定義と、フェーズごとのアプトプットが明確になっている', 152),\n",
       " ('先行開発ゲートの定義は、通常開発の定義と異なっており、技術の棚という考え方がある', 152),\n",
       " ('過去の作業マニュアルが適切に更新され、処分されている', 152),\n",
       " ('製品・部品の品質管理が定常的に実施されており、管理図などによりモニタリングしている', 152),\n",
       " ('デザインレビューのパフォーマンスを測定できる指標がある。', 152),\n",
       " ('グローバルに関連部門が一元化した製品情報を使って実施できる。', 152),\n",
       " ('DR結果を一元化したE-BOMに蓄積して、再利用できる。', 152),\n",
       " ('施策は、複数の候補を用意し、各効果を定量的に測れる、', 152),\n",
       " ('工程設計をの５点満点評価スキルマップを作成し、育成計画との連動させる。', 152),\n",
       " ('職務拡充により作業者のモチベーション向上。品質を作りこむ意識が醸成。', 152),\n",
       " ('チェックリスト（発生防止）とシート（流出）を使い分けている。', 152),\n",
       " ('CE、リーダからメンバーまで全員で確認できるしくみがある。', 152),\n",
       " ('目的、仕様確認をスムーズにするためにカテゴリー化できている。', 152),\n",
       " ('原価情報（試作・量産別）の目標と現状が確認できる', 152),\n",
       " ('過去の不具合に対し、今後の施策をまとめてあり新製品企画の段階で適用している。', 152),\n",
       " ('品質保証部がリードし、関連部門を巻き込んで内容がアップデートされている。', 152),\n",
       " ('プロジェクトの目標を明らかにし、アラインした原価目標を設定。手直し・仕損じ・固定費の配分などが入っている。', 152),\n",
       " ('削減目標を＞３０％といった値にし、組織・製品を横段した施策を設定。', 152),\n",
       " ('原価企画・原価計画・原価維持・原価改善といった視点で、原価管理を実施している。', 152),\n",
       " ('原価情報が取れない場合の施策があり、原価企画チームで一元化されている。', 152),\n",
       " ('ROI見積が、InputとOutputを決める定義があり、プロフィットが出る時期も予測できる。', 152),\n",
       " ('E-BOMと連携した原価情報があり、企画時に効率よく原価情報が入手できる。', 152),\n",
       " ('締め付けトルクの保証されている', 152),\n",
       " ('過去の作業マニュアルが適切に更新され、設計変更があった場合、ＢＯＭ連携で自動更新', 152),\n",
       " ('エンジン性能の検査（トヨタ）', 152),\n",
       " ('前機種の振り返りを実施。目標に対する達成度、課題と次の一手を適用している', 21),\n",
       " ('QCDの視点で、目標を設定。関連部品にも目標展開、仕様を決めた後、全体目標をアラインしている。', 21),\n",
       " ('目標を元にKGI/KPIを設定している。', 21),\n",
       " ('原価企画が全社的に認知されており、全社損益とリンク', 21),\n",
       " ('原価目標の設定は原価企画を適用し、施策と削減余地までを見積る。', 21),\n",
       " ('部品別、ライン別、事業別に原価を把握', 21),\n",
       " ('社内コストテーブルがあり、関連部門と連携しながら原価算出', 21),\n",
       " ('原価企画部門があり、システムも構築', 21),\n",
       " ('絶対値・差額見積りの両方を実施', 21),\n",
       " ('プロジェクト毎の開発費が設定され、目標管理されている', 21),\n",
       " ('設備投資企画を実施、全社損益とリンクしている', 21),\n",
       " ('設備投資企画のスケジュールがあり、投資（償却費）と工数の相関を確認', 21),\n",
       " ('予算管理が全社損益とリンク', 21),\n",
       " ('ベンチマーキングを実施、コストDBに蓄積', 21),\n",
       " ('VE/VAを実施。低減のためのアイデアの棚がある', 21),\n",
       " ('量産開始安定後の実績原価を把握', 21),\n",
       " ('原価を維持・改善する活動を実施', 21),\n",
       " ('個別原価が会社損益とリンク', 21),\n",
       " ('調達に関し管理部門とライン部門がある', 21),\n",
       " ('調達の必要工数につき全て査定基準がある', 21),\n",
       " ('工法（プレス、樹脂等）のコストテーブルがある', 21),\n",
       " ('現地会社の現調率が各国の規制値をクリヤー', 21),\n",
       " ('メンテナンスコストやクレーム対応コストを加味した原価設計、歩留まり率設計を実施している', 21),\n",
       " ('法規要求、コラム、ペダルストロークなどの基準を満足するシートHP決定する', 21),\n",
       " ('CAE解析データを作成し衝突ストーリーを作成する', 21),\n",
       " ('8の字旋回実働ひずみ測定', 21),\n",
       " ('ベンチεーM線図測定', 21),\n",
       " ('ベンチM－N線図測定', 21),\n",
       " ('ハブボルトスリップトルク測定', 21),\n",
       " ('Ｌスクリーニング(耐久テスト)', 21),\n",
       " ('台上耐久', 21),\n",
       " ('塩害テスト', 21),\n",
       " ('着氷テスト', 21),\n",
       " ('実車異音評価', 21),\n",
       " ('車両組付性', 21),\n",
       " ('車両、環境、周辺部品の変化点、変更点を確認し実施有無判断', 21),\n",
       " ('要求管理・製品コンフィグレータをCADと連携している', 21),\n",
       " ('システムズ・エンジニアリングを導入してモデルベース開発を実施', 21),\n",
       " ('モデルベース開発を実施、１Dのシミュレーションで機能評価を行っている', 21),\n",
       " ('E-BOMに原価情報を入れ、モデルベース開発を使ったVEを実施', 21),\n",
       " ('関連技術情報をCADとマクロで連携させている', 21),\n",
       " ('AI/機械学習と製品３Dデータを連携し自動設計を実現している', 21),\n",
       " ('解析と３Dデータを連携し評価している', 21),\n",
       " ('OM/OIといったアプリケーションとのデータ連携で最適化設計を実施', 21),\n",
       " ('標準部品のライブラリがあり、設計時に自動連携している', 21),\n",
       " ('E-BOM・P-BOM・M-BOMを連携させて３D製品データ変換を自動化', 21),\n",
       " ('データベースを使ったBOMのグローバル一元管理', 21),\n",
       " ('S-BOMを使ったサービス部品の管理', 21),\n",
       " ('マルチOEM管理を実施、各BOMをクラウドを使って実施できる。', 21),\n",
       " ('PLMのプロジェクト管理・ガバナンス・グローバルコラボレーション・３Dデータを連携ができる', 21),\n",
       " ('DRをPLMとCADを連携して、一元化した製品DBを共有しながら実施できる。', 21),\n",
       " ('課題管理をPLMを使って実施', 21),\n",
       " ('PLM上のBIで、原価目標と実績を見える化している。', 21),\n",
       " ('製品・プロセス・リソースを同時に管理できるCAMを使用', 21),\n",
       " ('工場のレイアウトを物と情報の流れを含めてシミュレーションできる', 21),\n",
       " ('BOP・BOEを使った生産工程設計をCAM上で実施', 21),\n",
       " ('現場情報をミリ秒で測定できるIoTのしくみがあり、エッジを通してMESに取り込んでいる。', 21),\n",
       " ('ERPと連携しており、生産計画を在庫も管理しながらシフトごとに指示できる。', 21),\n",
       " ('ERPと連携して、生産に合わせた部品の受発注が自動できる。', 21),\n",
       " ('作業者のスキルに合わせた生産計画を組め、計画と実績を管理している。', 21),\n",
       " ('ライン切り替え時の段取り作業の標準化、必要時間をMESの生産計画に組み込んでいる', 21),\n",
       " ('ライン切り替え時にNCマシンの数値設定が正しく変更されているか確認している', 21),\n",
       " ('製品知識を５点満点評価スキルマップを作成し、育成計画との連動させる。', 21),\n",
       " ('ライン切り替え時の段取り作業の標準化、低減目標を設定し、年間活動計画で実施している', 21),\n",
       " ('メインインインタミシャフト圧縮、ストロークの確認。', 5),\n",
       " ('意匠変更によるATレバーと足踏みパーキングブレーキ（ひざ当たり量の調整）', 5),\n",
       " ('ATシフトケーブルのクランプ最適化。PCU座面設定。', 5),\n",
       " ('３、コラムA/T要否Zに確認、要求によりA/T搭載検討を実施する', 4),\n",
       " ('インジェクタの作動音低減対策を実施しているか\\u3000（Ｏリング積層化）', 1),\n",
       " ('ポート噴射インジェクタの燃料微細化を実施しているか', 1),\n",
       " ('直噴インジェクタのノズル噴孔に対して、温度上昇抑制はあるか', 1),\n",
       " ('ノズルニードルとボディのあたり面形状は図面指示しているか', 1),\n",
       " ('直噴インジェクタプランジャ摺動面の耐焼き付き性は対策は良いか\\u3000（ＰＴＦＥ）', 1),\n",
       " ('バルブ穴のバリ規格はあるか', 1),\n",
       " ('インジェクターの色識別（品番との合致）', 1),\n",
       " ('噴射レベルの抜き取り検査', 1),\n",
       " ('インジェクターをエンジンに挿入する時の抵抗測定をする', 1),\n",
       " ('バリ、異物有無を検査している（画像検査等）', 1),\n",
       " ('機能検査を実施している（電流ｘ噴射量）', 1),\n",
       " ('リーク検査を実施しているる', 1),\n",
       " ('会社方針と中・長期計画を基に、商品企画と製品企画を経て、目標を設定している。', 0),\n",
       " ('社会との調和、社会への貢献実現のビジョンがある', 0),\n",
       " ('経営責任を明確化', 0),\n",
       " ('的確な経営判断が９０％の迅速さで下されている', 0),\n",
       " ('社長を補佐できるキーパーソンが各部門に存在', 0),\n",
       " ('方針管理は部門別と機能別を組み合わせたマトリックスマネジメントで管理', 0),\n",
       " ('日常管理が各機能ごとに項目と目標が明確で、管理責任者も設定', 0),\n",
       " ('管理目標から外れた場合のアクションが取られている', 0),\n",
       " ('研究開発の組織があり、売上当たりの比率が５％以上', 0),\n",
       " ('会社の経営ビジョン達成のための中・長期の技術開発戦略と目標・計画がある', 0),\n",
       " ('SE活動を実施、仕入れ先を含んだ関連部門が参加し運営要領がある', 0),\n",
       " ('各ゲートでDRを実施。それとは別で品質保証会議を実施し不具合の流出を防止', 0),\n",
       " ('FMEAを用いた不具合の検知や対策による未然防止を実施', 0),\n",
       " ('ベンチマークの体制・主管部署がある', 0),\n",
       " ('ベンチマークを計画的に実施、結果を開発に反映させる仕組みがある', 0),\n",
       " ('モデルベース開発を実施。モジュール化もシステム上で要件検証できる', 0),\n",
       " ('全社で一元化されたE-BOMを用いたCADを使用', 0),\n",
       " ('設計ノウハウを蓄積する体系・主管部署がある', 0),\n",
       " ('市場・後工程不具合事例集・べからず集がある', 0),\n",
       " ('新技術、新加工法などのDBがある', 0),\n",
       " ('該当する分野のベスト技術を蓄積する技術の棚がある', 0),\n",
       " ('技術情報システム（品番・材質・重量・工程・仕入先）の体系、主管部署がある', 0),\n",
       " ('一元化されたEBOM/PBOM/MBOMがありデータ連携', 0),\n",
       " ('実機評価はシステムレベルを含め自社で可能', 0),\n",
       " ('FEM, CAEを活用し評価結果の解析を実施。普及率８０％以上', 0),\n",
       " ('市場不具合を回収、分析、設計にフードバックする体制と組織がある', 0),\n",
       " ('市場良品回収を実施し、不具合予測を行っている', 0),\n",
       " ('評価方法は定期的に見直され、市場との整合性が維持', 0),\n",
       " ('自社の技術標準体系があり、管理体制と主管部署がある', 0),\n",
       " ('特許出願件数が過去3ヶ月連続増加', 0),\n",
       " ('過去3年以内に世界No1の技術が複数ある', 0),\n",
       " ('新製品・新技術の展示会を客先で定期的に実施', 0),\n",
       " ('技術部門（スタッフ、オペレータ）の教育・訓練体系がある', 0),\n",
       " ('業務毎にスタッフ・オペレータ能力の到達レベルが決まっている', 0),\n",
       " ('１、ステアリングコラムの衝突安全性を満足するプロセスを作る', 0),\n",
       " ('インパネ、コラムの固有振動数解析し最適サポート位置を決定する', 0),\n",
       " ('ステアリングヨークを選定後トルク変動計算し基準値以内にする', 0),\n",
       " ('カップリング、ヨーク部品の共通化を行う', 0),\n",
       " ('ホールカバー、カップリングのスペースを確保し組付性を工具も考慮し確認する', 0),\n",
       " ('２、PSギヤラックサイズ、シリンダサイズから搭載位置決定', 0),\n",
       " ('ラックストロークやサスペンションの要求を満たすＳ点を決定する', 0),\n",
       " ('コラム搭載から要求されるR&Pピニオン位置決定', 0),\n",
       " ('製品の開発・設計の段階から生産部門のスタッフが参画しており、開発から市場投入までのリードタイムを短縮している', 0),\n",
       " ('新工法（開発力）保有工法が5分野以上あり、過去5年で3分野以上の新工程で適用している', 0),\n",
       " ('開発～製造までの全社業務フロー図があり、連携を取りながら機能している', 0),\n",
       " ('内製工機部門があり、型・治具設計者が揃っている。生技と連携が取れるしくみがある', 0),\n",
       " ('再発防止提案書（不具合事例を含む）があり、次の開発に活かす仕組みがある', 0),\n",
       " ('SE活動体系図が整っており、実施タイミング・目的・実施内容が明記されている', 0),\n",
       " ('プロジェクトの開発～生準～立上までの全社的な活動を表す日程表がある', 0),\n",
       " ('設備メーカの能力評価基準があり、QCDで評価', 0),\n",
       " ('設備計画書があり、工程計画書の設備要件が盛り込まれている', 0),\n",
       " ('工程計画書、投資効果計算書、品質標準書、作り込み指示書あり機能している', 0),\n",
       " ('試作納期遵守率、実力LTと標準LT、客先要求精度の合格率、内製率のKPI', 0),\n",
       " ('試作費低減計画書がある', 0),\n",
       " ('金型・治具の原価企画を実施。目標を設定して低減活動を実施', 0),\n",
       " ('生準計画情報（工程計画、生準日程、品質標準、生準計画）が全社に開示できるシステムがある', 0),\n",
       " ('CADデータ受入不具合率を測定。連携課題が明確で対応策を実施', 0),\n",
       " ('限度見本を設定するルールがあり、限度見本が現場に掲示されている', 0),\n",
       " ('生準業務マニュアルが整備。型・設備・治具・製作・整備技能が標準化されている', 0),\n",
       " ('業務マニュアル、標準作業書を管理する専任部署がある', 0),\n",
       " ('チューニング仕様。バネ・アブソーバの絞り込み（フロントバウンドストッパー）', 0),\n",
       " ('ストラットの共通化。効果を明確に。', 0),\n",
       " ('ロアアーム、ケージダウン検討', 0),\n",
       " ('ロアアームブッシュの特性検討', 0),\n",
       " ('ナックルの強度（疲労、締結）を確認', 0),\n",
       " ('サスペンションのチューニング。フロントバウンズストッパ（ゴム＆ウレタンの高さとクリアランス確認', 0),\n",
       " ('ロアアームブラケット、ジョイント、ソケット部の左右共通化を確認', 0),\n",
       " ('ジオメトリ・性能の統合システム構成検討', 0),\n",
       " ('ジオメトリ・性能。駆動方式、操安、制動、要求快適性の確認と限界余地の設定', 0),\n",
       " ('車両増備マトリクスの確認', 0),\n",
       " ('アブソーバ仕様書作成', 0),\n",
       " ('意匠変更による検討項目のリストアップ', 0),\n",
       " ('仕向地、コンセプト、走行MODEの設定', 0),\n",
       " ('組立性、メンテ性の課題項目出し', 0),\n",
       " ('操舵レイアウトのディメンジョン設定', 0),\n",
       " ('油圧ラインレイアウト', 0),\n",
       " ('車両姿勢ﾊﾞﾝﾌﾟ/ﾘﾊﾞﾝﾌﾟ（1G～全屈）の確認', 0),\n",
       " ('センサーの取り付け位置検討', 0),\n",
       " ('電子制御システム。アクティブコントロールサスからCPUの連携仕様設計', 0),\n",
       " ('アッパーサポートの取り付けレイアウト設計', 0),\n",
       " ('アッパーサポートのスラット周り検討。強度と剛性。', 0),\n",
       " ('コイルのバネ定数、線径、巻き数、ストローク検討', 0),\n",
       " ('コイルの応答解析確認', 0),\n",
       " ('アブソーバの減衰力設定。制御システムと構成図。', 0),\n",
       " ('アブソーバの4輪トータルバランス', 0),\n",
       " ('R&P、ラック・アンド・ピニオンのタイプ選定。仮S点の算出。', 0),\n",
       " ('スタビライザーのタイプ選定。動的挙動干渉チェック。', 0),\n",
       " ('ブレーキの制動力（操作力、熱量）', 0),\n",
       " ('ブレーキの構成検討（ロータ、パッド設定、シール剛性）', 0),\n",
       " ('ブレーキのライン、センサー、制御システム仕様', 0),\n",
       " ('ナックルの支持レイアウト。共通化、断面構造検討。', 0),\n",
       " ('ハブベアリングの基本ディメンジョン設定。冷却効率化検討。物性情報（鋳物靭性）', 0),\n",
       " ('タイヤ・ホイールのバネ化重量設定。バリエーション設定。包落面作成', 0),\n",
       " ('ATシフトレバーアセンブリ。シフトケーブル長の確認（モータ地上高）。コラムカバーデザイン確認。', 0),\n",
       " ('クランプ最適化検討（ダッシュ中央～下部）', 0),\n",
       " ('フロントブレーキホース配策検討（DSのABAQUS検討）', 0),\n",
       " ('リヤブレーキホース配策（ボディ側ブラケット、DS ABAQUS検討）', 0),\n",
       " ('床下ブレーキチューブのクロスメンバー部配策、クランプの形状検討', 0),\n",
       " ('床下ブレーキチューブのバッテリーカバーとのクランプ成立の検討', 0),\n",
       " ('マスターシリンダー変更あるかどうかで設変', 0),\n",
       " ('マスターシリンダーのリザータンクのが新設計かどうかで変更', 0),\n",
       " ('足踏みパーキングのセンターフラーとの仕様', 0),\n",
       " ('レイアウトでATのコンプレッサーとステアリングの干渉注意', 0),\n",
       " ('ブレーキブースターと補助バッテリーの干渉確認', 0),\n",
       " ('ブレーキ配管、ワイヤーハーネス、シフトケーブルの干渉確認', 0),\n",
       " ('ブレーキ要求仕様。効き・フィーリング減速感。', 0),\n",
       " ('回生ブレーキシステムと油圧センサーの開発（応答性・フィーリング）', 0),\n",
       " ('油圧センサーの搭載。回転トルク制御マップからのブレーキパッド試作。', 0),\n",
       " ('ブレーキの効き（ブースター搭載検討、パッド材質検討）', 0),\n",
       " ('ブレーキの鳴き（キャリパー選定、パッド材質）の検討', 0),\n",
       " ('法規と燃料モードの考え方の統一化', 0),\n",
       " ('足踏みパーキングとフロア間の設計のVE提案', 0),\n",
       " ('足踏みパーキングとPバルブの軽量化検討', 0),\n",
       " ('緒元（ギヤ比、減速比、ラックストローク）', 0),\n",
       " ('最新製品を使う場合、搭載性と干渉の寸法確認', 0),\n",
       " ('内製EMPSギヤアセンブリの搭載を検討する', 0),\n",
       " ('左ハンドル仕様のEMPSを試作', 0),\n",
       " ('PSギヤは、ラックサイズ決定、キングピン軸廻りのトルク推定後、シリンダサイズを。PSギヤの搭載位置決定される', 0),\n",
       " ('シリンダサイズは、シリンダ内必要油量を算出後、ラック推力を算出し、必要・容量を算出後に決定', 0),\n",
       " ('必要ラックストロークは、ロールアッカーマンよりS点決定後に算出。', 0),\n",
       " ('PS計画図は、ピニオン位置、ターンチューブ配策、PS配管レイアウト、PSギヤ・ポンプより決まる', 0),\n",
       " ('PSギヤ支持率は、操舵安定性部門と確認', 0),\n",
       " ('トレッド変更した場合の設変', 0),\n",
       " ('強度。ベンチマーク基準を比べ、目処付け、入力側の実績確認', 0),\n",
       " ('原価、重量の達成度を確認。', 0),\n",
       " ('ワッシャー類を他車と共通化', 0),\n",
       " ('乗り心地、NVを先行結果と比較', 0),\n",
       " ('チューニング仕様を確認', 0),\n",
       " ('バルブボディとノズルニードルのクリアランス設計基準を作成しているか', 0),\n",
       " ('直噴インジェクタのノズル噴孔に対して、耐デポジット対策を実施しているか\\u3000（フッ素シリコンコーティング）', 0),\n",
       " ('直噴インジェクタのＯリング耐久性向上を実施しているか\\u3000（フッ素ゴム\\u3000ＰＭＶＥ共重合）', 0),\n",
       " ('直噴インジェクタの浮き上がり防止、回転防止はあるか', 0),\n",
       " ('バルブボディ内のＯリング挿入部の形状は最適か\\u3000（面取り＋Ｒ）', 0),\n",
       " ('製品機能保障の為のテストサイクルは、指示しているか', 0),\n",
       " ('製品機能保障の為のテストサイクルは、環境条件を考慮しているか (燃料,温度,気圧 等)', 0),\n",
       " ('気筒間バラツキが要求値に入るような単品設計になっているか', 0),\n",
       " ('ノズルニードルとボディ間クリアランスが、要求値に入るような累積寸法公差、形状公差になっているか', 0),\n",
       " ('スプリングの耐久性は良いか\\u3000（１０\\u3000６乗以上か）', 0),\n",
       " ('バルブボディ組付（締付、カシメ等）の条件基準は指示しているか', 0),\n",
       " ('バルブボディー穴内の異物を規格化しているか', 0),\n",
       " ('ソフトウェア開発をモデルベースを使って実施している', 0),\n",
       " ('生産途中でも計画と実績を比較しながら、生産計画をアジャストできる。', 0),\n",
       " ('社内にTQM推進組織がある', 0),\n",
       " ('社内外でのQC教育制度があり積極的に参加', 0),\n",
       " ('QCサークル・SQC活動が行われ、年一回の発表会を開催', 0),\n",
       " ('改善提案制度があり、一人年一回以上の提案がある', 0),\n",
       " ('社内のノウハウを標準化し、固有ノウハウを蓄積して効率的に活用', 0),\n",
       " ('社内標準は体系的に整備、定期的な見直しがされている', 0),\n",
       " ('チックリストから抜粋したポイントになる個所をA4一枚の用紙にまとめたシートがある', 0),\n",
       " ('納期に対してオクレ進みが解る（量産・補給・試作）', 0),\n",
       " ('オクレのある場合、挽回計画とその根拠が解る', 0),\n",
       " ('トラックの積載効率を月毎に把握し、納入便数検討のしくみがある', 0),\n",
       " ('物流コストを毎月把握し低減活動を実施。施策も見える化も実施。', 0),\n",
       " ('品番単位で粗材～完成までのラインメイトとリードタイムを把握', 0),\n",
       " ('品名単位でものと情報の流れが見える化。リードタイム短縮施策があり実施。', 0),\n",
       " ('納期と生産計画の整合性が取れている', 0),\n",
       " ('現場の生産能力と生産計画が生産管理部と製造で話われている', 0),\n",
       " ('得意先のかんばんの変動に対し、ある程度対応できる仕組みがある', 0),\n",
       " ('ライン毎の必要数とサイクルタイムでのライン負荷を把握', 0),\n",
       " ('ライン負荷に基づく人員調整を生産会議で決めている', 0),\n",
       " ('必要人員と配置人員の関係を見える化', 0),\n",
       " ('５Sの意味と実施していることを全員が言える', 0),\n",
       " ('多能工化の計画を実施できる指導要領がある', 0),\n",
       " ('ライン稼働率をシフトごとで把握（停止時間だけでなく）', 0),\n",
       " ('不良率（廃棄・手直し）が把握され、手直しルールが守られている', 0),\n",
       " ('生産ライン・品目・作業者をシフトごとに把握。フレキシブルに対応できる', 0),\n",
       " ('アンドンを活用した異常対応が迅速にされ、再発防止などの改善活動に使われている', 0),\n",
       " ('ライン毎の仕掛指示（何をどれだけどの順番で）が明確である', 0),\n",
       " ('定性的・定量的な需要予測に基づいた生産計画を立案している', 0),\n",
       " ('自社のコアコンピタンスに基づき内外作判断をしている', 0),\n",
       " ('在庫コストと発注コストを把握し、最適な経済的発注量を設定している', 0),\n",
       " ('欠品を防止するために安全在庫を確保している', 0),\n",
       " ('各部品の生産計画の特徴に応じて、定量、定期、都度発注などの発注方式が定められている', 0),\n",
       " ('製品開発プロセスは、PLM上にテンプレートとして定義されている。', 0),\n",
       " ('仕掛品の工程進捗が見える化されており、トレーサビリティシステムが構築されている', 0),\n",
       " ('仕掛品・部品にタグ付けされている仕様書の形式が統一されている', 0),\n",
       " ('ボトルネック工程を重点的に管理し、各工程のピッチタイムが同期化されている', 0),\n",
       " ('製品ごとに適性なロットサイズが決められている', 0),\n",
       " ('AI/機械学習を使い現場で最優先の施策が提案されてくる。', 0),\n",
       " ('プレス・切削機など危険を伴う工程では、安全に関するＫＰＩが設定、ポカヨケ対策を実施', 0),\n",
       " ('完全なバリ、切粉除去装置がある 切粉、バリの洗浄規格が有る', 0),\n",
       " ('誤品使用不可のポカヨケがある', 0),\n",
       " ('欠品防止のポカヨケがある', 0),\n",
       " ('誤組付け防止のポカヨケがある', 0),\n",
       " ('かしめ状態が保証されている', 0),\n",
       " ('バルブボディー穴バリ取りの工程保証度を持たせているか', 0),\n",
       " ('ボディ組付け（締付、カシメ等）方法を、定量化・自動化しているか', 0),\n",
       " ('ボディ組付け時、Ｏリング切れを起こさない手順を標準化・自動化しているか', 0),\n",
       " ('BOEに工具と保管場所が決められており、BOPと連携することにより使用時に直ちに取り出せる', 0),\n",
       " ('ＭＥＳに仕掛品・中間部品在庫の保管場所が決められており、量が見える化されている', 0),\n",
       " ('外注先とＰＬＭを介して自社の生産スケジュールを共有している', 0),\n",
       " ('作業者の移動やモノの運搬距離が短縮化・簡素化。工場レイアウトもＣＡＥを使って検証。', 0),\n",
       " ('作業者の健康・集中力向上を狙った作業台設計をしている', 0),\n",
       " ('各生産ラインの製造周期率を把握しており、', 0),\n",
       " ('ミスを直ちに共有する職場風土が醸成されている', 0),\n",
       " ('５S教育が作業員に浸透しており、安全・快適な職場環境である', 0),\n",
       " ('加工寸法を全数検査している', 0),\n",
       " ('異常有無を全数検査している（外観・異音・振動等）', 0),\n",
       " ('プランジャ当たり面の検査は、定量化・仕組み化・標準化しているか', 0),\n",
       " ('機能テストは、設計指示の環境条件を満足しているか (燃料,温度,気圧 等)', 0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taisaku_map = sorted(taisaku_count.items(),key=lambda x:x[1], reverse=True)\n",
    "taisaku_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パラメータ\n",
    "vec_length = 768\n",
    "vec_shape =(vec_length,)\n",
    "z_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    " #生成器\n",
    "def build_generator(z_dim,vec_shape):\n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(Dense(128,input_dim =( z_dim+vec_shape[0])))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    " \n",
    "        model.add(Dense(768))\n",
    "        model.add(Activation('tanh'))\n",
    "        print('generator')\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#識別器\n",
    "def build_discriminator(vec_shape):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, input_dim=vec_shape[0]*2)) #activation\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print('discriminator')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generatorを学習する時のもの\n",
    "def build_combined(generator,discriminator):\n",
    "    z_y = Input(shape=(z_dim+vec_shape[0],)) #ランダムベクトル＋部品ベクトル\n",
    "    vec_1 = Input(shape=vec_shape)              #部品ベクトル\n",
    "    \n",
    "    #ベクトルを生成\n",
    "    gen_vec = generator(z_y)\n",
    "    \n",
    "    #discriminatorに入力するやつ\n",
    "    vec_2 = Concatenate()([vec_1,gen_vec])  #[部品ベクトル,対策ベクトル]\n",
    "    #discriminatorの重みとバイアスを固定\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    valid = discriminator(vec_2)\n",
    "\n",
    "    model = Model(inputs = [z_y, vec_1], outputs = valid)\n",
    "    \n",
    "    print('combined')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,377\n",
      "Trainable params: 213,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "generator\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               104832    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 768)               99072     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 768)               0         \n",
      "=================================================================\n",
      "Total params: 203,904\n",
      "Trainable params: 203,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "combined\n",
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 818)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 768)          203904      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1536)         0           input_8[0][0]                    \n",
      "                                                                 sequential_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 1)            213377      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 417,281\n",
      "Trainable params: 203,904\n",
      "Non-trainable params: 213,377\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#コンパイル\n",
    "discriminator = build_discriminator(vec_shape)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator(z_dim,vec_shape)\n",
    "    \n",
    "combined = build_combined(generator,discriminator)\n",
    "    \n",
    "combined.compile(loss='binary_crossentropy',optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaling(data,change_min,change_max):\n",
    "    max_data = 10 #実数より大きい整数\n",
    "    min_data = -5#実数より小さい整数\n",
    "    data_std = (data - min_data) / (max_data - min_data)\n",
    "    data = data_std*(change_max - change_min) + change_min\n",
    "    return data , min_data, max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaling_inverse(data,change_min,change_max):\n",
    "    max_data = 1 #実数より大きい整数\n",
    "    min_data = -1#実数より小さい整数\n",
    "    data_std = (data - min_data) / (max_data - min_data)\n",
    "    data = data_std*(change_max - change_min) + change_min\n",
    "    return data , min_data, max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "iteration_checkpoints = []\n",
    "def train(discriminator_input, iterations, batch_size, sample_interval):\n",
    "    \n",
    "    generator_input = []\n",
    "    for i in discriminator_input:\n",
    "        generator_input.append(i[:768])\n",
    "    generator_input  = np.array(generator_input)\n",
    "    \n",
    "    #generator_input = 部品ベクトル\n",
    "    #discriminator_input = [部品ベクトル,対策ベクトル]\n",
    "    #-1~1へスケーリング\n",
    "    generator_input , _ , _ = data_scaling(generator_input,-1,1)\n",
    "    discriminator_input , _ , _ =  data_scaling(discriminator_input,-1,1)\n",
    "    \n",
    "    \n",
    "    #ラベル1\n",
    "    real = np.ones((batch_size,1))\n",
    "    #ラベル0\n",
    "    fake = np.zeros((batch_size,1))\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        #-------------------\n",
    "        #識別器の訓練\n",
    "        #-------------------\n",
    "        \n",
    "        #本物のデータから適当に選ぶ\n",
    "        idx = np.random.randint(0,len(discriminator_input),batch_size)\n",
    "        #discriminatorの入力にいれる本物の[部品ベクトル,対策ベクトル]\n",
    "        real_discriminator_input = discriminator_input[idx]\n",
    "        \n",
    "        #部品のベクトルの情報を含んだランダムベクトルを作る\n",
    "        \n",
    "        #noize_y = np.random.randint(0,len(generator_input),batch_size)\n",
    "        \n",
    "        noize_y = generator_input[idx]\n",
    "        \n",
    "        #平均0標準偏差１の正規分布に従う乱数を生成\n",
    "        noize_z = np.random.normal(0,2,(batch_size, z_dim))\n",
    "        \n",
    "        #ランダムベクトルと部品のベクトルを結合\n",
    "        noize_z_y = np.concatenate((noize_z, noize_y),axis=1)\n",
    "        \n",
    "        #ジェネレータで偽の対策文書ベクトルを生成させる\n",
    "        gen_vec = generator.predict(noize_z_y)\n",
    "        \n",
    "        #if (iteration +1) % sample_interval == 0:\n",
    "            #print(gen_vec)\n",
    "            #print(noize_z)\n",
    "        #discriminatorの中にいれる偽の[部品ベクトル,対策ベクル]の生成\n",
    "        fake_discriminator_input = np.concatenate((noize_y,gen_vec),axis=1)\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(real_discriminator_input, real)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_discriminator_input, fake)\n",
    "        \n",
    "        #それぞれの損失関数を平均\n",
    "        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        #-------------------\n",
    "        #生成器の訓練\n",
    "        #-------------------\n",
    "        #部品のベクトルの情報を含んだランダムベクトルを作る\n",
    "        \n",
    "        noize_y = np.random.randint(0,len(generator_input),batch_size)\n",
    "        \n",
    "        noize_y = generator_input[noize_y]\n",
    "\n",
    "        #平均0標準偏差１の正規分布に従う乱数を生成\n",
    "        noize_z = np.random.normal(0,2,(batch_size, z_dim))\n",
    "\n",
    "        #ランダムベクトルと部品のベクトルを結合\n",
    "        noize_z_y = np.concatenate((noize_z, noize_y),axis=1)\n",
    "\n",
    "        #ジェネレータで偽の対策文書ベクトルを生成させる\n",
    "        gen_vec = generator.predict(noize_z_y)\n",
    "        \n",
    "        g_loss = combined.train_on_batch([noize_z_y,noize_y],real)\n",
    "        \n",
    "        if (iteration +1) % sample_interval == 0:\n",
    "            losses.append((d_loss, g_loss))\n",
    "            accuracies.append(100.0 * accuracy)\n",
    "            iteration_checkpoints.append(iteration +1)\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %(iteration +1,d_loss, 100.0*accuracy,g_loss))\n",
    "    return generator,  discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 [D loss: 0.000000, acc.: 100.00%] [G loss: 86.892822]\n",
      "2000 [D loss: 0.000000, acc.: 100.00%] [G loss: 105.164200]\n"
     ]
    }
   ],
   "source": [
    "iterations =2000\n",
    "batch_size = 32\n",
    "sample_interval = 1000\n",
    "\n",
    "generator , discriminator= train(data , iterations, batch_size,sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "buhin_name_list = []\n",
    "for index, row in df.iterrows():#部品\n",
    "    buhin_name_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "class pycolor:\n",
    "    RED = '\\033[31m'\n",
    "    BLUE = '\\033[34m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_taisaku(text):\n",
    "    vec = weighted_mean_vec(text)\n",
    "    scaling_vec ,max_data , min_data = data_scaling(vec,-1,1)\n",
    "    print(max_data,min_data)\n",
    "    scaling_vec = scaling_vec.reshape(1,768)\n",
    "    noize_z = np.random.normal(0,1,(1, z_dim))\n",
    "    noize = np.concatenate((noize_z, scaling_vec),axis=1)\n",
    "    gen = np.array(generator(noize))\n",
    "    result , _ , _ = data_scaling(gen, -5, 10) \n",
    "    score_sim = {}\n",
    "    for k,v in taisaku_vec_word.items():\n",
    "        score_sim[k]=cos_sim(result, v)\n",
    "    score_sort = sorted(score_sim.items(),key=lambda x:x[1], reverse=True)\n",
    "    for i_1, i_2 in score_sort[0:10]:\n",
    "        print(i_1)\n",
    "        print(i_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "buhin_name_list = []\n",
    "for index, row in df.iterrows():#部品\n",
    "    buhin_name_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(text):\n",
    "    print('---------------部品----------------')\n",
    "    print(text)\n",
    "    print('---------------対策----------------')\n",
    "    taisaku = []\n",
    "    for index, row in df.iterrows():#部品\n",
    "            if (index == text):\n",
    "                for i in df:#対策\n",
    "                    if row[i] ==1:\n",
    "                        #print(i)\n",
    "                        taisaku.append(i)\n",
    "                break\n",
    "    print('---------------予想----------------')\n",
    "    vec = get_vector_cls(text)\n",
    "    scaling_vec , max_data , min_data = data_scaling(vec,-1,1)\n",
    "    scaling_vec = scaling_vec.reshape(1,768)\n",
    "    noize_z = np.random.normal(0,1,(1, z_dim))\n",
    "    noize = np.concatenate((noize_z, scaling_vec),axis=1)\n",
    "    gen = np.array(generator(noize))\n",
    "    result , _ , _ = data_scaling_inverse(gen, -5, 5)\n",
    "    ####print(result)\n",
    "    score_sim = {}\n",
    "    for k,v in taisaku_vec_word.items():\n",
    "        score_sim[k]=cos_sim(result, v)\n",
    "    score_sort = sorted(score_sim.items(),key=lambda x:x[1], reverse=True)\n",
    "    for i_1, i_2 in score_sort[0:10]:\n",
    "        print(i_1)\n",
    "        print(i_2)\n",
    "        print(taisaku_count[i_1])\n",
    "        if i_1 in taisaku:\n",
    "            print(pycolor.BLUE+\"ある\"+pycolor.END)\n",
    "        else:\n",
    "            print(pycolor.RED+\"なし\"+pycolor.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------部品----------------\n",
      "エンジン搭載 吸気レゾネーター\n",
      "---------------対策----------------\n",
      "---------------予想----------------\n",
      "DR結果を一元化したE-BOMに蓄積して、再利用できる。\n",
      "[0.03591824]\n",
      "152\n",
      "\u001b[34mある\u001b[0m\n",
      "品名単位でものと情報の流れが見える化。リードタイム短縮施策があり実施。\n",
      "[0.0250303]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "職務拡充により作業者のモチベーション向上。品質を作りこむ意識が醸成。\n",
      "[0.02337536]\n",
      "152\n",
      "\u001b[34mある\u001b[0m\n",
      "ATシフトケーブルのクランプ最適化。PCU座面設定。\n",
      "[0.02278919]\n",
      "5\n",
      "\u001b[31mなし\u001b[0m\n",
      "メインインインタミシャフト圧縮、ストロークの確認。\n",
      "[0.02155635]\n",
      "5\n",
      "\u001b[31mなし\u001b[0m\n",
      "ロアアーム、ケージダウン検討\n",
      "[0.01963898]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "ストラットの共通化。効果を明確に。\n",
      "[0.01635489]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "一元化されたEBOM/PBOM/MBOMがありデータ連携\n",
      "[0.016139]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "CAE解析データを作成し衝突ストーリーを作成する\n",
      "[0.01598884]\n",
      "21\n",
      "\u001b[31mなし\u001b[0m\n",
      "マルチOEM管理を実施、各BOMをクラウドを使って実施できる。\n",
      "[0.01592424]\n",
      "21\n",
      "\u001b[31mなし\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text_1 = buhin_name_list[0]\n",
    "test(text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_1 = buhin_name_list[10]\n",
    "test(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------部品----------------\n",
      "エンジン本体 オイルパン　サブアセンブリ\n",
      "---------------対策----------------\n",
      "---------------予想----------------\n",
      "DR結果を一元化したE-BOMに蓄積して、再利用できる。\n",
      "[0.03591824]\n",
      "152\n",
      "\u001b[31mなし\u001b[0m\n",
      "品名単位でものと情報の流れが見える化。リードタイム短縮施策があり実施。\n",
      "[0.0250303]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "職務拡充により作業者のモチベーション向上。品質を作りこむ意識が醸成。\n",
      "[0.02337536]\n",
      "152\n",
      "\u001b[31mなし\u001b[0m\n",
      "ATシフトケーブルのクランプ最適化。PCU座面設定。\n",
      "[0.02278919]\n",
      "5\n",
      "\u001b[31mなし\u001b[0m\n",
      "メインインインタミシャフト圧縮、ストロークの確認。\n",
      "[0.02155635]\n",
      "5\n",
      "\u001b[31mなし\u001b[0m\n",
      "ロアアーム、ケージダウン検討\n",
      "[0.01963898]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "ストラットの共通化。効果を明確に。\n",
      "[0.01635489]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "一元化されたEBOM/PBOM/MBOMがありデータ連携\n",
      "[0.016139]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "CAE解析データを作成し衝突ストーリーを作成する\n",
      "[0.01598884]\n",
      "21\n",
      "\u001b[34mある\u001b[0m\n",
      "マルチOEM管理を実施、各BOMをクラウドを使って実施できる。\n",
      "[0.01592424]\n",
      "21\n",
      "\u001b[34mある\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text_1 = buhin_name_list[50]\n",
    "test(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(text):\n",
    "    taisaku = []\n",
    "    precition = 0\n",
    "    for index, row in df.iterrows():#部品\n",
    "            if (index == text):\n",
    "                for i in df:#対策\n",
    "                    if row[i] ==1:\n",
    "                        #print(i)\n",
    "                        taisaku.append(i)\n",
    "                break\n",
    "    vec = get_vector_cls(text)\n",
    "    scaling_vec , max_data , min_data = data_scaling(vec,-1,1)\n",
    "    scaling_vec = scaling_vec.reshape(1,768)\n",
    "    noize_z = np.random.normal(0,1,(1, z_dim))\n",
    "    noize = np.concatenate((noize_z, scaling_vec),axis=1)\n",
    "    gen = np.array(generator(noize))\n",
    "    result , _ , _ = data_scaling_inverse(gen, -5, 5)\n",
    "    score_sim = {}\n",
    "    for k,v in taisaku_vec_word.items():\n",
    "        score_sim[k]=cos_sim(result, v)\n",
    "    score_sort = sorted(score_sim.items(),key=lambda x:x[1], reverse=True)\n",
    "    for i_1, i_2 in score_sort[0:10]:\n",
    "        if i_1 in taisaku:\n",
    "            precition += 1\n",
    "    return precition/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2057803468208091\n",
      "0.2057803468208091\n",
      "0.2057803468208091\n",
      "0.2057803468208091\n",
      "0.2057803468208091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2057803468208091"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_precition = 0\n",
    "tt = 0\n",
    "for ei in range(5):\n",
    "    total_precition = 0\n",
    "    for i in range(173):\n",
    "        text_1 = buhin_name_list[i]\n",
    "        precition = test(text_1)\n",
    "        total_precition += precition\n",
    "    print(total_precition/173)\n",
    "    tt += total_precition/173\n",
    "tt/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "buhim = []\n",
    "test_buhin_name = []\n",
    "for i in test_data:\n",
    "    buhim.append(tuple(i[:768]))\n",
    "for k,v in buhin_dict.items():\n",
    "    if v in buhim:\n",
    "        test_buhin_name.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_precition = 0\n",
    "tt = 0\n",
    "for ei in range(5):\n",
    "    total_precition = 0\n",
    "    for i in range(len(test_buhin_name)):\n",
    "        text_1 =test_buhin_name[i]\n",
    "        precition = test(text_1)\n",
    "        total_precition += precition\n",
    "    print(total_precition/len(test_buhin_name))\n",
    "    tt += total_precition/173\n",
    "tt/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000  21.1\n",
    "# 2000  20.5\n",
    "# 5000  20.5\n",
    "#10000  21.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [21.1,20.5,20.5,21.7]\n",
    "sum(a)/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1 = buhin_name_list[100]\n",
    "test(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  これはモード崩壊（mode collapse）と呼ばれる現象で、generatorの学習に失敗して、訓練データの（しばしば多峰性の）分布全体を表現できずに訓練データの最頻値（mode）のみを学習してしまいます。全国民の期待に応える能力がなく、とりあえず多数派のための政策をつくる、みたいなイメージですかね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input , _ , _ = data_scaling(buhin,-1,1)\n",
    "discriminator_input , _ , _ =  data_scaling(data,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "noize_y = np.random.randint(0,len(buhin),batch_size)\n",
    "noize_y = buhin2[noize_y]\n",
    "noize_z = np.random.normal(0,1,(batch_size, z_dim))\n",
    "noize_z_y = np.concatenate((noize_z, noize_y),axis=1)\n",
    "gen_vec = generator.predict(noize_z_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4359803 , -0.05961279, -0.04837259, ...,  0.00045572,\n",
       "         0.2699624 ,  0.04629831],\n",
       "       [ 0.4359803 , -0.05961279, -0.04837259, ...,  0.00045572,\n",
       "         0.2699624 ,  0.04629831],\n",
       "       [ 0.4359803 , -0.05961279, -0.04837259, ...,  0.00045572,\n",
       "         0.2699624 ,  0.04629831],\n",
       "       ...,\n",
       "       [ 0.4359803 , -0.05961279, -0.04837259, ...,  0.00045572,\n",
       "         0.2699624 ,  0.04629831],\n",
       "       [ 0.4359803 , -0.05961279, -0.04837259, ...,  0.00045572,\n",
       "         0.2699624 ,  0.04629831],\n",
       "       [ 0.4359803 , -0.05961279, -0.04837259, ...,  0.00045572,\n",
       "         0.2699624 ,  0.04629831]], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35201003, -0.48215373, -0.3709283 , ..., -0.37396668,\n",
       "         1.54047585, -0.89240008],\n",
       "       [ 1.02011903, -0.99039578,  0.6147865 , ..., -0.22950753,\n",
       "         1.30674742, -1.04674338],\n",
       "       [-1.89017538,  0.45027579, -0.33180167, ..., -0.37396668,\n",
       "         1.54047585, -0.89240008],\n",
       "       ...,\n",
       "       [ 1.13324786,  0.56365652, -0.66692741, ..., -0.49368474,\n",
       "         1.28149669, -0.86522065],\n",
       "       [ 0.68343362, -2.26604594, -0.14852601, ..., -0.49368474,\n",
       "         1.28149669, -0.86522065],\n",
       "       [ 0.47330225, -1.55339981, -0.02546597, ..., -0.49368474,\n",
       "         1.28149669, -0.86522065]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noize_z_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.38220024,  0.61715281, -0.45223216, ..., -0.37396668,\n",
       "         1.54047585, -0.89240008],\n",
       "       [ 0.55496857,  1.33560361,  0.10814531, ..., -0.22950753,\n",
       "         1.30674742, -1.04674338],\n",
       "       [ 1.38220024,  0.61715281, -0.45223216, ..., -0.37396668,\n",
       "         1.54047585, -0.89240008],\n",
       "       ...,\n",
       "       [ 1.50043327,  0.71683983,  0.20033428, ..., -0.49368474,\n",
       "         1.28149669, -0.86522065],\n",
       "       [ 1.50043327,  0.71683983,  0.20033428, ..., -0.49368474,\n",
       "         1.28149669, -0.86522065],\n",
       "       [ 1.50043327,  0.71683983,  0.20033428, ..., -0.49368474,\n",
       "         1.28149669, -0.86522065]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noize_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
