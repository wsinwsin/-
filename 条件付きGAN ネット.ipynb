{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.layers import Activation, BatchNormalization, Concatenate, Dense, Embedding, Flatten, Reshape, Input, Multiply,Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import MeCab\n",
    "import json\n",
    "import hashlib\n",
    "from googletrans import Translator\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import torch\n",
    "from transformers.modeling_bert import BertModel\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "mt = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "mt.parse('')\n",
    "model_word = word2vec.Word2Vec.load(\"wiki_plus.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(text):\n",
    "    word = {}\n",
    "    node = mt.parseToNode(text)\n",
    "    while node:\n",
    "        fields = node.feature.split(\",\")\n",
    "        if (fields[0] == '名詞' or fields[0] == '動詞' or fields[0] == '形容詞') and node.surface in model_word.wv:\n",
    "            w = node.surface\n",
    "            word[w] = word.get(w, 0) + 1\n",
    "        node = node.next\n",
    "    return word\n",
    "\n",
    "def weighted_mean_vec(text):\n",
    "    v = np.zeros(model_word.vector_size)\n",
    "    s = 1.0\n",
    "    for w,weight in get_tags(text).items():\n",
    "        v += weight * model_word.wv[w]  #Eventクラスeの単語wの個数＊単語wのベクトル\n",
    "        s += weight\n",
    "    return v / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event:\n",
    "    def __init__(self, id, type, score, desc, links):\n",
    "        self.id = id\n",
    "        self.type = type\n",
    "        self.score = score\n",
    "        self.desc = desc\n",
    "        self.links = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON ファイルから event set をロード\n",
    "def load_events(jsonfile):\n",
    "    with open(jsonfile) as f:\n",
    "        df = json.load(f)\n",
    "    events = {x['id']: Event(x['id'], x['type'], x['score'], x['desc'], x['links']) for x in df} #eventsにidをkeyとしそのオブジェクトをvalueとした辞書を生成\n",
    "    for k,x in events.items():\n",
    "        x.links = [events[e] for e in x.links] #Event.linkの中身をidの配列からEventの配列に変更\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = load_events('sesaku2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "labels = []\n",
    "columns=[]\n",
    "index=[]\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='部品':\n",
    "        index.append(v1.desc)\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='対策':\n",
    "        if not v1.desc in columns:\n",
    "            columns.append(v1.desc)\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "for k1, v1 in events.items():\n",
    "    if v1.type[-1]=='部品':\n",
    "        for k2, v2 in events.items():\n",
    "            if v2.type[-1] == '対策':\n",
    "                    if v2 in v1.links:\n",
    "                        df.at[v1.desc, v2.desc] = 1\n",
    "                    else:\n",
    "                        df.at[v1.desc, v2.desc] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 319)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "taisaku_vec_word = {}\n",
    "taisaku_count ={}\n",
    "for i in df:\n",
    "    taisaku_vec_word[i]=weighted_mean_vec(i)\n",
    "    taisaku_count[i] = 0\n",
    "\n",
    "#label →部品　生成→対策\n",
    "data = []\n",
    "buhin = []\n",
    "taisaku = []\n",
    "# data \n",
    "for index, row in df.iterrows():#部品\n",
    "    x1 = weighted_mean_vec(index)\n",
    "    buhin.append(x1)\n",
    "    for i in df:#対策\n",
    "        x2 =  taisaku_vec_word[i]#対策\n",
    "        if row[i] ==1:\n",
    "            data.extend([np.append(x1, x2)])\n",
    "            taisaku_count[i] += 1\n",
    "            \n",
    "for k,v in taisaku_vec_word.items():\n",
    "    taisaku.append(v)\n",
    "data = np.array(data)\n",
    "buhin = np.array(buhin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7303, 400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#関係がある部品と対策の文書ベクトル\n",
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#部品の文書ベクトルだけ\n",
    "buhin = np.array(buhin)\n",
    "buhin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.838822269439698"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buhin.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#対策の文書ベクトルだけ\n",
    "taisaku = np.array(taisaku)\n",
    "taisaku.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "taisaku_map = sorted(taisaku_count.items(),key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パラメータ\n",
    "vec_length = 200\n",
    "vec_shape =(vec_length,)\n",
    "z_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " #生成器\n",
    "def build_generator(z_dim,vec_shape):\n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(Dense(128,input_dim =( z_dim+vec_shape[0])))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    " \n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation('tanh'))\n",
    "        print('generator')\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#識別器\n",
    "def build_discriminator(vec_shape):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, input_dim=vec_shape[0]*2)) #activation\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print('discriminator')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generatorを学習する時のもの\n",
    "def build_combined(generator,discriminator):\n",
    "    z_y = Input(shape=(z_dim+vec_shape[0],)) #ランダムベクトル＋部品ベクトル\n",
    "    vec_1 = Input(shape=vec_shape)              #部品ベクトル\n",
    "    \n",
    "    #ベクトルを生成\n",
    "    gen_vec = generator(z_y)\n",
    "    \n",
    "    #discriminatorに入力するやつ\n",
    "    vec_2 = Concatenate()([vec_1,gen_vec])  #[部品ベクトル,対策ベクトル]\n",
    "    #discriminatorの重みとバイアスを固定\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    valid = discriminator(vec_2)\n",
    "\n",
    "    model = Model(inputs = [z_y, vec_1], outputs = valid)\n",
    "    \n",
    "    print('combined')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               51328     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 67,969\n",
      "Trainable params: 67,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "generator\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               32128     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               25800     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 57,928\n",
      "Trainable params: 57,928\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "combined\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 200)          57928       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 400)          0           input_2[0][0]                    \n",
      "                                                                 sequential_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 1)            67969       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 125,897\n",
      "Trainable params: 57,928\n",
      "Non-trainable params: 67,969\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#コンパイル\n",
    "discriminator = build_discriminator(vec_shape)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator(z_dim,vec_shape)\n",
    "    \n",
    "combined = build_combined(generator,discriminator)\n",
    "    \n",
    "combined.compile(loss='binary_crossentropy',optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaling(data,change_min,change_max):\n",
    "    max_data = math.ceil(data.max()) #実数より大きい整数\n",
    "    min_data = math.floor(data.min()) #実数より小さい整数\n",
    "    data_std = (data - min_data) / (max_data - min_data)\n",
    "    data = data_std*(change_max - change_min) + change_min\n",
    "    return data , min_data, max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "iteration_checkpoints = []\n",
    "generater_list = []\n",
    "def train(generator_input, discriminator_input, iterations, batch_size, sample_interval):\n",
    "    \n",
    "    #generator_input = 部品ベクトル\n",
    "    #discriminator_input = [部品ベクトル,対策ベクトル]\n",
    "    #-1~1へスケーリング\n",
    "    generator_input , _ , _ = data_scaling(generator_input,-1,1)\n",
    "    discriminator_input , _ , _ =  data_scaling(discriminator_input,-1,1)\n",
    "    #ラベル1\n",
    "    real = np.ones((batch_size,1))\n",
    "    #ラベル0\n",
    "    fake = np.zeros((batch_size,1))\n",
    "    generater_list = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        #-------------------\n",
    "        #識別器の訓練\n",
    "        #-------------------\n",
    "        \n",
    "        #本物のデータから適当に選ぶ\n",
    "        idx = np.random.randint(0,len(discriminator_input),batch_size)\n",
    "        #discriminatorの入力にいれる本物の[部品ベクトル,対策ベクトル]\n",
    "        real_discriminator_input = discriminator_input[idx]\n",
    "        \n",
    "        #部品のベクトルの情報を含んだランダムベクトルを作る\n",
    "        \n",
    "        noize_y = np.random.randint(0,len(generator_input),batch_size)\n",
    "        \n",
    "        noize_y = generator_input[noize_y]\n",
    "        \n",
    "        #平均0標準偏差１の正規分布に従う乱数を生成\n",
    "        noize_z = np.random.normal(0,1,(batch_size, z_dim))\n",
    "        \n",
    "        #ランダムベクトルと部品のベクトルを結合\n",
    "        noize_z_y = np.concatenate((noize_z, noize_y),axis=1)\n",
    "        \n",
    "        #ジェネレータで偽の対策文書ベクトルを生成させる\n",
    "        gen_vec = generator.predict(noize_z_y)\n",
    "        \n",
    "        #discriminatorの中にいれる偽の[部品ベクトル,対策ベクル]の生成\n",
    "        fake_discriminator_input = np.concatenate((noize_y,gen_vec),axis=1)\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(real_discriminator_input, real)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_discriminator_input, fake)\n",
    "        \n",
    "        #それぞれの損失関数を平均\n",
    "        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        #-------------------\n",
    "        #生成器の訓練\n",
    "        #-------------------\n",
    "        #部品のベクトルの情報を含んだランダムベクトルを作る\n",
    "        \n",
    "        noize_y = np.random.randint(0,len(generator_input),batch_size)\n",
    "        \n",
    "        noize_y = generator_input[noize_y]\n",
    "\n",
    "        #平均0標準偏差１の正規分布に従う乱数を生成\n",
    "        noize_z = np.random.normal(0,1,(batch_size, z_dim))\n",
    "\n",
    "        #ランダムベクトルと部品のベクトルを結合\n",
    "        noize_z_y = np.concatenate((noize_z, noize_y),axis=1)\n",
    "\n",
    "        #ジェネレータで偽の対策文書ベクトルを生成させる\n",
    "        gen_vec = generator.predict(noize_z_y)\n",
    "        \n",
    "        g_loss = combined.train_on_batch([noize_z_y,noize_y],real)\n",
    "        \n",
    "        if (iteration +1) % sample_interval == 0:\n",
    "            losses.append((d_loss, g_loss))\n",
    "            accuracies.append(100.0 * accuracy)\n",
    "            iteration_checkpoints.append(iteration +1)\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %(iteration +1,d_loss, 100.0*accuracy,g_loss))\n",
    "            generater_list.append(generator)\n",
    "    return generator, generater_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 [D loss: 0.047372, acc.: 98.44%] [G loss: 5.414024]\n",
      "2000 [D loss: 0.016686, acc.: 100.00%] [G loss: 7.921521]\n",
      "3000 [D loss: 0.072291, acc.: 95.31%] [G loss: 6.679328]\n",
      "4000 [D loss: 0.036468, acc.: 98.44%] [G loss: 10.885368]\n",
      "5000 [D loss: 0.016861, acc.: 100.00%] [G loss: 8.583508]\n",
      "6000 [D loss: 0.000201, acc.: 100.00%] [G loss: 8.370418]\n",
      "7000 [D loss: 0.157188, acc.: 96.88%] [G loss: 4.479087]\n",
      "8000 [D loss: 0.009539, acc.: 100.00%] [G loss: 4.338871]\n",
      "9000 [D loss: 0.009767, acc.: 100.00%] [G loss: 4.722936]\n",
      "10000 [D loss: 0.077230, acc.: 98.44%] [G loss: 3.815525]\n"
     ]
    }
   ],
   "source": [
    "iterations =10000\n",
    "batch_size = 32\n",
    "sample_interval = 1000\n",
    "\n",
    "generator , generator_list = train(buhin, data , iterations, batch_size,sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 200)\n",
      "(1, 250)\n"
     ]
    }
   ],
   "source": [
    "scaling_buhin ,min_data, max_data = data_scaling(buhin,-1,1)\n",
    "noize_y = np.random.randint(0,len(scaling_buhin),1)\n",
    "noize_y = buhin[noize_y]\n",
    "noize_z = np.random.normal(0,1,(1, z_dim))\n",
    "print(noize_y.shape)\n",
    "noize_z_y = np.concatenate((noize_z, noize_y),axis=1)\n",
    "print(noize_z_y.shape)\n",
    "gen = np.array(generator(noize_z_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result , _,_ = data_scaling(gen, min_data,max_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.reshape(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "class pycolor:\n",
    "    RED = '\\033[31m'\n",
    "    BLUE = '\\033[34m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_taisaku(text):\n",
    "    vec = weighted_mean_vec(text)\n",
    "    scaling_vec , _ , _ = data_scaling(vec,-1,1)\n",
    "    scaling_vec = scaling_vec.reshape(1,200)\n",
    "    noize_z = np.random.normal(0,1,(1, z_dim))\n",
    "    noize = np.concatenate((noize_z, scaling_vec),axis=1)\n",
    "    gen = np.array(generator(noize))\n",
    "    result , _ , _ = data_scaling(gen, -5, 5) \n",
    "    score_sim = {}\n",
    "    for k,v in taisaku_vec_word.items():\n",
    "        score_sim[k]=cos_sim(result, v)\n",
    "    score_sort = sorted(score_sim.items(),key=lambda x:x[1], reverse=True)\n",
    "    for i_1, i_2 in score_sort[0:10]:\n",
    "        print(i_1)\n",
    "        print(i_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "製造部門だけでなく開発・設計などホワイトカラー業務にも適用できれいる。\n",
      "[0.70772956]\n",
      "メンテナンスコストやクレーム対応コストを加味した原価設計、歩留まり率設計を実施している\n",
      "[0.707253]\n",
      "原価企画・原価計画・原価維持・原価改善といった視点で、原価管理を実施している。\n",
      "[0.70620052]\n",
      "先行開発ゲートの定義は、通常開発の定義と異なっており、技術の棚という考え方がある\n",
      "[0.68565887]\n",
      "市場不具合を回収、分析、設計にフードバックする体制と組織がある\n",
      "[0.67890323]\n",
      "製品の開発・設計の段階から生産部門のスタッフが参画しており、開発から市場投入までのリードタイムを短縮している\n",
      "[0.67247227]\n",
      "開発～製造までの全社業務フロー図があり、連携を取りながら機能している\n",
      "[0.6691743]\n",
      "コンフィグレータが、製品構成、評価、生産まで考慮した仕様になっている。\n",
      "[0.66738615]\n",
      "生準業務マニュアルが整備。型・設備・治具・製作・整備技能が標準化されている\n",
      "[0.66533257]\n",
      "プランジャ当たり面の検査は、定量化・仕組み化・標準化しているか\n",
      "[0.66473588]\n"
     ]
    }
   ],
   "source": [
    "search_taisaku('afe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "buhin_name_list = []\n",
    "for index, row in df.iterrows():#部品\n",
    "    buhin_name_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(text):\n",
    "    print('---------------部品----------------')\n",
    "    print(text)\n",
    "    print('---------------対策----------------')\n",
    "    taisaku = []\n",
    "    for index, row in df.iterrows():#部品\n",
    "            if (index == text):\n",
    "                for i in df:#対策\n",
    "                    if row[i] ==1:\n",
    "                        #print(i)\n",
    "                        taisaku.append(i)\n",
    "                break\n",
    "    print('---------------予想----------------')\n",
    "    vec = weighted_mean_vec(text)\n",
    "    scaling_vec , _ , _ = data_scaling(vec,-1,1)\n",
    "    scaling_vec = scaling_vec.reshape(1,200)\n",
    "    noize_z = np.random.normal(0,1,(1, z_dim))\n",
    "    noize = np.concatenate((noize_z, scaling_vec),axis=1)\n",
    "    gen = np.array(generator(noize))\n",
    "    result , _ , _ = data_scaling(gen, -5, 5)\n",
    "    ####print(result)\n",
    "    score_sim = {}\n",
    "    for k,v in taisaku_vec_word.items():\n",
    "        score_sim[k]=cos_sim(result, v)\n",
    "    score_sort = sorted(score_sim.items(),key=lambda x:x[1], reverse=True)\n",
    "    for i_1, i_2 in score_sort[0:10]:\n",
    "        print(i_1)\n",
    "        print(i_2)\n",
    "        print(taisaku_count[i_1])\n",
    "        if i_1 in taisaku:\n",
    "            print(pycolor.BLUE+\"ある\"+pycolor.END)\n",
    "        else:\n",
    "            print(pycolor.RED+\"なし\"+pycolor.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------部品----------------\n",
      "エンジン搭載 吸気レゾネーター\n",
      "---------------対策----------------\n",
      "---------------予想----------------\n",
      "定性的なアウトプットである個人の振り返りを実施。個人の成長・マネジメント軸を強化する。\n",
      "[0.97267932]\n",
      "152\n",
      "\u001b[34mある\u001b[0m\n",
      "原価を維持・改善する活動を実施\n",
      "[0.76019028]\n",
      "21\n",
      "\u001b[31mなし\u001b[0m\n",
      "会社目標から製品、各機能組織ごとの、目標展開を実施。メンバー目標とも連携している\n",
      "[0.69302565]\n",
      "152\n",
      "\u001b[34mある\u001b[0m\n",
      "職務拡充により作業者のモチベーション向上。品質を作りこむ意識が醸成。\n",
      "[0.68430051]\n",
      "152\n",
      "\u001b[34mある\u001b[0m\n",
      "原価企画・原価計画・原価維持・原価改善といった視点で、原価管理を実施している。\n",
      "[0.67269286]\n",
      "152\n",
      "\u001b[34mある\u001b[0m\n",
      "金型・治具の原価企画を実施。目標を設定して低減活動を実施\n",
      "[0.66889759]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "社内のノウハウを標準化し、固有ノウハウを蓄積して効率的に活用\n",
      "[0.65630081]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "ライン切り替え時の段取り作業の標準化、低減目標を設定し、年間活動計画で実施している\n",
      "[0.65117009]\n",
      "21\n",
      "\u001b[31mなし\u001b[0m\n",
      "物流コストを毎月把握し低減活動を実施。施策も見える化も実施。\n",
      "[0.64844295]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "前機種の振り返りを実施。目標に対する達成度、課題と次の一手を適用している\n",
      "[0.64817003]\n",
      "21\n",
      "\u001b[31mなし\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text_1 = buhin_name_list[0]\n",
    "test(text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_1 = buhin_name_list[10]\n",
    "test(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------部品----------------\n",
      "エンジン本体 オイルパン　サブアセンブリ\n",
      "---------------対策----------------\n",
      "---------------予想----------------\n",
      "定性的なアウトプットである個人の振り返りを実施。個人の成長・マネジメント軸を強化する。\n",
      "[0.97267932]\n",
      "152\n",
      "\u001b[31mなし\u001b[0m\n",
      "原価を維持・改善する活動を実施\n",
      "[0.76019028]\n",
      "21\n",
      "\u001b[34mある\u001b[0m\n",
      "会社目標から製品、各機能組織ごとの、目標展開を実施。メンバー目標とも連携している\n",
      "[0.69302565]\n",
      "152\n",
      "\u001b[31mなし\u001b[0m\n",
      "職務拡充により作業者のモチベーション向上。品質を作りこむ意識が醸成。\n",
      "[0.68430051]\n",
      "152\n",
      "\u001b[31mなし\u001b[0m\n",
      "原価企画・原価計画・原価維持・原価改善といった視点で、原価管理を実施している。\n",
      "[0.67269286]\n",
      "152\n",
      "\u001b[31mなし\u001b[0m\n",
      "金型・治具の原価企画を実施。目標を設定して低減活動を実施\n",
      "[0.66889759]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "社内のノウハウを標準化し、固有ノウハウを蓄積して効率的に活用\n",
      "[0.65630081]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "ライン切り替え時の段取り作業の標準化、低減目標を設定し、年間活動計画で実施している\n",
      "[0.65117009]\n",
      "21\n",
      "\u001b[34mある\u001b[0m\n",
      "物流コストを毎月把握し低減活動を実施。施策も見える化も実施。\n",
      "[0.64844295]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "前機種の振り返りを実施。目標に対する達成度、課題と次の一手を適用している\n",
      "[0.64817003]\n",
      "21\n",
      "\u001b[34mある\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text_1 = buhin_name_list[50]\n",
    "test(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------部品----------------\n",
      "エンジン本体 クリーナーアセンブリ、エアW /エレメント\n",
      "---------------対策----------------\n",
      "---------------予想----------------\n",
      "定性的なアウトプットである個人の振り返りを実施。個人の成長・マネジメント軸を強化する。\n",
      "[0.97267932]\n",
      "152\n",
      "\u001b[34mある\u001b[0m\n",
      "原価を維持・改善する活動を実施\n",
      "[0.76019028]\n",
      "21\n",
      "\u001b[31mなし\u001b[0m\n",
      "会社目標から製品、各機能組織ごとの、目標展開を実施。メンバー目標とも連携している\n",
      "[0.69302565]\n",
      "152\n",
      "\u001b[34mある\u001b[0m\n",
      "職務拡充により作業者のモチベーション向上。品質を作りこむ意識が醸成。\n",
      "[0.68430051]\n",
      "152\n",
      "\u001b[34mある\u001b[0m\n",
      "原価企画・原価計画・原価維持・原価改善といった視点で、原価管理を実施している。\n",
      "[0.67269286]\n",
      "152\n",
      "\u001b[34mある\u001b[0m\n",
      "金型・治具の原価企画を実施。目標を設定して低減活動を実施\n",
      "[0.66889759]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "社内のノウハウを標準化し、固有ノウハウを蓄積して効率的に活用\n",
      "[0.65630081]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "ライン切り替え時の段取り作業の標準化、低減目標を設定し、年間活動計画で実施している\n",
      "[0.65117009]\n",
      "21\n",
      "\u001b[31mなし\u001b[0m\n",
      "物流コストを毎月把握し低減活動を実施。施策も見える化も実施。\n",
      "[0.64844295]\n",
      "0\n",
      "\u001b[31mなし\u001b[0m\n",
      "前機種の振り返りを実施。目標に対する達成度、課題と次の一手を適用している\n",
      "[0.64817003]\n",
      "21\n",
      "\u001b[31mなし\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text_1 = buhin_name_list[100]\n",
    "test(text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  これはモード崩壊（mode collapse）と呼ばれる現象で、generatorの学習に失敗して、訓練データの（しばしば多峰性の）分布全体を表現できずに訓練データの最頻値（mode）のみを学習してしまいます。全国民の期待に応える能力がなく、とりあえず多数派のための政策をつくる、みたいなイメージですかね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
